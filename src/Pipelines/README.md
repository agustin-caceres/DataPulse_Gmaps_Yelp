# Pipeline ETL de Yelp y Google en Airflow üöÄ

## Descripci√≥n General üìù
Este proyecto implementa un pipeline ETL (Extracci√≥n, Transformaci√≥n, Carga) utilizando Google Cloud Platform (GCP) y Apache Airflow. El objetivo es procesar y cargar datos de Yelp y Google en BigQuery de manera automatizada, asegurando que los datos est√©n limpios y listos para el an√°lisis. Actualmente, el pipeline procesa los datos de check-in de Yelp, almacen√°ndolos en una tabla temporal en BigQuery antes de realizar transformaciones adicionales.

## Tecnolog√≠as Utilizadas üíª
- **Google Cloud Platform (GCP)**:
  - BigQuery para el almacenamiento y procesamiento de datos.
  - Google Cloud Storage (GCS) para almacenar los archivos fuente.
  - Google Composer para administrar la arquitectura de Airflow.
- **Apache Airflow**: Orquestaci√≥n del pipeline ETL.
- **Python**: Desarrollo de funciones auxiliares y transformaci√≥n de datos.

## Estructura del Proyecto üìÇ
- **DAG**:
  - `DAG_yelp_etl.py`: Este archivo contiene el DAG principal que coordina las tareas de creaci√≥n de tablas, extracci√≥n de datos y carga en BigQuery.
- **M√≥dulos Auxiliares**:
  - `extract_data_yelp.py`: Contiene funciones para la extracci√≥n de datos desde Google Cloud Storage.
  - `transform_data_yelp.py`: Gestiona las transformaciones espec√≠ficas para cada archivo de datos.
  - `load_data_yelp.py`: Incluye funciones para cargar los datos en BigQuery y gestionar las tablas.
- **Funciones de Utilidad**:
  - `bigquery_utils.py`: Se utiliz√≥ inicialmente para funciones generales, ahora organizadas en m√≥dulos separados.

## Estructura del DAG üóÇÔ∏è

1. **Inicio**:
    - `inicio`: Un `DummyOperator` que marca el comienzo del DAG para facilitar la visualizaci√≥n en Airflow.
2. **Creaci√≥n de Tabla Temporal**:
    - `crear_tabla_temporal`: Tarea encargada de crear una tabla temporal en BigQuery para almacenar los datos de los archivos en Storage con el esquema adecuado.
3. **Extracci√≥n y Transformaci√≥n de Datos**:
    - `cargar_archivo_en_tabla_temporal`: Esta tarea extrae el archivo desde Google Cloud Storage (GCS), aplica las transformaciones necesarias y carga los datos en la tabla temporal en BigQuery.
4. **Fin**:
    - `fin`: Un `DummyOperator` que marca el fin del DAG.

## Estructura de M√≥dulos y Funciones üîß
- **extract_data_yelp.py**:
  - `cargar_archivo_gcs_a_dataframe`: Extrae un archivo de GCS y lo convierte en un DataFrame, aplicando transformaciones si es necesario.
- **transform_data_yelp.py**:
  - `pre_transformar_checkin`: Procesa el campo `date` en `checkin.json`, separando fechas en filas individuales y asegurando el formato TIMESTAMP.
  - `aplicar_transformacion`: Aplica una transformaci√≥n espec√≠fica en funci√≥n del nombre del archivo, usando un diccionario de transformaciones.
- **load_data_yelp.py**:
  - `crear_tabla_temporal`: Crea la tabla temporal en BigQuery con el esquema especificado.
  - `cargar_dataframe_a_bigquery`: Carga un DataFrame en una tabla de BigQuery.

## Esquema de la Tabla Temporal üìã
La tabla temporal `checkin_temp` contiene los siguientes campos:
- **business_id**: Identificador del negocio (STRING).
- **date**: Fecha y hora del check-in en formato TIMESTAMP.

## Funcionalidades y Detalles T√©cnicos ‚öôÔ∏è
- **Diccionario de Transformaciones**: En `transform_data_yelp.py`, el diccionario `transformaciones` asocia cada archivo con su respectiva funci√≥n de transformaci√≥n. Esto facilita la extensi√≥n del pipeline a otros archivos: solo se necesita crear una funci√≥n de transformaci√≥n y a√±adirla al diccionario.
- **Validaciones y Control de Errores**: El pipeline incluye validaciones, como verificar si el DataFrame est√° vac√≠o antes de cargarlo en BigQuery. Los bloques `try-except` se eliminaron para mejorar la transparencia y permitir que los errores se manejen adecuadamente a trav√©s de los registros de Airflow.
- **Modularizaci√≥n del C√≥digo**: Las funciones de extracci√≥n, transformaci√≥n y carga se han reorganizado en m√≥dulos independientes (`extract_data_yelp.py`, `transform_data_yelp.py` y `load_data_yelp.py`) para mejorar la claridad y la reutilizaci√≥n del c√≥digo.

## Pr√≥ximos Pasos üîú
- **Implementaci√≥n de Transformaci√≥n en BigQuery**:
  - Realizar transformaciones adicionales en BigQuery sobre la tabla temporal (`checkin_temp`) antes de mover los datos a la tabla final.
  - Eliminar la tabla temporal una vez que los datos hayan sido transformados y cargados en la tabla final, para optimizar el uso de recursos.
- **Automatizaci√≥n para Otros Datasets**:
  - Extender el pipeline para procesar otros archivos de Yelp y Google, agregando sus transformaciones espec√≠ficas al diccionario `transformaciones`.
- **Automatizaci√≥n del Pipeline**:
  - Configurar sensores o triggers en Airflow para que el pipeline se active autom√°ticamente al detectar nuevos archivos en GCS.
- **Monitoreo y Alertas**:
  - Implementar alertas y notificaciones en caso de fallos en alguna de las tareas del DAG, utilizando los servicios de Airflow y GCP.

## Posibles Mejoras y Consideraciones Futuras üåü
- **Optimizaci√≥n de Transformaciones en BigQuery**:
  - A medida que se agreguen m√°s datos, puede ser necesario optimizar las consultas de transformaci√≥n en BigQuery para reducir costos y tiempo de procesamiento.
- **Documentaci√≥n y Comentarios en el C√≥digo**:
  - Es importante mantener la documentaci√≥n del c√≥digo y actualizar el markdown conforme avancemos en el proyecto. Esto facilitar√° la colaboraci√≥n en equipo y la comprensi√≥n de los cambios realizados.
- **Manejo de Versiones**:
  - Considerar la posibilidad de versionar el c√≥digo y los datasets para mantener un historial de cambios y facilitar la reversi√≥n en caso de problemas.

## Notas Finales üìù
Este markdown documenta el progreso actual del pipeline y proporciona una gu√≠a sobre la arquitectura, el flujo de trabajo y las consideraciones clave. A medida que avancemos en el proyecto, este documento se actualizar√° para reflejar nuevos desarrollos y decisiones de dise√±o.
