# Pipeline ETL de Yelp y Google en Airflow 

## Introducci贸n 

Este proyecto implementa un pipeline **ETL (Extracci贸n, Transformaci贸n, Carga)** automatizado utilizando **Google Cloud Platform (GCP)** y **Apache Airflow**. El objetivo es procesar y cargar datos de Yelp y Google en BigQuery sin intervenci贸n manual, asegurando que los datos est茅n limpios y listos para el an谩lisis.

Actualmente, el pipeline procesa los datos desde un Bucket en Cloud Storage, almacen谩ndolos en tablas temporales en BigQuery antes de realizar transformaciones adicionales y cargar los datos en tablas finales semanal o diariamente.

## Arquitectura y Tecnolog铆as utilizada.  
![Arquitectura del pipeline](../../assets/Images/arquitectura_pipeline.png)

- **`Google Cloud Platform (GCP)`**:
  - **BigQuery** para el almacenamiento y procesamiento de datos estructurado.
  - **Google Cloud Storage** para almacenar los archivos fuente sin estructurar.
  - **Google Cloud Build** para sincronizar y automatizar los DAGs y las functions desde el repositorio hacia Composer.
  - **Google Composer** para administrar la arquitectura de Airflow.
  - **Google API Places** para carga incremental de nuevos datos.
- **`Apache Airflow`**: Orquestaci贸n del pipeline ETL.
- **`Python`**: Desarrollo de funciones auxiliares y transformaci贸n de datos.


## Estructura del Pipeline 
- **DAGs**:
  - `DAG_api.py`: Contiene la orquestaci贸n ETL de la carga incremental de nuevos datos desde API Places.
  - `DAG_yelp.py`: Contiene la orquestaci贸n de los datos de Yelp.
  - `DAG-Googl.py`: Contiene la orquestaci贸n de los datos de Google.
- **Functions**:
  - `extract_data_yelp.py`: Funciones para la extracci贸n de datos desde Google Cloud Storage para los archivos de Yelp.
  - `transform_data_yelp.py`: Gestiona las transformaciones espec铆ficas para cada archivo de datos, con funciones adaptadas para `checkin.json`y `tip.json`.
  - `load_data_yelp.py`: Incluye funciones para cargar los datos en BigQuery y gestionar las tablas de Yelp.
  - `google_bigquery.py`: Incluye funciones para la creaci贸n, eliminaci贸n y gesti贸n de las tablas en Bigquery de los datos de Google.
  - `desanidar_columnas.py`: Incluye funciones para el desanidado de las columnas que ser谩n nuevas tablas, en los datos de Google.
  - `etl_api.py`: Incluye funciones para la extracci贸n, transformaci贸n y carga incremental de nuevos datos desde la API Places de Google hacia nuevas tablas en BigQuery.
  

## Diagrama y flujo de los DAGs 

### **``Flujo Yelp DAG:``**
Este diagrama muestra c贸mo el flujo del DAG se adapta para diferentes archivos (checkin y tip) y c贸mo se asegura la modularidad y flexibilidad en el procesamiento con una carga incremental de archivos semanales.

- **Inicio**:
  - `inicio`: Un `DummyOperator` que marca el comienzo del DAG para facilitar la visualizaci贸n en Airflow.
- **Verificaci贸n de Archivos Procesados**:
  - `verificar_archivo_procesado`: Utiliza `BranchPythonOperator` para verificar si el archivo ya fue procesado consultando la tabla de control en BigQuery.
- **Creaci贸n de Tablas Temporales**:
  - `crear_tabla_temporal`: Crea tablas temporales en BigQuery para almacenar los datos con el esquema adecuado.
- **Extracci贸n y Transformaci贸n de Datos**:
  - `cargar_archivo_en_tabla_temporal`: Extrae los archivos desde Google Cloud Storage (GCS), aplica transformaciones necesarias y carga los datos en las tablas temporales en BigQuery.
- **Transformaci贸n en BigQuery y Carga en Tablas Finales**:
  - `transformar_checkin` y `transformar_tip`: Transforman los datos en las tablas temporales y los cargan en las tablas finales `checkin_yelp` y `tip_yelp`.
- **Eliminaci贸n de Tablas Temporales**:
  - `eliminar_tabla_temporal`: Elimina las tablas temporales despu茅s de cargar los datos en las tablas finales para optimizar el uso de recursos.
- **Registro en la Tabla de Control**:
  - `registrar_archivo_procesado`: Registra el archivo como procesado en la tabla de control para evitar reprocesamiento.
- **Fin**:
  - `fin_checkin` y `fin_tip`: `DummyOperator` que marca el final de cada flujo para `checkin.json` y `tip.json`.

![Diagrama Yelp DAG](../../assets/Images/dag_yelp.png)

### **``Flujo Google DAG:``**
Este diagrama muestra c贸mo se desanidan los datos en distintas tablas estructuradas y se aplican normalizaciones en tablas temporales para trasladarlo limpio a la tabla original cada semana.

![Diagrama Google DAG]()

### **``Flujo API Incremental DAG:``**
vender un poco de humo y especificar que es automatico cada tanto tiempo (ejemplo cada mes). Insertar imagen tambien

![Diagrama API DAG]()

## Funcionalidades y Detalles T茅cnicos 锔

- **Control de Archivos ya procesados**: El pipeline verifica si cada archivo ha sido procesado anteriormente consultando una tabla de control en BigQuery. Esto evita procesamientos duplicados de archivos hacia las tablas.
- **Modularizaci贸n del C贸digo**: Las funciones de extracci贸n, transformaci贸n y carga est谩n organizadas en m贸dulos independientes para mejorar la claridad y la reutilizaci贸n del c贸digo.
- **Diccionario de Transformaciones**: En `transform_data_yelp.py`, el diccionario `transformaciones` asocia cada archivo con su respectiva funci贸n de transformaci贸n, facilitando la extensi贸n del pipeline a otros archivos. Solo se necesita crear una funci贸n de transformaci贸n y a帽adirla al diccionario.
- **Documentaci贸n de funciones y tareas**: Cada funci贸n y tarea dentro del DAG est谩 documentada utilizando docstrings conforme a las buenas pr谩cticas de PEP 8, facilitando su comprensi贸n y mantenimiento.
- **Carga incremental automatica**: Realiza la carga de datos de forma automatizada, ejecut谩ndose semanal y mensualmente sin intervenci贸n manual.

## Pr贸ximos Pasos 

- **Optimizaci贸n del Procesamiento de Nuevas Versiones de Archivos**:
  - Implementar una estrategia para detectar cambios en archivos existentes en el bucket de GCS, permitiendo la actualizaci贸n de versiones de los datasets.
- **Automatizaci贸n para Otros Datasets**:
  - Extender el pipeline para procesar otros archivos de Yelp y Google, agregando sus transformaciones espec铆ficas al diccionario `transformaciones`.
- **Monitoreo y Alertas**:
  - Implementar alertas y notificaciones en caso de fallos en alguna de las tareas del DAG, utilizando los servicios de Airflow y GCP.

## Posibles Mejoras y Consideraciones Futuras 

- **Optimizaci贸n de Transformaciones en BigQuery**:
  - A medida que se agreguen m谩s datos, puede ser necesario optimizar las consultas de transformaci贸n en BigQuery para reducir costos y tiempo de procesamiento.
- **Versionado de C贸digo y Datos**:
  - Considerar el uso de herramientas de control de versiones para el c贸digo y el manejo de versiones de datasets en GCS para mantener un historial claro y facilitar el rollback.

## Notas Finales 

Este markdown documenta el progreso actual del pipeline y proporciona una gu铆a sobre la arquitectura, el flujo de trabajo y las consideraciones clave. Se actualizar谩 para reflejar nuevos desarrollos y decisiones de dise帽o a medida que avancemos en el proyecto.

