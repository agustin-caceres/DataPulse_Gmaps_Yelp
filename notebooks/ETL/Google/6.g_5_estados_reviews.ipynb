{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que haremos aquí es recopilar los datos de las reviews de los estados que utilizaremos en el análisis. Los archivos en crudo se presentaron en formato Json, en archivos individuales para cada uno de los 51 estados de EEUU, por lo que procederemos a ordenar los mismos, en formato parquet, y por la dimension, en dataframes individuales, para luego crear una tabla única para los 5 estados pertinentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New York"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se convierten los archivos Json a formato parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convertido: 1.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE_parquet\\Rev-New_York.parquet\\1.parquet\n",
      "Convertido: 10.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE_parquet\\Rev-New_York.parquet\\10.parquet\n",
      "Convertido: 11.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE_parquet\\Rev-New_York.parquet\\11.parquet\n",
      "Convertido: 12.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE_parquet\\Rev-New_York.parquet\\12.parquet\n",
      "Convertido: 13.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE_parquet\\Rev-New_York.parquet\\13.parquet\n",
      "Convertido: 14.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE_parquet\\Rev-New_York.parquet\\14.parquet\n",
      "Convertido: 15.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE_parquet\\Rev-New_York.parquet\\15.parquet\n",
      "Convertido: 16.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE_parquet\\Rev-New_York.parquet\\16.parquet\n",
      "Convertido: 17.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE_parquet\\Rev-New_York.parquet\\17.parquet\n",
      "Convertido: 18.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE_parquet\\Rev-New_York.parquet\\18.parquet\n",
      "Convertido: 2.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE_parquet\\Rev-New_York.parquet\\2.parquet\n",
      "Convertido: 3.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE_parquet\\Rev-New_York.parquet\\3.parquet\n",
      "Convertido: 4.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE_parquet\\Rev-New_York.parquet\\4.parquet\n",
      "Convertido: 5.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE_parquet\\Rev-New_York.parquet\\5.parquet\n",
      "Convertido: 6.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE_parquet\\Rev-New_York.parquet\\6.parquet\n",
      "Convertido: 7.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE_parquet\\Rev-New_York.parquet\\7.parquet\n",
      "Convertido: 8.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE_parquet\\Rev-New_York.parquet\\8.parquet\n",
      "Convertido: 9.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE_parquet\\Rev-New_York.parquet\\9.parquet\n",
      "Conversión completada.\n"
     ]
    }
   ],
   "source": [
    "# Carpeta de entrada que contiene los archivos JSON\n",
    "input_folder = r\"C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE\\review-New_York\"\n",
    "# Carpeta de salida para guardar los archivos Parquet\n",
    "output_folder =r\"C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE_parquet\\Rev-New_York.parquet\"\n",
    "\n",
    "# Crear la carpeta de salida si no existe\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterar sobre todos los archivos en la carpeta de entrada\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.json'):\n",
    "        # Ruta completa del archivo JSON\n",
    "        json_file_path = os.path.join(input_folder, filename)\n",
    "        \n",
    "        # Leer el archivo JSON\n",
    "        try:\n",
    "            df = pd.read_json(json_file_path, lines=True)\n",
    "            \n",
    "            # Crear nombre de archivo Parquet (sin la extensión .json)\n",
    "            parquet_file_path = os.path.join(output_folder, filename.replace('.json', '.parquet'))\n",
    "            \n",
    "            # Guardar el DataFrame como archivo Parquet\n",
    "            df.to_parquet(parquet_file_path, index=False, compression='snappy', engine='pyarrow')\n",
    "            \n",
    "            print(f\"Convertido: {filename} a {parquet_file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al convertir {filename}: {e}\")\n",
    "\n",
    "print(\"Conversión completada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2700000 entries, 0 to 2699999\n",
      "Data columns (total 8 columns):\n",
      " #   Column   Dtype  \n",
      "---  ------   -----  \n",
      " 0   user_id  float64\n",
      " 1   name     object \n",
      " 2   time     int64  \n",
      " 3   rating   int64  \n",
      " 4   text     object \n",
      " 5   pics     object \n",
      " 6   resp     object \n",
      " 7   gmap_id  object \n",
      "dtypes: float64(1), int64(2), object(5)\n",
      "memory usage: 164.8+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Leer el archivo Parquet\n",
    "Rev_New_York_1 = pd.read_parquet(r\"C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE_parquet\\Rev-New_York.parquet\")\n",
    "\n",
    "print(Rev_New_York_1.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se concatenan los archivos parquet para formar un dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2700000, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function print(*args, sep=' ', end='\\n', file=None, flush=False)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carpeta que contiene los archivos Parquet\n",
    "input_folder_parquet = r\"C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE_parquet\\Rev-New_York.parquet\"\n",
    "\n",
    "# Lista para almacenar DataFrames\n",
    "df_list_New_York = []\n",
    "\n",
    "# Iterar sobre todos los archivos Parquet en la carpeta de entrada\n",
    "for filename in os.listdir(input_folder_parquet):\n",
    "    if filename.endswith('.parquet'):\n",
    "        # Ruta completa del archivo Parquet\n",
    "        parquet_file_path_ny = os.path.join(input_folder_parquet, filename)\n",
    "        \n",
    "        # Leer el archivo Parquet y añadir el DataFrame a la lista\n",
    "        df = pd.read_parquet(parquet_file_path_ny)\n",
    "        df_list_New_York.append(df)\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_new_york = pd.concat(df_list_New_York, ignore_index=True)\n",
    "\n",
    "# Verificar el DataFrame combinado\n",
    "print(df_new_york.shape)\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se elimina columna irrelevante para el análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_york = df_new_york.drop(columns=['pics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se convierten las columnas necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1.1372210469230823e+20\n",
      "1    1.0729344149210933e+20\n",
      "2     1.003785858018194e+20\n",
      "Name: user_id, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Se convierte la columna 'user_id' a tipo object\n",
    "df_new_york['user_id'] = df_new_york['user_id'].astype('str')\n",
    "print(df_new_york['user_id'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2020-10-23\n",
      "1   2021-05-04\n",
      "2   2020-08-14\n",
      "Name: time, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# Se convierte la columna 'time' a datetime y luego a solo fecha manteniendo tipo datetime\n",
    "df_new_york['time'] = pd.to_datetime(df_new_york['time'], unit='ms').dt.normalize()\n",
    "print(df_new_york['time'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se normaliza la columna 'resp' para agregar su contenido como dos nuevas columnas a df_new_york\n",
    "resp_df_ny = pd.json_normalize(df_new_york['resp'])\n",
    "df_new_york[['resp_text', 'resp_time']] = resp_df_ny[['text', 'time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          NaT\n",
      "1   2021-05-06\n",
      "2          NaT\n",
      "Name: resp_time, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# Se convierte la columna 'resp_time' a datetime y luego a solo fecha manteniendo tipo datetime\n",
    "df_new_york['resp_time'] = pd.to_datetime(df_new_york['resp_time'], unit='ms').dt.normalize()\n",
    "print(df_new_york['resp_time'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se elimina la columna 'resp' original\n",
    "df_new_york = df_new_york.drop(columns=['resp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se renombra la columna gmap_id como 'id_negocio', para mayor comprensión en el análisis\n",
    "df_new_york = df_new_york.rename(columns={'gmap_id': 'id_negocio'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se reordena la columna 'id_negocio'\n",
    "columnas = list(df_new_york.columns)\n",
    "columnas.insert(0, columnas.pop(columnas.index('id_negocio')))\n",
    "df_new_york = df_new_york[columnas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se añade la columna 'review_id' autoincremental\n",
    "df_new_york.insert(0, 'review_id', range(1, 1 + len(df_new_york)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se añade la columna 'Estado'\n",
    "df_new_york.insert(2, 'estado', 'New York')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2700000 entries, 0 to 2699999\n",
      "Data columns (total 10 columns):\n",
      " #   Column      Dtype         \n",
      "---  ------      -----         \n",
      " 0   review_id   int64         \n",
      " 1   id_negocio  object        \n",
      " 2   estado      object        \n",
      " 3   user_id     object        \n",
      " 4   name        object        \n",
      " 5   time        datetime64[ns]\n",
      " 6   rating      int64         \n",
      " 7   text        object        \n",
      " 8   resp_text   object        \n",
      " 9   resp_time   datetime64[ns]\n",
      "dtypes: datetime64[ns](2), int64(2), object(6)\n",
      "memory usage: 206.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Se verifica como queda el dataframe\n",
    "print(df_new_york.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>id_negocio</th>\n",
       "      <th>estado</th>\n",
       "      <th>user_id</th>\n",
       "      <th>name</th>\n",
       "      <th>time</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>resp_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0x89c25fc9494dce47:0x6d63c807b59a55</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.1372210469230823e+20</td>\n",
       "      <td>Alvin Martinez</td>\n",
       "      <td>2020-10-23</td>\n",
       "      <td>5</td>\n",
       "      <td>I'm late to posting this but this store especi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0x89c25fc9494dce47:0x6d63c807b59a55</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.0729344149210933e+20</td>\n",
       "      <td>Johnnie Jackson</td>\n",
       "      <td>2021-05-04</td>\n",
       "      <td>1</td>\n",
       "      <td>Very dissatisfied I did not get my phone the p...</td>\n",
       "      <td>We pride ourselves on providing an awesome exp...</td>\n",
       "      <td>2021-05-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0x89c25fc9494dce47:0x6d63c807b59a55</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.003785858018194e+20</td>\n",
       "      <td>Manie Blazer</td>\n",
       "      <td>2020-08-14</td>\n",
       "      <td>5</td>\n",
       "      <td>Excellent very well done with professional car...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0x89c25fc9494dce47:0x6d63c807b59a55</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.1499816115301982e+20</td>\n",
       "      <td>Fashion Fiinds</td>\n",
       "      <td>2018-12-02</td>\n",
       "      <td>5</td>\n",
       "      <td>Basing my review strictly on the service I rec...</td>\n",
       "      <td>Thanks for the awesome review!  We work hard t...</td>\n",
       "      <td>2018-12-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0x89c25fc9494dce47:0x6d63c807b59a55</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.171781857284223e+20</td>\n",
       "      <td>Andres Rieloff</td>\n",
       "      <td>2020-08-13</td>\n",
       "      <td>1</td>\n",
       "      <td>Bad! Disorganized. I'm being totally honest. I...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699995</th>\n",
       "      <td>2699996</td>\n",
       "      <td>0x89de0b8a0905153d:0x976fc4a006084f03</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.1508884569871743e+20</td>\n",
       "      <td>Gourav Saha</td>\n",
       "      <td>2019-04-16</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699996</th>\n",
       "      <td>2699997</td>\n",
       "      <td>0x89de0b8a0905153d:0x976fc4a006084f03</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.1170639723279111e+20</td>\n",
       "      <td>jenn mosher</td>\n",
       "      <td>2016-05-11</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699997</th>\n",
       "      <td>2699998</td>\n",
       "      <td>0x89de0b8a0905153d:0x976fc4a006084f03</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.0836978407597677e+20</td>\n",
       "      <td>Michele Huck</td>\n",
       "      <td>2019-11-16</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699998</th>\n",
       "      <td>2699999</td>\n",
       "      <td>0x89de0b8a0905153d:0x976fc4a006084f03</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.0078678731635175e+20</td>\n",
       "      <td>Frank B</td>\n",
       "      <td>2020-10-27</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699999</th>\n",
       "      <td>2700000</td>\n",
       "      <td>0x89de0b8a0905153d:0x976fc4a006084f03</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.0419091514708658e+20</td>\n",
       "      <td>Vincent Viscanti</td>\n",
       "      <td>2019-06-29</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2700000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         review_id                             id_negocio    estado  \\\n",
       "0                1    0x89c25fc9494dce47:0x6d63c807b59a55  New York   \n",
       "1                2    0x89c25fc9494dce47:0x6d63c807b59a55  New York   \n",
       "2                3    0x89c25fc9494dce47:0x6d63c807b59a55  New York   \n",
       "3                4    0x89c25fc9494dce47:0x6d63c807b59a55  New York   \n",
       "4                5    0x89c25fc9494dce47:0x6d63c807b59a55  New York   \n",
       "...            ...                                    ...       ...   \n",
       "2699995    2699996  0x89de0b8a0905153d:0x976fc4a006084f03  New York   \n",
       "2699996    2699997  0x89de0b8a0905153d:0x976fc4a006084f03  New York   \n",
       "2699997    2699998  0x89de0b8a0905153d:0x976fc4a006084f03  New York   \n",
       "2699998    2699999  0x89de0b8a0905153d:0x976fc4a006084f03  New York   \n",
       "2699999    2700000  0x89de0b8a0905153d:0x976fc4a006084f03  New York   \n",
       "\n",
       "                        user_id              name       time  rating  \\\n",
       "0        1.1372210469230823e+20    Alvin Martinez 2020-10-23       5   \n",
       "1        1.0729344149210933e+20   Johnnie Jackson 2021-05-04       1   \n",
       "2         1.003785858018194e+20      Manie Blazer 2020-08-14       5   \n",
       "3        1.1499816115301982e+20    Fashion Fiinds 2018-12-02       5   \n",
       "4         1.171781857284223e+20    Andres Rieloff 2020-08-13       1   \n",
       "...                         ...               ...        ...     ...   \n",
       "2699995  1.1508884569871743e+20       Gourav Saha 2019-04-16       4   \n",
       "2699996  1.1170639723279111e+20       jenn mosher 2016-05-11       5   \n",
       "2699997  1.0836978407597677e+20      Michele Huck 2019-11-16       5   \n",
       "2699998  1.0078678731635175e+20           Frank B 2020-10-27       5   \n",
       "2699999  1.0419091514708658e+20  Vincent Viscanti 2019-06-29       5   \n",
       "\n",
       "                                                      text  \\\n",
       "0        I'm late to posting this but this store especi...   \n",
       "1        Very dissatisfied I did not get my phone the p...   \n",
       "2        Excellent very well done with professional car...   \n",
       "3        Basing my review strictly on the service I rec...   \n",
       "4        Bad! Disorganized. I'm being totally honest. I...   \n",
       "...                                                    ...   \n",
       "2699995                                               None   \n",
       "2699996                                               None   \n",
       "2699997                                               None   \n",
       "2699998                                               None   \n",
       "2699999                                               None   \n",
       "\n",
       "                                                 resp_text  resp_time  \n",
       "0                                                      NaN        NaT  \n",
       "1        We pride ourselves on providing an awesome exp... 2021-05-06  \n",
       "2                                                      NaN        NaT  \n",
       "3        Thanks for the awesome review!  We work hard t... 2018-12-03  \n",
       "4                                                      NaN        NaT  \n",
       "...                                                    ...        ...  \n",
       "2699995                                                NaN        NaT  \n",
       "2699996                                                NaN        NaT  \n",
       "2699997                                                NaN        NaT  \n",
       "2699998                                                NaN        NaT  \n",
       "2699999                                                NaN        NaT  \n",
       "\n",
       "[2700000 rows x 10 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new_york"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# California"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se pasan de formato Json a Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convertido: 1.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\OESTE.parquet\\Rev-California.parquet\\1.parquet\n",
      "Convertido: 10.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\OESTE.parquet\\Rev-California.parquet\\10.parquet\n",
      "Convertido: 11.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\OESTE.parquet\\Rev-California.parquet\\11.parquet\n",
      "Convertido: 12.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\OESTE.parquet\\Rev-California.parquet\\12.parquet\n",
      "Convertido: 13.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\OESTE.parquet\\Rev-California.parquet\\13.parquet\n",
      "Convertido: 14.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\OESTE.parquet\\Rev-California.parquet\\14.parquet\n",
      "Convertido: 15.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\OESTE.parquet\\Rev-California.parquet\\15.parquet\n",
      "Convertido: 16.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\OESTE.parquet\\Rev-California.parquet\\16.parquet\n",
      "Convertido: 17.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\OESTE.parquet\\Rev-California.parquet\\17.parquet\n",
      "Convertido: 18.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\OESTE.parquet\\Rev-California.parquet\\18.parquet\n",
      "Convertido: 2.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\OESTE.parquet\\Rev-California.parquet\\2.parquet\n",
      "Convertido: 3.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\OESTE.parquet\\Rev-California.parquet\\3.parquet\n",
      "Convertido: 4.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\OESTE.parquet\\Rev-California.parquet\\4.parquet\n",
      "Convertido: 5.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\OESTE.parquet\\Rev-California.parquet\\5.parquet\n",
      "Convertido: 6.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\OESTE.parquet\\Rev-California.parquet\\6.parquet\n",
      "Convertido: 7.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\OESTE.parquet\\Rev-California.parquet\\7.parquet\n",
      "Convertido: 8.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\OESTE.parquet\\Rev-California.parquet\\8.parquet\n",
      "Convertido: 9.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\OESTE.parquet\\Rev-California.parquet\\9.parquet\n",
      "Conversión completada.\n"
     ]
    }
   ],
   "source": [
    "# Carpeta de entrada que contiene los archivos JSON\n",
    "input_folder = r\"C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\OESTE\\review-California\"\n",
    "# Carpeta de salida para guardar los archivos Parquet\n",
    "output_folder = r\"C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\OESTE.parquet\\Rev-California.parquet\"\n",
    "\n",
    "# Crear la carpeta de salida si no existe\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterar sobre todos los archivos en la carpeta de entrada\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.json'):\n",
    "        # Ruta completa del archivo JSON\n",
    "        json_file_path = os.path.join(input_folder, filename)\n",
    "        \n",
    "        # Leer el archivo JSON\n",
    "        try:\n",
    "            df = pd.read_json(json_file_path, lines=True)\n",
    "            \n",
    "            # Crear nombre de archivo Parquet (sin la extensión .json)\n",
    "            parquet_file_path = os.path.join(output_folder, filename.replace('.json', '.parquet'))\n",
    "            \n",
    "            # Guardar el DataFrame como archivo Parquet\n",
    "            df.to_parquet(parquet_file_path, index=False, compression='snappy', engine='pyarrow')\n",
    "            \n",
    "            print(f\"Convertido: {filename} a {parquet_file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al convertir {filename}: {e}\")\n",
    "\n",
    "print(\"Conversión completada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2700000 entries, 0 to 2699999\n",
      "Data columns (total 8 columns):\n",
      " #   Column   Dtype  \n",
      "---  ------   -----  \n",
      " 0   user_id  float64\n",
      " 1   name     object \n",
      " 2   time     int64  \n",
      " 3   rating   int64  \n",
      " 4   text     object \n",
      " 5   pics     object \n",
      " 6   resp     object \n",
      " 7   gmap_id  object \n",
      "dtypes: float64(1), int64(2), object(5)\n",
      "memory usage: 164.8+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Leer el archivo Parquet\n",
    "Rev_California_1 = pd.read_parquet(r\"C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\OESTE.parquet\\Rev-California.parquet\")\n",
    "\n",
    "# Mostrar las primeras filas\n",
    "print(Rev_California_1.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se concatenan los archivos .parquet para formar un Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2700000, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function print(*args, sep=' ', end='\\n', file=None, flush=False)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carpeta que contiene los archivos Parquet\n",
    "input_folder_parquet = r\"C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\OESTE.parquet\\Rev-California.parquet\"\n",
    "\n",
    "# Lista para almacenar DataFrames\n",
    "df_list_California = []\n",
    "\n",
    "# Iterar sobre todos los archivos Parquet en la carpeta de entrada\n",
    "for filename in os.listdir(input_folder_parquet):\n",
    "    if filename.endswith('.parquet'):\n",
    "        # Ruta completa del archivo Parquet\n",
    "        parquet_file_path_nmex = os.path.join(input_folder_parquet, filename)\n",
    "        \n",
    "        # Leer el archivo Parquet y añadir el DataFrame a la lista\n",
    "        df = pd.read_parquet(parquet_file_path_nmex)\n",
    "        df_list_California.append(df)\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_california = pd.concat(df_list_California, ignore_index=True)\n",
    "\n",
    "# Verificar el DataFrame combinado\n",
    "print(df_california.shape)\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se elimina columna irrelevante para el análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_california = df_california.drop(columns=['pics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se convierten las columnas necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1.0899115226265579e+20\n",
      "1    1.1129032221979622e+20\n",
      "2    1.1264035744961195e+20\n",
      "Name: user_id, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Se convierte la columna 'user_id' a tipo object\n",
    "df_california['user_id'] = df_california['user_id'].astype('str')\n",
    "print(df_california['user_id'].head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2021-01-06\n",
      "1   2021-02-09\n",
      "2   2020-03-08\n",
      "Name: time, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# Se convierte la columna 'time' a datetime y luego a solo fecha manteniendo tipo datetime\n",
    "df_california['time'] = pd.to_datetime(df_california['time'], unit='ms').dt.normalize()\n",
    "print(df_california['time'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se normaliza la columna 'resp' para agregar su contenido como dos nuevas columnas a df_new_york\n",
    "resp_df_cal = pd.json_normalize(df_california['resp'])\n",
    "df_california[['resp_text', 'resp_time']] = resp_df_cal[['text', 'time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   NaT\n",
      "1   NaT\n",
      "2   NaT\n",
      "Name: resp_time, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# Se convierte la columna 'time' a datetime y luego a solo fecha manteniendo tipo datetime\n",
    "df_california['resp_time'] = pd.to_datetime(df_california['resp_time'], unit='ms').dt.normalize()\n",
    "print(df_california['resp_time'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se elimina la columna 'resp' original\n",
    "df_california = df_california.drop(columns=['resp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se renombra la columna gmap_id como 'id_negocio', para mayor comprensión en el análisis\n",
    "df_california = df_california.rename(columns={'gmap_id': 'id_negocio'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se reordena la columna 'id_negocio'\n",
    "columnas = list(df_california.columns)\n",
    "columnas.insert(0, columnas.pop(columnas.index('id_negocio')))\n",
    "df_california = df_california[columnas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se añade la columna 'review_id' autoincremental desde 2700001 para continuar desde el df_new_york\n",
    "df_california.insert(0, 'review_id', range(2700001, 2700001 + len(df_california)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se añade la columna 'Estado'\n",
    "df_california.insert(2, 'estado', 'California')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2700000 entries, 0 to 2699999\n",
      "Data columns (total 10 columns):\n",
      " #   Column      Dtype         \n",
      "---  ------      -----         \n",
      " 0   review_id   int64         \n",
      " 1   id_negocio  object        \n",
      " 2   estado      object        \n",
      " 3   user_id     object        \n",
      " 4   name        object        \n",
      " 5   time        datetime64[ns]\n",
      " 6   rating      int64         \n",
      " 7   text        object        \n",
      " 8   resp_text   object        \n",
      " 9   resp_time   datetime64[ns]\n",
      "dtypes: datetime64[ns](2), int64(2), object(6)\n",
      "memory usage: 206.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Se verifica como queda el dataframe\n",
    "print(df_california.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>id_negocio</th>\n",
       "      <th>estado</th>\n",
       "      <th>user_id</th>\n",
       "      <th>name</th>\n",
       "      <th>time</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>resp_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2700001</td>\n",
       "      <td>0x80c2c778e3b73d33:0xbdc58662a4a97d49</td>\n",
       "      <td>California</td>\n",
       "      <td>1.0899115226265579e+20</td>\n",
       "      <td>Song Ro</td>\n",
       "      <td>2021-01-06</td>\n",
       "      <td>5</td>\n",
       "      <td>Love there korean rice cake.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2700002</td>\n",
       "      <td>0x80c2c778e3b73d33:0xbdc58662a4a97d49</td>\n",
       "      <td>California</td>\n",
       "      <td>1.1129032221979622e+20</td>\n",
       "      <td>Rafa Robles</td>\n",
       "      <td>2021-02-09</td>\n",
       "      <td>5</td>\n",
       "      <td>Good very good</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2700003</td>\n",
       "      <td>0x80c2c778e3b73d33:0xbdc58662a4a97d49</td>\n",
       "      <td>California</td>\n",
       "      <td>1.1264035744961195e+20</td>\n",
       "      <td>David Han</td>\n",
       "      <td>2020-03-08</td>\n",
       "      <td>4</td>\n",
       "      <td>They make Korean traditional food very properly.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2700004</td>\n",
       "      <td>0x80c2c778e3b73d33:0xbdc58662a4a97d49</td>\n",
       "      <td>California</td>\n",
       "      <td>1.1744034972382365e+20</td>\n",
       "      <td>Anthony Kim</td>\n",
       "      <td>2019-03-07</td>\n",
       "      <td>5</td>\n",
       "      <td>Short ribs are very delicious.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2700005</td>\n",
       "      <td>0x80c2c778e3b73d33:0xbdc58662a4a97d49</td>\n",
       "      <td>California</td>\n",
       "      <td>1.0058077083612353e+20</td>\n",
       "      <td>Mario Marzouk</td>\n",
       "      <td>2017-05-16</td>\n",
       "      <td>5</td>\n",
       "      <td>Great food and prices the portions are large</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699995</th>\n",
       "      <td>5399996</td>\n",
       "      <td>0x80c2bea30829f279:0x39aa953ee93734ed</td>\n",
       "      <td>California</td>\n",
       "      <td>1.1134233708701893e+20</td>\n",
       "      <td>Byunguk Kim</td>\n",
       "      <td>2019-11-10</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699996</th>\n",
       "      <td>5399997</td>\n",
       "      <td>0x8094675073616747:0x9f935a9b9046a9ba</td>\n",
       "      <td>California</td>\n",
       "      <td>1.080818202512737e+20</td>\n",
       "      <td>Ceveda Craytonhooks</td>\n",
       "      <td>2020-02-02</td>\n",
       "      <td>5</td>\n",
       "      <td>Beautiful salon. The staff and atmosphere were...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699997</th>\n",
       "      <td>5399998</td>\n",
       "      <td>0x8094675073616747:0x9f935a9b9046a9ba</td>\n",
       "      <td>California</td>\n",
       "      <td>1.1759138313533325e+20</td>\n",
       "      <td>Natacha Thompson</td>\n",
       "      <td>2018-06-10</td>\n",
       "      <td>5</td>\n",
       "      <td>Flo is a great beautician.   She is very patie...</td>\n",
       "      <td>Thank you!  It was my pleasure to give you exa...</td>\n",
       "      <td>2018-06-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699998</th>\n",
       "      <td>5399999</td>\n",
       "      <td>0x8094675073616747:0x9f935a9b9046a9ba</td>\n",
       "      <td>California</td>\n",
       "      <td>1.1242661065579289e+20</td>\n",
       "      <td>Eleanor Aikins</td>\n",
       "      <td>2019-02-26</td>\n",
       "      <td>5</td>\n",
       "      <td>I Been going to have a variety of hair style a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699999</th>\n",
       "      <td>5400000</td>\n",
       "      <td>0x8094675073616747:0x9f935a9b9046a9ba</td>\n",
       "      <td>California</td>\n",
       "      <td>1.1221994366642322e+20</td>\n",
       "      <td>Thai Love</td>\n",
       "      <td>2019-03-29</td>\n",
       "      <td>5</td>\n",
       "      <td>Teri is awesome she is a dread loc master. She...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2700000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         review_id                             id_negocio      estado  \\\n",
       "0          2700001  0x80c2c778e3b73d33:0xbdc58662a4a97d49  California   \n",
       "1          2700002  0x80c2c778e3b73d33:0xbdc58662a4a97d49  California   \n",
       "2          2700003  0x80c2c778e3b73d33:0xbdc58662a4a97d49  California   \n",
       "3          2700004  0x80c2c778e3b73d33:0xbdc58662a4a97d49  California   \n",
       "4          2700005  0x80c2c778e3b73d33:0xbdc58662a4a97d49  California   \n",
       "...            ...                                    ...         ...   \n",
       "2699995    5399996  0x80c2bea30829f279:0x39aa953ee93734ed  California   \n",
       "2699996    5399997  0x8094675073616747:0x9f935a9b9046a9ba  California   \n",
       "2699997    5399998  0x8094675073616747:0x9f935a9b9046a9ba  California   \n",
       "2699998    5399999  0x8094675073616747:0x9f935a9b9046a9ba  California   \n",
       "2699999    5400000  0x8094675073616747:0x9f935a9b9046a9ba  California   \n",
       "\n",
       "                        user_id                 name       time  rating  \\\n",
       "0        1.0899115226265579e+20              Song Ro 2021-01-06       5   \n",
       "1        1.1129032221979622e+20          Rafa Robles 2021-02-09       5   \n",
       "2        1.1264035744961195e+20            David Han 2020-03-08       4   \n",
       "3        1.1744034972382365e+20          Anthony Kim 2019-03-07       5   \n",
       "4        1.0058077083612353e+20        Mario Marzouk 2017-05-16       5   \n",
       "...                         ...                  ...        ...     ...   \n",
       "2699995  1.1134233708701893e+20          Byunguk Kim 2019-11-10       5   \n",
       "2699996   1.080818202512737e+20  Ceveda Craytonhooks 2020-02-02       5   \n",
       "2699997  1.1759138313533325e+20     Natacha Thompson 2018-06-10       5   \n",
       "2699998  1.1242661065579289e+20       Eleanor Aikins 2019-02-26       5   \n",
       "2699999  1.1221994366642322e+20            Thai Love 2019-03-29       5   \n",
       "\n",
       "                                                      text  \\\n",
       "0                             Love there korean rice cake.   \n",
       "1                                           Good very good   \n",
       "2         They make Korean traditional food very properly.   \n",
       "3                           Short ribs are very delicious.   \n",
       "4             Great food and prices the portions are large   \n",
       "...                                                    ...   \n",
       "2699995                                               None   \n",
       "2699996  Beautiful salon. The staff and atmosphere were...   \n",
       "2699997  Flo is a great beautician.   She is very patie...   \n",
       "2699998  I Been going to have a variety of hair style a...   \n",
       "2699999  Teri is awesome she is a dread loc master. She...   \n",
       "\n",
       "                                                 resp_text  resp_time  \n",
       "0                                                      NaN        NaT  \n",
       "1                                                      NaN        NaT  \n",
       "2                                                      NaN        NaT  \n",
       "3                                                      NaN        NaT  \n",
       "4                                                      NaN        NaT  \n",
       "...                                                    ...        ...  \n",
       "2699995                                                NaN        NaT  \n",
       "2699996                                                NaN        NaT  \n",
       "2699997  Thank you!  It was my pleasure to give you exa... 2018-06-10  \n",
       "2699998                                                NaN        NaT  \n",
       "2699999                                                NaN        NaT  \n",
       "\n",
       "[2700000 rows x 10 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_california"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pennsylvania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se convierten los archivos Json a formato parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convertido: 1.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE_parquet\\Rev-Pennsylvania.parquet\\1.parquet\n",
      "Convertido: 10.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE_parquet\\Rev-Pennsylvania.parquet\\10.parquet\n",
      "Convertido: 11.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE_parquet\\Rev-Pennsylvania.parquet\\11.parquet\n",
      "Convertido: 12.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE_parquet\\Rev-Pennsylvania.parquet\\12.parquet\n",
      "Convertido: 13.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE_parquet\\Rev-Pennsylvania.parquet\\13.parquet\n",
      "Convertido: 14.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE_parquet\\Rev-Pennsylvania.parquet\\14.parquet\n",
      "Convertido: 15.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE_parquet\\Rev-Pennsylvania.parquet\\15.parquet\n",
      "Convertido: 16.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE_parquet\\Rev-Pennsylvania.parquet\\16.parquet\n",
      "Convertido: 2.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE_parquet\\Rev-Pennsylvania.parquet\\2.parquet\n",
      "Convertido: 3.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE_parquet\\Rev-Pennsylvania.parquet\\3.parquet\n",
      "Convertido: 4.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE_parquet\\Rev-Pennsylvania.parquet\\4.parquet\n",
      "Convertido: 5.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE_parquet\\Rev-Pennsylvania.parquet\\5.parquet\n",
      "Convertido: 6.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE_parquet\\Rev-Pennsylvania.parquet\\6.parquet\n",
      "Convertido: 7.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE_parquet\\Rev-Pennsylvania.parquet\\7.parquet\n",
      "Convertido: 8.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE_parquet\\Rev-Pennsylvania.parquet\\8.parquet\n",
      "Convertido: 9.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE_parquet\\Rev-Pennsylvania.parquet\\9.parquet\n",
      "Conversión completada.\n"
     ]
    }
   ],
   "source": [
    "# Carpeta de entrada que contiene los archivos JSON\n",
    "input_folder = r\"C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE\\review-Pennsylvania\"\n",
    "# Carpeta de salida para guardar los archivos Parquet\n",
    "output_folder = r\"C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE_parquet\\Rev-Pennsylvania.parquet\"\n",
    "\n",
    "# Crear la carpeta de salida si no existe\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterar sobre todos los archivos en la carpeta de entrada\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.json'):\n",
    "        # Ruta completa del archivo JSON\n",
    "        json_file_path = os.path.join(input_folder, filename)\n",
    "        \n",
    "        # Leer el archivo JSON\n",
    "        try:\n",
    "            df = pd.read_json(json_file_path, lines=True)\n",
    "            \n",
    "            # Crear nombre de archivo Parquet (sin la extensión .json)\n",
    "            parquet_file_path = os.path.join(output_folder, filename.replace('.json', '.parquet'))\n",
    "            \n",
    "            # Guardar el DataFrame como archivo Parquet\n",
    "            df.to_parquet(parquet_file_path, index=False, compression='snappy', engine='pyarrow')\n",
    "            \n",
    "            print(f\"Convertido: {filename} a {parquet_file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al convertir {filename}: {e}\")\n",
    "\n",
    "print(\"Conversión completada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2400000 entries, 0 to 2399999\n",
      "Data columns (total 8 columns):\n",
      " #   Column   Dtype  \n",
      "---  ------   -----  \n",
      " 0   user_id  float64\n",
      " 1   name     object \n",
      " 2   time     int64  \n",
      " 3   rating   int64  \n",
      " 4   text     object \n",
      " 5   pics     object \n",
      " 6   resp     object \n",
      " 7   gmap_id  object \n",
      "dtypes: float64(1), int64(2), object(5)\n",
      "memory usage: 146.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Leer el archivo Parquet\n",
    "Rev_Pennsylvania_1 = pd.read_parquet(r\"C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE_parquet\\Rev-Pennsylvania.parquet\")\n",
    "\n",
    "print(Rev_Pennsylvania_1.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se concatenan los archivos parquet para formar un solo dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400000, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function print(*args, sep=' ', end='\\n', file=None, flush=False)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carpeta que contiene los archivos Parquet\n",
    "input_folder_parquet = r\"C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\NORESTE_parquet\\Rev-Pennsylvania.parquet\"\n",
    "\n",
    "# Lista para almacenar DataFrames\n",
    "df_list_Pennsylvania = []\n",
    "\n",
    "# Iterar sobre todos los archivos Parquet en la carpeta de entrada\n",
    "for filename in os.listdir(input_folder_parquet):\n",
    "    if filename.endswith('.parquet'):\n",
    "        # Ruta completa del archivo Parquet\n",
    "        parquet_file_path_psv = os.path.join(input_folder_parquet, filename)\n",
    "        \n",
    "        # Leer el archivo Parquet y añadir el DataFrame a la lista\n",
    "        df = pd.read_parquet(parquet_file_path_psv)\n",
    "        df_list_Pennsylvania.append(df)\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_pennsylvania = pd.concat(df_list_Pennsylvania, ignore_index=True)\n",
    "\n",
    "# Verificar el DataFrame combinado\n",
    "print(df_pennsylvania.shape)\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se elimina columna irrelevante para el análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pennsylvania = df_pennsylvania.drop(columns=['pics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se convierten las columnas necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1.0494474255907976e+20\n",
      "1    1.1760970283298033e+20\n",
      "2    1.1056324201842663e+20\n",
      "Name: user_id, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Se convierte la columna 'user_id' a tipo object\n",
    "df_pennsylvania['user_id'] = df_pennsylvania['user_id'].astype('str')\n",
    "print(df_pennsylvania['user_id'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2018-02-04\n",
      "1   2016-10-12\n",
      "2   2012-06-04\n",
      "Name: time, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# Se convierte la columna 'time' a datetime y luego a solo fecha manteniendo tipo datetime\n",
    "df_pennsylvania['time'] = pd.to_datetime(df_pennsylvania['time'], unit='ms').dt.normalize()\n",
    "print(df_pennsylvania['time'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se normaliza la columna 'resp' para agregar su contenido como dos nuevas columnas a df_new_york\n",
    "resp_df_pen = pd.json_normalize(df_pennsylvania['resp'])\n",
    "df_pennsylvania[['resp_text', 'resp_time']] = resp_df_pen[['text', 'time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   NaT\n",
      "1   NaT\n",
      "2   NaT\n",
      "Name: resp_time, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# Se convierte la columna 'resp_time' a datetime y luego a solo fecha manteniendo tipo datetime\n",
    "df_pennsylvania['resp_time'] = pd.to_datetime(df_pennsylvania['resp_time'], unit='ms').dt.normalize()\n",
    "print(df_pennsylvania['resp_time'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se elimina la columna 'resp' original\n",
    "df_pennsylvania = df_pennsylvania.drop(columns=['resp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se renombra la columna gmap_id como 'id_negocio', para mayor comprensión en el análisis\n",
    "df_pennsylvania = df_pennsylvania.rename(columns={'gmap_id': 'id_negocio'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se reordena la columna 'id_negocio'\n",
    "columnas = list(df_pennsylvania.columns)\n",
    "columnas.insert(0, columnas.pop(columnas.index('id_negocio')))\n",
    "df_pennsylvania = df_pennsylvania[columnas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se añade la columna 'review_id' autoincremental desde 4500001 para continuar desde el df_california\n",
    "df_pennsylvania.insert(0, 'review_id', range(4500001, 4500001 + len(df_pennsylvania)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se añade la columna 'Estado'\n",
    "df_pennsylvania.insert(2, 'estado', 'Pennsylvania')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2400000 entries, 0 to 2399999\n",
      "Data columns (total 10 columns):\n",
      " #   Column      Dtype         \n",
      "---  ------      -----         \n",
      " 0   review_id   int64         \n",
      " 1   id_negocio  object        \n",
      " 2   estado      object        \n",
      " 3   user_id     object        \n",
      " 4   name        object        \n",
      " 5   time        datetime64[ns]\n",
      " 6   rating      int64         \n",
      " 7   text        object        \n",
      " 8   resp_text   object        \n",
      " 9   resp_time   datetime64[ns]\n",
      "dtypes: datetime64[ns](2), int64(2), object(6)\n",
      "memory usage: 183.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Se verifica como queda el dataframe\n",
    "print(df_pennsylvania.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>id_negocio</th>\n",
       "      <th>estado</th>\n",
       "      <th>user_id</th>\n",
       "      <th>name</th>\n",
       "      <th>time</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>resp_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4500001</td>\n",
       "      <td>0x89c6c63c8cd87141:0x54d0d283872eecbb</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>1.0494474255907976e+20</td>\n",
       "      <td>Jaron Whitfield</td>\n",
       "      <td>2018-02-04</td>\n",
       "      <td>5</td>\n",
       "      <td>Joe is quite unique of his line of work, he as...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4500002</td>\n",
       "      <td>0x89c6c63c8cd87141:0x54d0d283872eecbb</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>1.1760970283298033e+20</td>\n",
       "      <td>Jonathan McCarthy</td>\n",
       "      <td>2016-10-12</td>\n",
       "      <td>5</td>\n",
       "      <td>For such a small place their impact on my life...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4500003</td>\n",
       "      <td>0x89c6c63c8cd87141:0x54d0d283872eecbb</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>1.1056324201842663e+20</td>\n",
       "      <td>Rocky Kev</td>\n",
       "      <td>2012-06-04</td>\n",
       "      <td>5</td>\n",
       "      <td>I usually give them a call before I stop by to...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4500004</td>\n",
       "      <td>0x89c6c63c8cd87141:0x54d0d283872eecbb</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>1.128959735054014e+20</td>\n",
       "      <td>Josep Valls</td>\n",
       "      <td>2013-03-14</td>\n",
       "      <td>5</td>\n",
       "      <td>My bike had been sitting outdoors for a good w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4500005</td>\n",
       "      <td>0x89c6c63c8cd87141:0x54d0d283872eecbb</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>1.1061967488596383e+20</td>\n",
       "      <td>Timaree Schmit</td>\n",
       "      <td>2019-01-29</td>\n",
       "      <td>5</td>\n",
       "      <td>Always an easy experience. Service is knowledg...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399995</th>\n",
       "      <td>6899996</td>\n",
       "      <td>0x89cb5b2714dfeecf:0xe4f25ae9daa774ff</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>1.091213518597228e+20</td>\n",
       "      <td>Aileen Blas</td>\n",
       "      <td>2021-04-05</td>\n",
       "      <td>5</td>\n",
       "      <td>Great cookies, wonderful staff.  Very enjoyabl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399996</th>\n",
       "      <td>6899997</td>\n",
       "      <td>0x89cb5b2714dfeecf:0xe4f25ae9daa774ff</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>1.0787372814043614e+20</td>\n",
       "      <td>Joshua Juda</td>\n",
       "      <td>2020-10-29</td>\n",
       "      <td>4</td>\n",
       "      <td>Good fresh baked cookies. Prices are a little ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399997</th>\n",
       "      <td>6899998</td>\n",
       "      <td>0x89cb5b2714dfeecf:0xe4f25ae9daa774ff</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>1.0722323937392198e+20</td>\n",
       "      <td>Anthony Casto</td>\n",
       "      <td>2021-03-01</td>\n",
       "      <td>5</td>\n",
       "      <td>Best cookies ever</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399998</th>\n",
       "      <td>6899999</td>\n",
       "      <td>0x89cb5b2714dfeecf:0xe4f25ae9daa774ff</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>1.0584293034572984e+20</td>\n",
       "      <td>Mark McKillop</td>\n",
       "      <td>2021-01-19</td>\n",
       "      <td>5</td>\n",
       "      <td>Great place for a snack. Open really late and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399999</th>\n",
       "      <td>6900000</td>\n",
       "      <td>0x89cb5b2714dfeecf:0xe4f25ae9daa774ff</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>1.0131873984468684e+20</td>\n",
       "      <td>Shawn Smith</td>\n",
       "      <td>2020-02-23</td>\n",
       "      <td>5</td>\n",
       "      <td>Who doesn't love a walk in cookie store that h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2400000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         review_id                             id_negocio        estado  \\\n",
       "0          4500001  0x89c6c63c8cd87141:0x54d0d283872eecbb  Pennsylvania   \n",
       "1          4500002  0x89c6c63c8cd87141:0x54d0d283872eecbb  Pennsylvania   \n",
       "2          4500003  0x89c6c63c8cd87141:0x54d0d283872eecbb  Pennsylvania   \n",
       "3          4500004  0x89c6c63c8cd87141:0x54d0d283872eecbb  Pennsylvania   \n",
       "4          4500005  0x89c6c63c8cd87141:0x54d0d283872eecbb  Pennsylvania   \n",
       "...            ...                                    ...           ...   \n",
       "2399995    6899996  0x89cb5b2714dfeecf:0xe4f25ae9daa774ff  Pennsylvania   \n",
       "2399996    6899997  0x89cb5b2714dfeecf:0xe4f25ae9daa774ff  Pennsylvania   \n",
       "2399997    6899998  0x89cb5b2714dfeecf:0xe4f25ae9daa774ff  Pennsylvania   \n",
       "2399998    6899999  0x89cb5b2714dfeecf:0xe4f25ae9daa774ff  Pennsylvania   \n",
       "2399999    6900000  0x89cb5b2714dfeecf:0xe4f25ae9daa774ff  Pennsylvania   \n",
       "\n",
       "                        user_id               name       time  rating  \\\n",
       "0        1.0494474255907976e+20    Jaron Whitfield 2018-02-04       5   \n",
       "1        1.1760970283298033e+20  Jonathan McCarthy 2016-10-12       5   \n",
       "2        1.1056324201842663e+20          Rocky Kev 2012-06-04       5   \n",
       "3         1.128959735054014e+20        Josep Valls 2013-03-14       5   \n",
       "4        1.1061967488596383e+20     Timaree Schmit 2019-01-29       5   \n",
       "...                         ...                ...        ...     ...   \n",
       "2399995   1.091213518597228e+20        Aileen Blas 2021-04-05       5   \n",
       "2399996  1.0787372814043614e+20        Joshua Juda 2020-10-29       4   \n",
       "2399997  1.0722323937392198e+20      Anthony Casto 2021-03-01       5   \n",
       "2399998  1.0584293034572984e+20      Mark McKillop 2021-01-19       5   \n",
       "2399999  1.0131873984468684e+20        Shawn Smith 2020-02-23       5   \n",
       "\n",
       "                                                      text resp_text resp_time  \n",
       "0        Joe is quite unique of his line of work, he as...       NaN       NaT  \n",
       "1        For such a small place their impact on my life...       NaN       NaT  \n",
       "2        I usually give them a call before I stop by to...       NaN       NaT  \n",
       "3        My bike had been sitting outdoors for a good w...       NaN       NaT  \n",
       "4        Always an easy experience. Service is knowledg...       NaN       NaT  \n",
       "...                                                    ...       ...       ...  \n",
       "2399995  Great cookies, wonderful staff.  Very enjoyabl...       NaN       NaT  \n",
       "2399996  Good fresh baked cookies. Prices are a little ...       NaN       NaT  \n",
       "2399997                                  Best cookies ever       NaN       NaT  \n",
       "2399998  Great place for a snack. Open really late and ...       NaN       NaT  \n",
       "2399999  Who doesn't love a walk in cookie store that h...       NaN       NaT  \n",
       "\n",
       "[2400000 rows x 10 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pennsylvania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Florida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se convierten los archivos Json a formato parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convertido: 1.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR_parquet\\Rev-Florida.parquet\\1.parquet\n",
      "Convertido: 10.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR_parquet\\Rev-Florida.parquet\\10.parquet\n",
      "Convertido: 11.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR_parquet\\Rev-Florida.parquet\\11.parquet\n",
      "Convertido: 12.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR_parquet\\Rev-Florida.parquet\\12.parquet\n",
      "Convertido: 13.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR_parquet\\Rev-Florida.parquet\\13.parquet\n",
      "Convertido: 14.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR_parquet\\Rev-Florida.parquet\\14.parquet\n",
      "Convertido: 15.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR_parquet\\Rev-Florida.parquet\\15.parquet\n",
      "Convertido: 16.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR_parquet\\Rev-Florida.parquet\\16.parquet\n",
      "Convertido: 17.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR_parquet\\Rev-Florida.parquet\\17.parquet\n",
      "Convertido: 18.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR_parquet\\Rev-Florida.parquet\\18.parquet\n",
      "Convertido: 19.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR_parquet\\Rev-Florida.parquet\\19.parquet\n",
      "Convertido: 2.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR_parquet\\Rev-Florida.parquet\\2.parquet\n",
      "Convertido: 3.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR_parquet\\Rev-Florida.parquet\\3.parquet\n",
      "Convertido: 4.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR_parquet\\Rev-Florida.parquet\\4.parquet\n",
      "Convertido: 5.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR_parquet\\Rev-Florida.parquet\\5.parquet\n",
      "Convertido: 6.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR_parquet\\Rev-Florida.parquet\\6.parquet\n",
      "Convertido: 7.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR_parquet\\Rev-Florida.parquet\\7.parquet\n",
      "Convertido: 8.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR_parquet\\Rev-Florida.parquet\\8.parquet\n",
      "Convertido: 9.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR_parquet\\Rev-Florida.parquet\\9.parquet\n",
      "Conversión completada.\n"
     ]
    }
   ],
   "source": [
    "# Carpeta de entrada que contiene los archivos JSON\n",
    "input_folder = r\"C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR\\review-Florida\"\n",
    "# Carpeta de salida para guardar los archivos Parquet\n",
    "output_folder = r\"C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR_parquet\\Rev-Florida.parquet\"\n",
    "\n",
    "# Crear la carpeta de salida si no existe\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterar sobre todos los archivos en la carpeta de entrada\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.json'):\n",
    "        # Ruta completa del archivo JSON\n",
    "        json_file_path = os.path.join(input_folder, filename)\n",
    "        \n",
    "        # Leer el archivo JSON\n",
    "        try:\n",
    "            df = pd.read_json(json_file_path, lines=True)\n",
    "            \n",
    "            # Crear nombre de archivo Parquet (sin la extensión .json)\n",
    "            parquet_file_path = os.path.join(output_folder, filename.replace('.json', '.parquet'))\n",
    "            \n",
    "            # Guardar el DataFrame como archivo Parquet\n",
    "            df.to_parquet(parquet_file_path, index=False, compression='snappy', engine='pyarrow')\n",
    "            \n",
    "            print(f\"Convertido: {filename} a {parquet_file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al convertir {filename}: {e}\")\n",
    "\n",
    "print(\"Conversión completada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2850000 entries, 0 to 2849999\n",
      "Data columns (total 8 columns):\n",
      " #   Column   Dtype  \n",
      "---  ------   -----  \n",
      " 0   user_id  float64\n",
      " 1   name     object \n",
      " 2   time     int64  \n",
      " 3   rating   int64  \n",
      " 4   text     object \n",
      " 5   pics     object \n",
      " 6   resp     object \n",
      " 7   gmap_id  object \n",
      "dtypes: float64(1), int64(2), object(5)\n",
      "memory usage: 174.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Leer el archivo Parquet\n",
    "Rev_Florida_1 = pd.read_parquet(r\"C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR_parquet\\Rev-Florida.parquet\")\n",
    "\n",
    "print(Rev_Florida_1.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se concatenan los archivos parquet para formar un solo dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2850000, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function print(*args, sep=' ', end='\\n', file=None, flush=False)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carpeta que contiene los archivos Parquet\n",
    "input_folder_parquet = r\"C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR_parquet\\Rev-Florida.parquet\"\n",
    "\n",
    "# Lista para almacenar DataFrames\n",
    "df_list_Florida = []\n",
    "\n",
    "# Iterar sobre todos los archivos Parquet en la carpeta de entrada\n",
    "for filename in os.listdir(input_folder_parquet):\n",
    "    if filename.endswith('.parquet'):\n",
    "        # Ruta completa del archivo Parquet\n",
    "        parquet_file_path_psv = os.path.join(input_folder_parquet, filename)\n",
    "        \n",
    "        # Leer el archivo Parquet y añadir el DataFrame a la lista\n",
    "        df = pd.read_parquet(parquet_file_path_psv)\n",
    "        df_list_Florida.append(df)\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_florida = pd.concat(df_list_Florida, ignore_index=True)\n",
    "\n",
    "# Verificar el DataFrame combinado\n",
    "print(df_florida.shape)\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se elimina columna irrelevante para el análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_florida = df_florida.drop(columns=['pics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se convierten las columnas necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1.0147185615514873e+20\n",
      "1    1.1547723478903833e+20\n",
      "2    1.0180501024489284e+20\n",
      "Name: user_id, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Se convierte la columna 'user_id' a tipo object\n",
    "df_florida['user_id'] = df_florida['user_id'].astype('str')\n",
    "print(df_florida['user_id'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2021-08-03\n",
      "1   2020-07-18\n",
      "2   2018-04-05\n",
      "Name: time, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# Se convierte la columna 'time' a datetime y luego a solo fecha manteniendo tipo datetime\n",
    "df_florida['time'] = pd.to_datetime(df_florida['time'], unit='ms').dt.normalize()\n",
    "print(df_florida['time'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se normaliza la columna 'resp' para agregar su contenido como dos nuevas columnas a df_new_york\n",
    "resp_df_fl = pd.json_normalize(df_florida['resp'])\n",
    "df_florida[['resp_text', 'resp_time']] = resp_df_fl[['text', 'time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2021-07-23\n",
      "1   2020-02-23\n",
      "2          NaT\n",
      "Name: resp_time, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# Se convierte la columna 'resp_time' a datetime y luego a solo fecha manteniendo tipo datetime\n",
    "df_florida['resp_time'] = pd.to_datetime(df_florida['resp_time'], unit='ms').dt.normalize()\n",
    "print(df_florida['resp_time'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se elimina la columna 'resp' original\n",
    "df_florida = df_florida.drop(columns=['resp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se renombra la columna gmap_id como 'id_negocio', para mayor comprensión en el análisis\n",
    "df_florida = df_florida.rename(columns={'gmap_id': 'id_negocio'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se reordena la columna 'id_negocio'\n",
    "columnas = list(df_florida.columns)\n",
    "columnas.insert(0, columnas.pop(columnas.index('id_negocio')))\n",
    "df_florida = df_florida[columnas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se añade la columna 'review_id' autoincremental desde 6900001 para continuar desde el df_pennsylvania\n",
    "df_florida.insert(0, 'review_id', range(6900001, 6900001 + len(df_florida)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se añade la columna 'estado'\n",
    "df_florida.insert(2, 'estado', 'Florida')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2850000 entries, 0 to 2849999\n",
      "Data columns (total 10 columns):\n",
      " #   Column      Dtype         \n",
      "---  ------      -----         \n",
      " 0   review_id   int64         \n",
      " 1   id_negocio  object        \n",
      " 2   estado      object        \n",
      " 3   user_id     object        \n",
      " 4   name        object        \n",
      " 5   time        datetime64[ns]\n",
      " 6   rating      int64         \n",
      " 7   text        object        \n",
      " 8   resp_text   object        \n",
      " 9   resp_time   datetime64[ns]\n",
      "dtypes: datetime64[ns](2), int64(2), object(6)\n",
      "memory usage: 217.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Se verifica como queda el dataframe\n",
    "print(df_florida.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>id_negocio</th>\n",
       "      <th>estado</th>\n",
       "      <th>user_id</th>\n",
       "      <th>name</th>\n",
       "      <th>time</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>resp_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2849995</th>\n",
       "      <td>9749996</td>\n",
       "      <td>0x8890966585e36d3f:0x131d47c2c60a8d31</td>\n",
       "      <td>Florida</td>\n",
       "      <td>1.0028771480080918e+20</td>\n",
       "      <td>James Rudolph</td>\n",
       "      <td>2021-04-01</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>Thank you!</td>\n",
       "      <td>2021-04-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2849996</th>\n",
       "      <td>9749997</td>\n",
       "      <td>0x8890966585e36d3f:0x131d47c2c60a8d31</td>\n",
       "      <td>Florida</td>\n",
       "      <td>1.1409893309742336e+20</td>\n",
       "      <td>Vincent Alexander</td>\n",
       "      <td>2020-02-16</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2849997</th>\n",
       "      <td>9749998</td>\n",
       "      <td>0x8890966585e36d3f:0x131d47c2c60a8d31</td>\n",
       "      <td>Florida</td>\n",
       "      <td>1.1234480481217744e+20</td>\n",
       "      <td>Brett Owen</td>\n",
       "      <td>2019-02-27</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2849998</th>\n",
       "      <td>9749999</td>\n",
       "      <td>0x8890966585e36d3f:0x131d47c2c60a8d31</td>\n",
       "      <td>Florida</td>\n",
       "      <td>1.0544647183143027e+20</td>\n",
       "      <td>ashly kindle</td>\n",
       "      <td>2018-03-15</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>Thank you, Ashly</td>\n",
       "      <td>2018-01-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2849999</th>\n",
       "      <td>9750000</td>\n",
       "      <td>0x8890966585e36d3f:0x131d47c2c60a8d31</td>\n",
       "      <td>Florida</td>\n",
       "      <td>1.0874328165743621e+20</td>\n",
       "      <td>Jomarra Auge</td>\n",
       "      <td>2018-09-09</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>Thank you, Mrs. Jomarra:)</td>\n",
       "      <td>2018-09-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         review_id                             id_negocio   estado  \\\n",
       "2849995    9749996  0x8890966585e36d3f:0x131d47c2c60a8d31  Florida   \n",
       "2849996    9749997  0x8890966585e36d3f:0x131d47c2c60a8d31  Florida   \n",
       "2849997    9749998  0x8890966585e36d3f:0x131d47c2c60a8d31  Florida   \n",
       "2849998    9749999  0x8890966585e36d3f:0x131d47c2c60a8d31  Florida   \n",
       "2849999    9750000  0x8890966585e36d3f:0x131d47c2c60a8d31  Florida   \n",
       "\n",
       "                        user_id               name       time  rating  text  \\\n",
       "2849995  1.0028771480080918e+20      James Rudolph 2021-04-01       5  None   \n",
       "2849996  1.1409893309742336e+20  Vincent Alexander 2020-02-16       3  None   \n",
       "2849997  1.1234480481217744e+20         Brett Owen 2019-02-27       5  None   \n",
       "2849998  1.0544647183143027e+20       ashly kindle 2018-03-15       5  None   \n",
       "2849999  1.0874328165743621e+20       Jomarra Auge 2018-09-09       5  None   \n",
       "\n",
       "                         resp_text  resp_time  \n",
       "2849995                 Thank you! 2021-04-04  \n",
       "2849996                        NaN        NaT  \n",
       "2849997                        NaN        NaT  \n",
       "2849998           Thank you, Ashly 2018-01-25  \n",
       "2849999  Thank you, Mrs. Jomarra:) 2018-09-09  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_florida.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Texas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se convierten los archivos Json a formato parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convertido: 1.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR_parquet\\Rev-Texas.parquet\\1.parquet\n",
      "Convertido: 10.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR_parquet\\Rev-Texas.parquet\\10.parquet\n",
      "Convertido: 11.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR_parquet\\Rev-Texas.parquet\\11.parquet\n",
      "Convertido: 12.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR_parquet\\Rev-Texas.parquet\\12.parquet\n",
      "Convertido: 13.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR_parquet\\Rev-Texas.parquet\\13.parquet\n",
      "Convertido: 14.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR_parquet\\Rev-Texas.parquet\\14.parquet\n",
      "Convertido: 15.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR_parquet\\Rev-Texas.parquet\\15.parquet\n",
      "Convertido: 16.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR_parquet\\Rev-Texas.parquet\\16.parquet\n",
      "Convertido: 2.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR_parquet\\Rev-Texas.parquet\\2.parquet\n",
      "Convertido: 3.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR_parquet\\Rev-Texas.parquet\\3.parquet\n",
      "Convertido: 4.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR_parquet\\Rev-Texas.parquet\\4.parquet\n",
      "Convertido: 5.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR_parquet\\Rev-Texas.parquet\\5.parquet\n",
      "Convertido: 6.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR_parquet\\Rev-Texas.parquet\\6.parquet\n",
      "Convertido: 7.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR_parquet\\Rev-Texas.parquet\\7.parquet\n",
      "Convertido: 8.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR_parquet\\Rev-Texas.parquet\\8.parquet\n",
      "Convertido: 9.json a C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR_parquet\\Rev-Texas.parquet\\9.parquet\n",
      "Conversión completada.\n"
     ]
    }
   ],
   "source": [
    "# Carpeta de entrada que contiene los archivos JSON\n",
    "input_folder = r\"C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR\\review-Texas\"\n",
    "# Carpeta de salida para guardar los archivos Parquet\n",
    "output_folder = r\"C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR_parquet\\Rev-Texas.parquet\"\n",
    "\n",
    "# Crear la carpeta de salida si no existe\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterar sobre todos los archivos en la carpeta de entrada\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.json'):\n",
    "        # Ruta completa del archivo JSON\n",
    "        json_file_path = os.path.join(input_folder, filename)\n",
    "        \n",
    "        # Leer el archivo JSON\n",
    "        try:\n",
    "            df = pd.read_json(json_file_path, lines=True)\n",
    "            \n",
    "            # Crear nombre de archivo Parquet (sin la extensión .json)\n",
    "            parquet_file_path = os.path.join(output_folder, filename.replace('.json', '.parquet'))\n",
    "            \n",
    "            # Guardar el DataFrame como archivo Parquet\n",
    "            df.to_parquet(parquet_file_path, index=False, compression='snappy', engine='pyarrow')\n",
    "            \n",
    "            print(f\"Convertido: {filename} a {parquet_file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al convertir {filename}: {e}\")\n",
    "\n",
    "print(\"Conversión completada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2296824 entries, 0 to 2296823\n",
      "Data columns (total 8 columns):\n",
      " #   Column   Dtype  \n",
      "---  ------   -----  \n",
      " 0   user_id  float64\n",
      " 1   name     object \n",
      " 2   time     int64  \n",
      " 3   rating   int64  \n",
      " 4   text     object \n",
      " 5   pics     object \n",
      " 6   resp     object \n",
      " 7   gmap_id  object \n",
      "dtypes: float64(1), int64(2), object(5)\n",
      "memory usage: 140.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Leer el archivo Parquet\n",
    "Rev_Texas_1 = pd.read_parquet(r\"C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR_parquet\\Rev-Texas.parquet\")\n",
    "\n",
    "print(Rev_Texas_1.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se concatenan los archivos parquet para formar un solo dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2296824, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function print(*args, sep=' ', end='\\n', file=None, flush=False)>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carpeta que contiene los archivos Parquet\n",
    "input_folder_parquet = r\"C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\SUR_parquet\\Rev-Texas.parquet\"\n",
    "\n",
    "# Lista para almacenar DataFrames\n",
    "df_list_Texas = []\n",
    "\n",
    "# Iterar sobre todos los archivos Parquet en la carpeta de entrada\n",
    "for filename in os.listdir(input_folder_parquet):\n",
    "    if filename.endswith('.parquet'):\n",
    "        # Ruta completa del archivo Parquet\n",
    "        parquet_file_path_psv = os.path.join(input_folder_parquet, filename)\n",
    "        \n",
    "        # Leer el archivo Parquet y añadir el DataFrame a la lista\n",
    "        df = pd.read_parquet(parquet_file_path_psv)\n",
    "        df_list_Texas.append(df)\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_texas = pd.concat(df_list_Texas, ignore_index=True)\n",
    "\n",
    "# Verificar el DataFrame combinado\n",
    "print(df_texas.shape)\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se elimina columna irrelevante para el análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_texas = df_texas.drop(columns=['pics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se convierten las columnas necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1.1054529907897031e+20\n",
      "1    1.0361978809750612e+20\n",
      "2    1.0191666310909164e+20\n",
      "Name: user_id, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Se convierte la columna 'user_id' a tipo object\n",
    "df_texas['user_id'] = df_texas['user_id'].astype('str')\n",
    "print(df_texas['user_id'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2021-07-04\n",
      "1   2019-09-13\n",
      "2   2018-03-01\n",
      "Name: time, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# Se convierte la columna 'time' a datetime y luego a solo fecha manteniendo tipo datetime\n",
    "df_texas['time'] = pd.to_datetime(df_texas['time'], unit='ms').dt.normalize()\n",
    "print(df_texas['time'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se normaliza la columna 'resp' para agregar su contenido como dos nuevas columnas a df_new_york\n",
    "resp_df_tx = pd.json_normalize(df_texas['resp'])\n",
    "df_texas[['resp_text', 'resp_time']] = resp_df_tx[['text', 'time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   NaT\n",
      "1   NaT\n",
      "2   NaT\n",
      "Name: resp_time, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# Se convierte la columna 'resp_time' a datetime y luego a solo fecha manteniendo tipo datetime\n",
    "df_texas['resp_time'] = pd.to_datetime(df_texas['resp_time'], unit='ms').dt.normalize()\n",
    "print(df_texas['resp_time'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se elimina la columna 'resp' original\n",
    "df_texas = df_texas.drop(columns=['resp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se renombra la columna gmap_id como 'id_negocio', para mayor comprensión en el análisis\n",
    "df_texas = df_texas.rename(columns={'gmap_id': 'id_negocio'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se reordena la columna 'id_negocio'\n",
    "columnas = list(df_texas.columns)\n",
    "columnas.insert(0, columnas.pop(columnas.index('id_negocio')))\n",
    "df_texas = df_texas[columnas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se añade la columna 'review_id' autoincremental desde 9750001 para continuar desde el df_florida\n",
    "df_texas.insert(0, 'review_id', range(9750001, 9750001 + len(df_texas)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se añade la columna 'estado'\n",
    "df_texas.insert(2, 'estado', 'Texas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2296824 entries, 0 to 2296823\n",
      "Data columns (total 10 columns):\n",
      " #   Column      Dtype         \n",
      "---  ------      -----         \n",
      " 0   review_id   int64         \n",
      " 1   id_negocio  object        \n",
      " 2   estado      object        \n",
      " 3   user_id     object        \n",
      " 4   name        object        \n",
      " 5   time        datetime64[ns]\n",
      " 6   rating      int64         \n",
      " 7   text        object        \n",
      " 8   resp_text   object        \n",
      " 9   resp_time   datetime64[ns]\n",
      "dtypes: datetime64[ns](2), int64(2), object(6)\n",
      "memory usage: 175.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Se verifica como queda el dataframe\n",
    "print(df_texas.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>id_negocio</th>\n",
       "      <th>estado</th>\n",
       "      <th>user_id</th>\n",
       "      <th>name</th>\n",
       "      <th>time</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>resp_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9750001</td>\n",
       "      <td>0x864c3998b8d8dc83:0x57ffabe1e2322320</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.1054529907897031e+20</td>\n",
       "      <td>Kimberly Feger</td>\n",
       "      <td>2021-07-04</td>\n",
       "      <td>5</td>\n",
       "      <td>The pharmacist, Erin, is phenomenal. She was s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9750002</td>\n",
       "      <td>0x864c3998b8d8dc83:0x57ffabe1e2322320</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.0361978809750612e+20</td>\n",
       "      <td>Briana Streit</td>\n",
       "      <td>2019-09-13</td>\n",
       "      <td>2</td>\n",
       "      <td>I gave them 2 stars because they offer prescri...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9750003</td>\n",
       "      <td>0x864c3998b8d8dc83:0x57ffabe1e2322320</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.0191666310909164e+20</td>\n",
       "      <td>Sylvia Caudillo</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>1</td>\n",
       "      <td>If I could put minus stars I would. This has t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9750004</td>\n",
       "      <td>0x864c3998b8d8dc83:0x57ffabe1e2322320</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.178708983045825e+20</td>\n",
       "      <td>Ginger Kinyon</td>\n",
       "      <td>2019-10-21</td>\n",
       "      <td>1</td>\n",
       "      <td>Please fix your restroom doors</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9750005</td>\n",
       "      <td>0x864c3998b8d8dc83:0x57ffabe1e2322320</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.1039734611541672e+20</td>\n",
       "      <td>Angeles Arellano</td>\n",
       "      <td>2019-01-04</td>\n",
       "      <td>1</td>\n",
       "      <td>This pharmacy Walmart dose not work not come a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2296819</th>\n",
       "      <td>12046820</td>\n",
       "      <td>0x8640d215ff1e43e5:0xaeeb5211a25191f4</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.0216813568763861e+20</td>\n",
       "      <td>t williams</td>\n",
       "      <td>2019-07-11</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2296820</th>\n",
       "      <td>12046821</td>\n",
       "      <td>0x8640d215ff1e43e5:0xaeeb5211a25191f4</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.0541730918471213e+20</td>\n",
       "      <td>Erika Lira</td>\n",
       "      <td>2018-07-19</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2296821</th>\n",
       "      <td>12046822</td>\n",
       "      <td>0x8640d215ff1e43e5:0xaeeb5211a25191f4</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.10785699186536e+20</td>\n",
       "      <td>Jon Brent</td>\n",
       "      <td>2019-09-08</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2296822</th>\n",
       "      <td>12046823</td>\n",
       "      <td>0x8640d215ff1e43e5:0xaeeb5211a25191f4</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.0916760195743087e+20</td>\n",
       "      <td>lunna cabal</td>\n",
       "      <td>2017-09-27</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2296823</th>\n",
       "      <td>12046824</td>\n",
       "      <td>0x8640d215ff1e43e5:0xaeeb5211a25191f4</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.012700900751175e+20</td>\n",
       "      <td>Michelle Torres</td>\n",
       "      <td>2019-04-18</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2296824 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         review_id                             id_negocio estado  \\\n",
       "0          9750001  0x864c3998b8d8dc83:0x57ffabe1e2322320  Texas   \n",
       "1          9750002  0x864c3998b8d8dc83:0x57ffabe1e2322320  Texas   \n",
       "2          9750003  0x864c3998b8d8dc83:0x57ffabe1e2322320  Texas   \n",
       "3          9750004  0x864c3998b8d8dc83:0x57ffabe1e2322320  Texas   \n",
       "4          9750005  0x864c3998b8d8dc83:0x57ffabe1e2322320  Texas   \n",
       "...            ...                                    ...    ...   \n",
       "2296819   12046820  0x8640d215ff1e43e5:0xaeeb5211a25191f4  Texas   \n",
       "2296820   12046821  0x8640d215ff1e43e5:0xaeeb5211a25191f4  Texas   \n",
       "2296821   12046822  0x8640d215ff1e43e5:0xaeeb5211a25191f4  Texas   \n",
       "2296822   12046823  0x8640d215ff1e43e5:0xaeeb5211a25191f4  Texas   \n",
       "2296823   12046824  0x8640d215ff1e43e5:0xaeeb5211a25191f4  Texas   \n",
       "\n",
       "                        user_id              name       time  rating  \\\n",
       "0        1.1054529907897031e+20    Kimberly Feger 2021-07-04       5   \n",
       "1        1.0361978809750612e+20     Briana Streit 2019-09-13       2   \n",
       "2        1.0191666310909164e+20   Sylvia Caudillo 2018-03-01       1   \n",
       "3         1.178708983045825e+20     Ginger Kinyon 2019-10-21       1   \n",
       "4        1.1039734611541672e+20  Angeles Arellano 2019-01-04       1   \n",
       "...                         ...               ...        ...     ...   \n",
       "2296819  1.0216813568763861e+20        t williams 2019-07-11       5   \n",
       "2296820  1.0541730918471213e+20        Erika Lira 2018-07-19       5   \n",
       "2296821    1.10785699186536e+20         Jon Brent 2019-09-08       5   \n",
       "2296822  1.0916760195743087e+20       lunna cabal 2017-09-27       4   \n",
       "2296823   1.012700900751175e+20   Michelle Torres 2019-04-18       5   \n",
       "\n",
       "                                                      text resp_text resp_time  \n",
       "0        The pharmacist, Erin, is phenomenal. She was s...       NaN       NaT  \n",
       "1        I gave them 2 stars because they offer prescri...       NaN       NaT  \n",
       "2        If I could put minus stars I would. This has t...       NaN       NaT  \n",
       "3                           Please fix your restroom doors       NaN       NaT  \n",
       "4        This pharmacy Walmart dose not work not come a...       NaN       NaT  \n",
       "...                                                    ...       ...       ...  \n",
       "2296819                                               None       NaN       NaT  \n",
       "2296820                                               None       NaN       NaT  \n",
       "2296821                                               None       NaN       NaT  \n",
       "2296822                                               None       NaN       NaT  \n",
       "2296823                                               None       NaN       NaT  \n",
       "\n",
       "[2296824 rows x 10 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_texas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se concatenan los  DataFrames anteriores para formar uno solo\n",
    "g_estados_seleccionados = pd.concat([df_new_york, df_california, df_pennsylvania, df_florida, df_texas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 12946824 entries, 0 to 2296823\n",
      "Data columns (total 10 columns):\n",
      " #   Column      Dtype         \n",
      "---  ------      -----         \n",
      " 0   review_id   int64         \n",
      " 1   id_negocio  object        \n",
      " 2   estado      object        \n",
      " 3   user_id     object        \n",
      " 4   name        object        \n",
      " 5   time        datetime64[ns]\n",
      " 6   rating      int64         \n",
      " 7   text        object        \n",
      " 8   resp_text   object        \n",
      " 9   resp_time   datetime64[ns]\n",
      "dtypes: datetime64[ns](2), int64(2), object(6)\n",
      "memory usage: 1.1+ GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(g_estados_seleccionados.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>id_negocio</th>\n",
       "      <th>estado</th>\n",
       "      <th>user_id</th>\n",
       "      <th>name</th>\n",
       "      <th>time</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>resp_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0x89c25fc9494dce47:0x6d63c807b59a55</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.1372210469230823e+20</td>\n",
       "      <td>Alvin Martinez</td>\n",
       "      <td>2020-10-23</td>\n",
       "      <td>5</td>\n",
       "      <td>I'm late to posting this but this store especi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0x89c25fc9494dce47:0x6d63c807b59a55</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.0729344149210933e+20</td>\n",
       "      <td>Johnnie Jackson</td>\n",
       "      <td>2021-05-04</td>\n",
       "      <td>1</td>\n",
       "      <td>Very dissatisfied I did not get my phone the p...</td>\n",
       "      <td>We pride ourselves on providing an awesome exp...</td>\n",
       "      <td>2021-05-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0x89c25fc9494dce47:0x6d63c807b59a55</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.003785858018194e+20</td>\n",
       "      <td>Manie Blazer</td>\n",
       "      <td>2020-08-14</td>\n",
       "      <td>5</td>\n",
       "      <td>Excellent very well done with professional car...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0x89c25fc9494dce47:0x6d63c807b59a55</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.1499816115301982e+20</td>\n",
       "      <td>Fashion Fiinds</td>\n",
       "      <td>2018-12-02</td>\n",
       "      <td>5</td>\n",
       "      <td>Basing my review strictly on the service I rec...</td>\n",
       "      <td>Thanks for the awesome review!  We work hard t...</td>\n",
       "      <td>2018-12-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0x89c25fc9494dce47:0x6d63c807b59a55</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.171781857284223e+20</td>\n",
       "      <td>Andres Rieloff</td>\n",
       "      <td>2020-08-13</td>\n",
       "      <td>1</td>\n",
       "      <td>Bad! Disorganized. I'm being totally honest. I...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2296819</th>\n",
       "      <td>12046820</td>\n",
       "      <td>0x8640d215ff1e43e5:0xaeeb5211a25191f4</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.0216813568763861e+20</td>\n",
       "      <td>t williams</td>\n",
       "      <td>2019-07-11</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2296820</th>\n",
       "      <td>12046821</td>\n",
       "      <td>0x8640d215ff1e43e5:0xaeeb5211a25191f4</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.0541730918471213e+20</td>\n",
       "      <td>Erika Lira</td>\n",
       "      <td>2018-07-19</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2296821</th>\n",
       "      <td>12046822</td>\n",
       "      <td>0x8640d215ff1e43e5:0xaeeb5211a25191f4</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.10785699186536e+20</td>\n",
       "      <td>Jon Brent</td>\n",
       "      <td>2019-09-08</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2296822</th>\n",
       "      <td>12046823</td>\n",
       "      <td>0x8640d215ff1e43e5:0xaeeb5211a25191f4</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.0916760195743087e+20</td>\n",
       "      <td>lunna cabal</td>\n",
       "      <td>2017-09-27</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2296823</th>\n",
       "      <td>12046824</td>\n",
       "      <td>0x8640d215ff1e43e5:0xaeeb5211a25191f4</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.012700900751175e+20</td>\n",
       "      <td>Michelle Torres</td>\n",
       "      <td>2019-04-18</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12946824 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         review_id                             id_negocio    estado  \\\n",
       "0                1    0x89c25fc9494dce47:0x6d63c807b59a55  New York   \n",
       "1                2    0x89c25fc9494dce47:0x6d63c807b59a55  New York   \n",
       "2                3    0x89c25fc9494dce47:0x6d63c807b59a55  New York   \n",
       "3                4    0x89c25fc9494dce47:0x6d63c807b59a55  New York   \n",
       "4                5    0x89c25fc9494dce47:0x6d63c807b59a55  New York   \n",
       "...            ...                                    ...       ...   \n",
       "2296819   12046820  0x8640d215ff1e43e5:0xaeeb5211a25191f4     Texas   \n",
       "2296820   12046821  0x8640d215ff1e43e5:0xaeeb5211a25191f4     Texas   \n",
       "2296821   12046822  0x8640d215ff1e43e5:0xaeeb5211a25191f4     Texas   \n",
       "2296822   12046823  0x8640d215ff1e43e5:0xaeeb5211a25191f4     Texas   \n",
       "2296823   12046824  0x8640d215ff1e43e5:0xaeeb5211a25191f4     Texas   \n",
       "\n",
       "                        user_id             name       time  rating  \\\n",
       "0        1.1372210469230823e+20   Alvin Martinez 2020-10-23       5   \n",
       "1        1.0729344149210933e+20  Johnnie Jackson 2021-05-04       1   \n",
       "2         1.003785858018194e+20     Manie Blazer 2020-08-14       5   \n",
       "3        1.1499816115301982e+20   Fashion Fiinds 2018-12-02       5   \n",
       "4         1.171781857284223e+20   Andres Rieloff 2020-08-13       1   \n",
       "...                         ...              ...        ...     ...   \n",
       "2296819  1.0216813568763861e+20       t williams 2019-07-11       5   \n",
       "2296820  1.0541730918471213e+20       Erika Lira 2018-07-19       5   \n",
       "2296821    1.10785699186536e+20        Jon Brent 2019-09-08       5   \n",
       "2296822  1.0916760195743087e+20      lunna cabal 2017-09-27       4   \n",
       "2296823   1.012700900751175e+20  Michelle Torres 2019-04-18       5   \n",
       "\n",
       "                                                      text  \\\n",
       "0        I'm late to posting this but this store especi...   \n",
       "1        Very dissatisfied I did not get my phone the p...   \n",
       "2        Excellent very well done with professional car...   \n",
       "3        Basing my review strictly on the service I rec...   \n",
       "4        Bad! Disorganized. I'm being totally honest. I...   \n",
       "...                                                    ...   \n",
       "2296819                                               None   \n",
       "2296820                                               None   \n",
       "2296821                                               None   \n",
       "2296822                                               None   \n",
       "2296823                                               None   \n",
       "\n",
       "                                                 resp_text  resp_time  \n",
       "0                                                      NaN        NaT  \n",
       "1        We pride ourselves on providing an awesome exp... 2021-05-06  \n",
       "2                                                      NaN        NaT  \n",
       "3        Thanks for the awesome review!  We work hard t... 2018-12-03  \n",
       "4                                                      NaN        NaT  \n",
       "...                                                    ...        ...  \n",
       "2296819                                                NaN        NaT  \n",
       "2296820                                                NaN        NaT  \n",
       "2296821                                                NaN        NaT  \n",
       "2296822                                                NaN        NaT  \n",
       "2296823                                                NaN        NaT  \n",
       "\n",
       "[12946824 rows x 10 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_estados_seleccionados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_id            0\n",
      "id_negocio           0\n",
      "estado               0\n",
      "user_id              0\n",
      "name                 0\n",
      "time                 0\n",
      "rating               0\n",
      "text           5374662\n",
      "resp_text     11307310\n",
      "resp_time     11307310\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "nulos_por_columna = g_estados_seleccionados.isnull().sum()\n",
    "print(nulos_por_columna)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los nulos están en las columnas text, resp_text y resp_time. Para tratarlos voy a reemplazarlos por: \n",
    "- nulos en 'text' por \"sin comentario\"\n",
    "- nulos en 'resp_text' por \"sin respuesta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reemplaza los valores nulos en la columna 'text' con \"sin comentario\"\n",
    "g_estados_seleccionados['text'] = g_estados_seleccionados['text'].fillna(\"sin comentario\")\n",
    "\n",
    "# Reemplaza los valores nulos en la columna 'resp_text' con \"sin respuesta\"\n",
    "g_estados_seleccionados['resp_text'] = g_estados_seleccionados['resp_text'].fillna(\"sin respuesta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>id_negocio</th>\n",
       "      <th>estado</th>\n",
       "      <th>user_id</th>\n",
       "      <th>name</th>\n",
       "      <th>time</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>resp_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0x89c25fc9494dce47:0x6d63c807b59a55</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.1372210469230823e+20</td>\n",
       "      <td>Alvin Martinez</td>\n",
       "      <td>2020-10-23</td>\n",
       "      <td>5</td>\n",
       "      <td>I'm late to posting this but this store especi...</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0x89c25fc9494dce47:0x6d63c807b59a55</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.0729344149210933e+20</td>\n",
       "      <td>Johnnie Jackson</td>\n",
       "      <td>2021-05-04</td>\n",
       "      <td>1</td>\n",
       "      <td>Very dissatisfied I did not get my phone the p...</td>\n",
       "      <td>We pride ourselves on providing an awesome exp...</td>\n",
       "      <td>2021-05-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0x89c25fc9494dce47:0x6d63c807b59a55</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.003785858018194e+20</td>\n",
       "      <td>Manie Blazer</td>\n",
       "      <td>2020-08-14</td>\n",
       "      <td>5</td>\n",
       "      <td>Excellent very well done with professional car...</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0x89c25fc9494dce47:0x6d63c807b59a55</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.1499816115301982e+20</td>\n",
       "      <td>Fashion Fiinds</td>\n",
       "      <td>2018-12-02</td>\n",
       "      <td>5</td>\n",
       "      <td>Basing my review strictly on the service I rec...</td>\n",
       "      <td>Thanks for the awesome review!  We work hard t...</td>\n",
       "      <td>2018-12-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0x89c25fc9494dce47:0x6d63c807b59a55</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.171781857284223e+20</td>\n",
       "      <td>Andres Rieloff</td>\n",
       "      <td>2020-08-13</td>\n",
       "      <td>1</td>\n",
       "      <td>Bad! Disorganized. I'm being totally honest. I...</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2296819</th>\n",
       "      <td>12046820</td>\n",
       "      <td>0x8640d215ff1e43e5:0xaeeb5211a25191f4</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.0216813568763861e+20</td>\n",
       "      <td>t williams</td>\n",
       "      <td>2019-07-11</td>\n",
       "      <td>5</td>\n",
       "      <td>sin comentario</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2296820</th>\n",
       "      <td>12046821</td>\n",
       "      <td>0x8640d215ff1e43e5:0xaeeb5211a25191f4</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.0541730918471213e+20</td>\n",
       "      <td>Erika Lira</td>\n",
       "      <td>2018-07-19</td>\n",
       "      <td>5</td>\n",
       "      <td>sin comentario</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2296821</th>\n",
       "      <td>12046822</td>\n",
       "      <td>0x8640d215ff1e43e5:0xaeeb5211a25191f4</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.10785699186536e+20</td>\n",
       "      <td>Jon Brent</td>\n",
       "      <td>2019-09-08</td>\n",
       "      <td>5</td>\n",
       "      <td>sin comentario</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2296822</th>\n",
       "      <td>12046823</td>\n",
       "      <td>0x8640d215ff1e43e5:0xaeeb5211a25191f4</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.0916760195743087e+20</td>\n",
       "      <td>lunna cabal</td>\n",
       "      <td>2017-09-27</td>\n",
       "      <td>4</td>\n",
       "      <td>sin comentario</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2296823</th>\n",
       "      <td>12046824</td>\n",
       "      <td>0x8640d215ff1e43e5:0xaeeb5211a25191f4</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.012700900751175e+20</td>\n",
       "      <td>Michelle Torres</td>\n",
       "      <td>2019-04-18</td>\n",
       "      <td>5</td>\n",
       "      <td>sin comentario</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12946824 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         review_id                             id_negocio    estado  \\\n",
       "0                1    0x89c25fc9494dce47:0x6d63c807b59a55  New York   \n",
       "1                2    0x89c25fc9494dce47:0x6d63c807b59a55  New York   \n",
       "2                3    0x89c25fc9494dce47:0x6d63c807b59a55  New York   \n",
       "3                4    0x89c25fc9494dce47:0x6d63c807b59a55  New York   \n",
       "4                5    0x89c25fc9494dce47:0x6d63c807b59a55  New York   \n",
       "...            ...                                    ...       ...   \n",
       "2296819   12046820  0x8640d215ff1e43e5:0xaeeb5211a25191f4     Texas   \n",
       "2296820   12046821  0x8640d215ff1e43e5:0xaeeb5211a25191f4     Texas   \n",
       "2296821   12046822  0x8640d215ff1e43e5:0xaeeb5211a25191f4     Texas   \n",
       "2296822   12046823  0x8640d215ff1e43e5:0xaeeb5211a25191f4     Texas   \n",
       "2296823   12046824  0x8640d215ff1e43e5:0xaeeb5211a25191f4     Texas   \n",
       "\n",
       "                        user_id             name       time  rating  \\\n",
       "0        1.1372210469230823e+20   Alvin Martinez 2020-10-23       5   \n",
       "1        1.0729344149210933e+20  Johnnie Jackson 2021-05-04       1   \n",
       "2         1.003785858018194e+20     Manie Blazer 2020-08-14       5   \n",
       "3        1.1499816115301982e+20   Fashion Fiinds 2018-12-02       5   \n",
       "4         1.171781857284223e+20   Andres Rieloff 2020-08-13       1   \n",
       "...                         ...              ...        ...     ...   \n",
       "2296819  1.0216813568763861e+20       t williams 2019-07-11       5   \n",
       "2296820  1.0541730918471213e+20       Erika Lira 2018-07-19       5   \n",
       "2296821    1.10785699186536e+20        Jon Brent 2019-09-08       5   \n",
       "2296822  1.0916760195743087e+20      lunna cabal 2017-09-27       4   \n",
       "2296823   1.012700900751175e+20  Michelle Torres 2019-04-18       5   \n",
       "\n",
       "                                                      text  \\\n",
       "0        I'm late to posting this but this store especi...   \n",
       "1        Very dissatisfied I did not get my phone the p...   \n",
       "2        Excellent very well done with professional car...   \n",
       "3        Basing my review strictly on the service I rec...   \n",
       "4        Bad! Disorganized. I'm being totally honest. I...   \n",
       "...                                                    ...   \n",
       "2296819                                     sin comentario   \n",
       "2296820                                     sin comentario   \n",
       "2296821                                     sin comentario   \n",
       "2296822                                     sin comentario   \n",
       "2296823                                     sin comentario   \n",
       "\n",
       "                                                 resp_text  resp_time  \n",
       "0                                            sin respuesta        NaT  \n",
       "1        We pride ourselves on providing an awesome exp... 2021-05-06  \n",
       "2                                            sin respuesta        NaT  \n",
       "3        Thanks for the awesome review!  We work hard t... 2018-12-03  \n",
       "4                                            sin respuesta        NaT  \n",
       "...                                                    ...        ...  \n",
       "2296819                                      sin respuesta        NaT  \n",
       "2296820                                      sin respuesta        NaT  \n",
       "2296821                                      sin respuesta        NaT  \n",
       "2296822                                      sin respuesta        NaT  \n",
       "2296823                                      sin respuesta        NaT  \n",
       "\n",
       "[12946824 rows x 10 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_estados_seleccionados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_id            0\n",
      "id_negocio           0\n",
      "estado               0\n",
      "user_id              0\n",
      "name                 0\n",
      "time                 0\n",
      "rating               0\n",
      "text                 0\n",
      "resp_text            0\n",
      "resp_time     11307310\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Verifico nulos\n",
    "nulos_por_columna = g_estados_seleccionados.isnull().sum()\n",
    "print(nulos_por_columna)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los nulos en la columna resp_time se dejaran así para poder almacenarse luego el dataframe en formato parquet. En un futuro análisis para su utilización se trataran dichos nulos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voy a importar el dataframe v1.g_sitios con el objetivo de filtrar el dataframe creado recién por las categorías elegidas para este proyecto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>id_negocio</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>num_of_reviews</th>\n",
       "      <th>url</th>\n",
       "      <th>monday</th>\n",
       "      <th>tuesday</th>\n",
       "      <th>wednesday</th>\n",
       "      <th>thursday</th>\n",
       "      <th>friday</th>\n",
       "      <th>saturday</th>\n",
       "      <th>sunday</th>\n",
       "      <th>category_general_id</th>\n",
       "      <th>id_estado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oneyda's Bakery</td>\n",
       "      <td>0x88dae191ee505917:0x6ba3e25388d3fad4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>19</td>\n",
       "      <td>https://www.google.com/maps/place//data=!4m2!3...</td>\n",
       "      <td>8AM–6PM</td>\n",
       "      <td>8AM–6PM</td>\n",
       "      <td>8AM–6PM</td>\n",
       "      <td>8AM–6PM</td>\n",
       "      <td>8AM–6PM</td>\n",
       "      <td>8AM–6PM</td>\n",
       "      <td>Closed</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TACOS LA CABANA</td>\n",
       "      <td>0x808f879f35b5088b:0xe3541cec7a95bd88</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>https://www.google.com/maps/place//data=!4m2!3...</td>\n",
       "      <td>5–11PM</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Closed</td>\n",
       "      <td>5–11PM</td>\n",
       "      <td>5–11PM</td>\n",
       "      <td>5–11PM</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tropical Park Liquors</td>\n",
       "      <td>0x88d9b99475d9fd7b:0xea6083d207b2471a</td>\n",
       "      <td>4.7</td>\n",
       "      <td>8</td>\n",
       "      <td>https://www.google.com/maps/place//data=!4m2!3...</td>\n",
       "      <td>10AM–10PM</td>\n",
       "      <td>10AM–10PM</td>\n",
       "      <td>10AM–12AM</td>\n",
       "      <td>10AM–12AM</td>\n",
       "      <td>10AM–12AM</td>\n",
       "      <td>10AM–12AM</td>\n",
       "      <td>10AM–10PM</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Nutrition Group</td>\n",
       "      <td>0x89c88de475520cc7:0xeff46469445b5212</td>\n",
       "      <td>3.2</td>\n",
       "      <td>17</td>\n",
       "      <td>https://www.google.com/maps/place//data=!4m2!3...</td>\n",
       "      <td>9AM–8PM</td>\n",
       "      <td>9AM–8PM</td>\n",
       "      <td>9AM–8PM</td>\n",
       "      <td>9AM–8PM</td>\n",
       "      <td>9AM–8PM</td>\n",
       "      <td>9AM–5PM</td>\n",
       "      <td>Closed</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Top Cat Seafood Restaurant</td>\n",
       "      <td>0x864e9891e381f3df:0x4cefe6219bc9199c</td>\n",
       "      <td>3.9</td>\n",
       "      <td>8</td>\n",
       "      <td>https://www.google.com/maps/place//data=!4m2!3...</td>\n",
       "      <td>12–8PM</td>\n",
       "      <td>12–8PM</td>\n",
       "      <td>12–8PM</td>\n",
       "      <td>12–8PM</td>\n",
       "      <td>12–8PM</td>\n",
       "      <td>12–8PM</td>\n",
       "      <td>12–8PM</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         name                             id_negocio  \\\n",
       "0             Oneyda's Bakery  0x88dae191ee505917:0x6ba3e25388d3fad4   \n",
       "1             TACOS LA CABANA  0x808f879f35b5088b:0xe3541cec7a95bd88   \n",
       "2       Tropical Park Liquors  0x88d9b99475d9fd7b:0xea6083d207b2471a   \n",
       "3         The Nutrition Group  0x89c88de475520cc7:0xeff46469445b5212   \n",
       "4  Top Cat Seafood Restaurant  0x864e9891e381f3df:0x4cefe6219bc9199c   \n",
       "\n",
       "   avg_rating  num_of_reviews  \\\n",
       "0         4.6              19   \n",
       "1         5.0               2   \n",
       "2         4.7               8   \n",
       "3         3.2              17   \n",
       "4         3.9               8   \n",
       "\n",
       "                                                 url     monday    tuesday  \\\n",
       "0  https://www.google.com/maps/place//data=!4m2!3...    8AM–6PM    8AM–6PM   \n",
       "1  https://www.google.com/maps/place//data=!4m2!3...     5–11PM     Closed   \n",
       "2  https://www.google.com/maps/place//data=!4m2!3...  10AM–10PM  10AM–10PM   \n",
       "3  https://www.google.com/maps/place//data=!4m2!3...    9AM–8PM    9AM–8PM   \n",
       "4  https://www.google.com/maps/place//data=!4m2!3...     12–8PM     12–8PM   \n",
       "\n",
       "   wednesday   thursday     friday   saturday     sunday  category_general_id  \\\n",
       "0    8AM–6PM    8AM–6PM    8AM–6PM    8AM–6PM     Closed                    4   \n",
       "1     Closed     Closed     5–11PM     5–11PM     5–11PM                    3   \n",
       "2  10AM–12AM  10AM–12AM  10AM–12AM  10AM–12AM  10AM–10PM                    4   \n",
       "3    9AM–8PM    9AM–8PM    9AM–8PM    9AM–5PM     Closed                    4   \n",
       "4     12–8PM     12–8PM     12–8PM     12–8PM     12–8PM                    3   \n",
       "\n",
       "   id_estado  \n",
       "0          7  \n",
       "1          9  \n",
       "2          7  \n",
       "3         13  \n",
       "4         18  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sitios= pd.read_parquet(r\"C:\\Users\\Juli\\Desktop\\Data Science\\Archivos Proyecto Final\\v1.g_sitios.parquet\")\n",
    "sitios.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>id_negocio</th>\n",
       "      <th>estado</th>\n",
       "      <th>user_id</th>\n",
       "      <th>name_x</th>\n",
       "      <th>time</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>resp_time</th>\n",
       "      <th>...</th>\n",
       "      <th>url</th>\n",
       "      <th>monday</th>\n",
       "      <th>tuesday</th>\n",
       "      <th>wednesday</th>\n",
       "      <th>thursday</th>\n",
       "      <th>friday</th>\n",
       "      <th>saturday</th>\n",
       "      <th>sunday</th>\n",
       "      <th>category_general_id</th>\n",
       "      <th>id_estado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148</td>\n",
       "      <td>0x89c261f60bdf13db:0x38da730e4687a97b</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.018634613890113e+20</td>\n",
       "      <td>Maria Patricia Londoño</td>\n",
       "      <td>2021-08-16</td>\n",
       "      <td>4</td>\n",
       "      <td>The donuts is always a good place to buy somet...</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.google.com/maps/place//data=!4m2!3...</td>\n",
       "      <td>6AM–7PM</td>\n",
       "      <td>6AM–7PM</td>\n",
       "      <td>6AM–7PM</td>\n",
       "      <td>6AM–7PM</td>\n",
       "      <td>6AM–7PM</td>\n",
       "      <td>6AM–7PM</td>\n",
       "      <td>6AM–7PM</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>149</td>\n",
       "      <td>0x89c261f60bdf13db:0x38da730e4687a97b</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.0195463333253585e+20</td>\n",
       "      <td>Kristal</td>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>5</td>\n",
       "      <td>I went into this Dunkin' yesterday and got som...</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.google.com/maps/place//data=!4m2!3...</td>\n",
       "      <td>6AM–7PM</td>\n",
       "      <td>6AM–7PM</td>\n",
       "      <td>6AM–7PM</td>\n",
       "      <td>6AM–7PM</td>\n",
       "      <td>6AM–7PM</td>\n",
       "      <td>6AM–7PM</td>\n",
       "      <td>6AM–7PM</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150</td>\n",
       "      <td>0x89c261f60bdf13db:0x38da730e4687a97b</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.0898768409594107e+20</td>\n",
       "      <td>Efrain Hernandez</td>\n",
       "      <td>2020-07-07</td>\n",
       "      <td>3</td>\n",
       "      <td>Bought a Machiato and as soon as I touched the...</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.google.com/maps/place//data=!4m2!3...</td>\n",
       "      <td>6AM–7PM</td>\n",
       "      <td>6AM–7PM</td>\n",
       "      <td>6AM–7PM</td>\n",
       "      <td>6AM–7PM</td>\n",
       "      <td>6AM–7PM</td>\n",
       "      <td>6AM–7PM</td>\n",
       "      <td>6AM–7PM</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>151</td>\n",
       "      <td>0x89c261f60bdf13db:0x38da730e4687a97b</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.1014632117097557e+20</td>\n",
       "      <td>Kiyoshi Sudo</td>\n",
       "      <td>2019-08-02</td>\n",
       "      <td>5</td>\n",
       "      <td>Friendly staffs, nice donuts and muffins and c...</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.google.com/maps/place//data=!4m2!3...</td>\n",
       "      <td>6AM–7PM</td>\n",
       "      <td>6AM–7PM</td>\n",
       "      <td>6AM–7PM</td>\n",
       "      <td>6AM–7PM</td>\n",
       "      <td>6AM–7PM</td>\n",
       "      <td>6AM–7PM</td>\n",
       "      <td>6AM–7PM</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>152</td>\n",
       "      <td>0x89c261f60bdf13db:0x38da730e4687a97b</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.1262588515936902e+20</td>\n",
       "      <td>Charlotte Sheppard</td>\n",
       "      <td>2020-10-11</td>\n",
       "      <td>1</td>\n",
       "      <td>They got my order wrong  food wasn't done unco...</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.google.com/maps/place//data=!4m2!3...</td>\n",
       "      <td>6AM–7PM</td>\n",
       "      <td>6AM–7PM</td>\n",
       "      <td>6AM–7PM</td>\n",
       "      <td>6AM–7PM</td>\n",
       "      <td>6AM–7PM</td>\n",
       "      <td>6AM–7PM</td>\n",
       "      <td>6AM–7PM</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128016</th>\n",
       "      <td>12046147</td>\n",
       "      <td>0x863f79137a27e1e7:0x5e916e7fe74dcff5</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.086757371066782e+20</td>\n",
       "      <td>Gary Kohr</td>\n",
       "      <td>2014-08-10</td>\n",
       "      <td>2</td>\n",
       "      <td>Maybe it was bad because it's Sunday but only ...</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.google.com/maps/place//data=!4m2!3...</td>\n",
       "      <td>11AM–9:30PM</td>\n",
       "      <td>11AM–9:30PM</td>\n",
       "      <td>11AM–9:30PM</td>\n",
       "      <td>11AM–9:30PM</td>\n",
       "      <td>11AM–10PM</td>\n",
       "      <td>11AM–10PM</td>\n",
       "      <td>11AM–9:30PM</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128017</th>\n",
       "      <td>12046148</td>\n",
       "      <td>0x863f79137a27e1e7:0x5e916e7fe74dcff5</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.0378064778394966e+20</td>\n",
       "      <td>Joyce Earls</td>\n",
       "      <td>2016-09-23</td>\n",
       "      <td>5</td>\n",
       "      <td>sin comentario</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.google.com/maps/place//data=!4m2!3...</td>\n",
       "      <td>11AM–9:30PM</td>\n",
       "      <td>11AM–9:30PM</td>\n",
       "      <td>11AM–9:30PM</td>\n",
       "      <td>11AM–9:30PM</td>\n",
       "      <td>11AM–10PM</td>\n",
       "      <td>11AM–10PM</td>\n",
       "      <td>11AM–9:30PM</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128018</th>\n",
       "      <td>12046149</td>\n",
       "      <td>0x863f79137a27e1e7:0x5e916e7fe74dcff5</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.1824656293609538e+20</td>\n",
       "      <td>Brenda Leffler</td>\n",
       "      <td>2012-09-28</td>\n",
       "      <td>3</td>\n",
       "      <td>sin comentario</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.google.com/maps/place//data=!4m2!3...</td>\n",
       "      <td>11AM–9:30PM</td>\n",
       "      <td>11AM–9:30PM</td>\n",
       "      <td>11AM–9:30PM</td>\n",
       "      <td>11AM–9:30PM</td>\n",
       "      <td>11AM–10PM</td>\n",
       "      <td>11AM–10PM</td>\n",
       "      <td>11AM–9:30PM</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128019</th>\n",
       "      <td>12046150</td>\n",
       "      <td>0x863f79137a27e1e7:0x5e916e7fe74dcff5</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.0440035146720169e+20</td>\n",
       "      <td>Andrés Peña</td>\n",
       "      <td>2017-06-07</td>\n",
       "      <td>1</td>\n",
       "      <td>sin comentario</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.google.com/maps/place//data=!4m2!3...</td>\n",
       "      <td>11AM–9:30PM</td>\n",
       "      <td>11AM–9:30PM</td>\n",
       "      <td>11AM–9:30PM</td>\n",
       "      <td>11AM–9:30PM</td>\n",
       "      <td>11AM–10PM</td>\n",
       "      <td>11AM–10PM</td>\n",
       "      <td>11AM–9:30PM</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128020</th>\n",
       "      <td>12046151</td>\n",
       "      <td>0x863f79137a27e1e7:0x5e916e7fe74dcff5</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.1718112804617642e+20</td>\n",
       "      <td>Shasha Robles</td>\n",
       "      <td>2018-05-15</td>\n",
       "      <td>3</td>\n",
       "      <td>sin comentario</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.google.com/maps/place//data=!4m2!3...</td>\n",
       "      <td>11AM–9:30PM</td>\n",
       "      <td>11AM–9:30PM</td>\n",
       "      <td>11AM–9:30PM</td>\n",
       "      <td>11AM–9:30PM</td>\n",
       "      <td>11AM–10PM</td>\n",
       "      <td>11AM–10PM</td>\n",
       "      <td>11AM–9:30PM</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4128021 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         review_id                             id_negocio    estado  \\\n",
       "0              148  0x89c261f60bdf13db:0x38da730e4687a97b  New York   \n",
       "1              149  0x89c261f60bdf13db:0x38da730e4687a97b  New York   \n",
       "2              150  0x89c261f60bdf13db:0x38da730e4687a97b  New York   \n",
       "3              151  0x89c261f60bdf13db:0x38da730e4687a97b  New York   \n",
       "4              152  0x89c261f60bdf13db:0x38da730e4687a97b  New York   \n",
       "...            ...                                    ...       ...   \n",
       "4128016   12046147  0x863f79137a27e1e7:0x5e916e7fe74dcff5     Texas   \n",
       "4128017   12046148  0x863f79137a27e1e7:0x5e916e7fe74dcff5     Texas   \n",
       "4128018   12046149  0x863f79137a27e1e7:0x5e916e7fe74dcff5     Texas   \n",
       "4128019   12046150  0x863f79137a27e1e7:0x5e916e7fe74dcff5     Texas   \n",
       "4128020   12046151  0x863f79137a27e1e7:0x5e916e7fe74dcff5     Texas   \n",
       "\n",
       "                        user_id                  name_x       time  rating  \\\n",
       "0         1.018634613890113e+20  Maria Patricia Londoño 2021-08-16       4   \n",
       "1        1.0195463333253585e+20                 Kristal 2020-07-01       5   \n",
       "2        1.0898768409594107e+20        Efrain Hernandez 2020-07-07       3   \n",
       "3        1.1014632117097557e+20            Kiyoshi Sudo 2019-08-02       5   \n",
       "4        1.1262588515936902e+20      Charlotte Sheppard 2020-10-11       1   \n",
       "...                         ...                     ...        ...     ...   \n",
       "4128016   1.086757371066782e+20               Gary Kohr 2014-08-10       2   \n",
       "4128017  1.0378064778394966e+20             Joyce Earls 2016-09-23       5   \n",
       "4128018  1.1824656293609538e+20          Brenda Leffler 2012-09-28       3   \n",
       "4128019  1.0440035146720169e+20             Andrés Peña 2017-06-07       1   \n",
       "4128020  1.1718112804617642e+20           Shasha Robles 2018-05-15       3   \n",
       "\n",
       "                                                      text      resp_text  \\\n",
       "0        The donuts is always a good place to buy somet...  sin respuesta   \n",
       "1        I went into this Dunkin' yesterday and got som...  sin respuesta   \n",
       "2        Bought a Machiato and as soon as I touched the...  sin respuesta   \n",
       "3        Friendly staffs, nice donuts and muffins and c...  sin respuesta   \n",
       "4        They got my order wrong  food wasn't done unco...  sin respuesta   \n",
       "...                                                    ...            ...   \n",
       "4128016  Maybe it was bad because it's Sunday but only ...  sin respuesta   \n",
       "4128017                                     sin comentario  sin respuesta   \n",
       "4128018                                     sin comentario  sin respuesta   \n",
       "4128019                                     sin comentario  sin respuesta   \n",
       "4128020                                     sin comentario  sin respuesta   \n",
       "\n",
       "        resp_time  ...                                                url  \\\n",
       "0             NaT  ...  https://www.google.com/maps/place//data=!4m2!3...   \n",
       "1             NaT  ...  https://www.google.com/maps/place//data=!4m2!3...   \n",
       "2             NaT  ...  https://www.google.com/maps/place//data=!4m2!3...   \n",
       "3             NaT  ...  https://www.google.com/maps/place//data=!4m2!3...   \n",
       "4             NaT  ...  https://www.google.com/maps/place//data=!4m2!3...   \n",
       "...           ...  ...                                                ...   \n",
       "4128016       NaT  ...  https://www.google.com/maps/place//data=!4m2!3...   \n",
       "4128017       NaT  ...  https://www.google.com/maps/place//data=!4m2!3...   \n",
       "4128018       NaT  ...  https://www.google.com/maps/place//data=!4m2!3...   \n",
       "4128019       NaT  ...  https://www.google.com/maps/place//data=!4m2!3...   \n",
       "4128020       NaT  ...  https://www.google.com/maps/place//data=!4m2!3...   \n",
       "\n",
       "              monday      tuesday    wednesday     thursday     friday  \\\n",
       "0            6AM–7PM      6AM–7PM      6AM–7PM      6AM–7PM    6AM–7PM   \n",
       "1            6AM–7PM      6AM–7PM      6AM–7PM      6AM–7PM    6AM–7PM   \n",
       "2            6AM–7PM      6AM–7PM      6AM–7PM      6AM–7PM    6AM–7PM   \n",
       "3            6AM–7PM      6AM–7PM      6AM–7PM      6AM–7PM    6AM–7PM   \n",
       "4            6AM–7PM      6AM–7PM      6AM–7PM      6AM–7PM    6AM–7PM   \n",
       "...              ...          ...          ...          ...        ...   \n",
       "4128016  11AM–9:30PM  11AM–9:30PM  11AM–9:30PM  11AM–9:30PM  11AM–10PM   \n",
       "4128017  11AM–9:30PM  11AM–9:30PM  11AM–9:30PM  11AM–9:30PM  11AM–10PM   \n",
       "4128018  11AM–9:30PM  11AM–9:30PM  11AM–9:30PM  11AM–9:30PM  11AM–10PM   \n",
       "4128019  11AM–9:30PM  11AM–9:30PM  11AM–9:30PM  11AM–9:30PM  11AM–10PM   \n",
       "4128020  11AM–9:30PM  11AM–9:30PM  11AM–9:30PM  11AM–9:30PM  11AM–10PM   \n",
       "\n",
       "          saturday       sunday category_general_id id_estado  \n",
       "0          6AM–7PM      6AM–7PM                   4        24  \n",
       "1          6AM–7PM      6AM–7PM                   4        24  \n",
       "2          6AM–7PM      6AM–7PM                   4        24  \n",
       "3          6AM–7PM      6AM–7PM                   4        24  \n",
       "4          6AM–7PM      6AM–7PM                   4        24  \n",
       "...            ...          ...                 ...       ...  \n",
       "4128016  11AM–10PM  11AM–9:30PM                   3        18  \n",
       "4128017  11AM–10PM  11AM–9:30PM                   3        18  \n",
       "4128018  11AM–10PM  11AM–9:30PM                   3        18  \n",
       "4128019  11AM–10PM  11AM–9:30PM                   3        18  \n",
       "4128020  11AM–10PM  11AM–9:30PM                   3        18  \n",
       "\n",
       "[4128021 rows x 23 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Realizamos el merge para obtener solo las filas de g_estados_seleccionados que tienen coincidencia en sitios\n",
    "tabla_final = pd.merge(g_estados_seleccionados, sitios, on='id_negocio', how='inner')\n",
    "tabla_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>id_negocio</th>\n",
       "      <th>estado</th>\n",
       "      <th>user_id</th>\n",
       "      <th>name_x</th>\n",
       "      <th>time</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>resp_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148</td>\n",
       "      <td>0x89c261f60bdf13db:0x38da730e4687a97b</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.018634613890113e+20</td>\n",
       "      <td>Maria Patricia Londoño</td>\n",
       "      <td>2021-08-16</td>\n",
       "      <td>4</td>\n",
       "      <td>The donuts is always a good place to buy somet...</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>149</td>\n",
       "      <td>0x89c261f60bdf13db:0x38da730e4687a97b</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.0195463333253585e+20</td>\n",
       "      <td>Kristal</td>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>5</td>\n",
       "      <td>I went into this Dunkin' yesterday and got som...</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150</td>\n",
       "      <td>0x89c261f60bdf13db:0x38da730e4687a97b</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.0898768409594107e+20</td>\n",
       "      <td>Efrain Hernandez</td>\n",
       "      <td>2020-07-07</td>\n",
       "      <td>3</td>\n",
       "      <td>Bought a Machiato and as soon as I touched the...</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>151</td>\n",
       "      <td>0x89c261f60bdf13db:0x38da730e4687a97b</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.1014632117097557e+20</td>\n",
       "      <td>Kiyoshi Sudo</td>\n",
       "      <td>2019-08-02</td>\n",
       "      <td>5</td>\n",
       "      <td>Friendly staffs, nice donuts and muffins and c...</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>152</td>\n",
       "      <td>0x89c261f60bdf13db:0x38da730e4687a97b</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.1262588515936902e+20</td>\n",
       "      <td>Charlotte Sheppard</td>\n",
       "      <td>2020-10-11</td>\n",
       "      <td>1</td>\n",
       "      <td>They got my order wrong  food wasn't done unco...</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128016</th>\n",
       "      <td>12046147</td>\n",
       "      <td>0x863f79137a27e1e7:0x5e916e7fe74dcff5</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.086757371066782e+20</td>\n",
       "      <td>Gary Kohr</td>\n",
       "      <td>2014-08-10</td>\n",
       "      <td>2</td>\n",
       "      <td>Maybe it was bad because it's Sunday but only ...</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128017</th>\n",
       "      <td>12046148</td>\n",
       "      <td>0x863f79137a27e1e7:0x5e916e7fe74dcff5</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.0378064778394966e+20</td>\n",
       "      <td>Joyce Earls</td>\n",
       "      <td>2016-09-23</td>\n",
       "      <td>5</td>\n",
       "      <td>sin comentario</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128018</th>\n",
       "      <td>12046149</td>\n",
       "      <td>0x863f79137a27e1e7:0x5e916e7fe74dcff5</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.1824656293609538e+20</td>\n",
       "      <td>Brenda Leffler</td>\n",
       "      <td>2012-09-28</td>\n",
       "      <td>3</td>\n",
       "      <td>sin comentario</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128019</th>\n",
       "      <td>12046150</td>\n",
       "      <td>0x863f79137a27e1e7:0x5e916e7fe74dcff5</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.0440035146720169e+20</td>\n",
       "      <td>Andrés Peña</td>\n",
       "      <td>2017-06-07</td>\n",
       "      <td>1</td>\n",
       "      <td>sin comentario</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128020</th>\n",
       "      <td>12046151</td>\n",
       "      <td>0x863f79137a27e1e7:0x5e916e7fe74dcff5</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.1718112804617642e+20</td>\n",
       "      <td>Shasha Robles</td>\n",
       "      <td>2018-05-15</td>\n",
       "      <td>3</td>\n",
       "      <td>sin comentario</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4128021 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         review_id                             id_negocio    estado  \\\n",
       "0              148  0x89c261f60bdf13db:0x38da730e4687a97b  New York   \n",
       "1              149  0x89c261f60bdf13db:0x38da730e4687a97b  New York   \n",
       "2              150  0x89c261f60bdf13db:0x38da730e4687a97b  New York   \n",
       "3              151  0x89c261f60bdf13db:0x38da730e4687a97b  New York   \n",
       "4              152  0x89c261f60bdf13db:0x38da730e4687a97b  New York   \n",
       "...            ...                                    ...       ...   \n",
       "4128016   12046147  0x863f79137a27e1e7:0x5e916e7fe74dcff5     Texas   \n",
       "4128017   12046148  0x863f79137a27e1e7:0x5e916e7fe74dcff5     Texas   \n",
       "4128018   12046149  0x863f79137a27e1e7:0x5e916e7fe74dcff5     Texas   \n",
       "4128019   12046150  0x863f79137a27e1e7:0x5e916e7fe74dcff5     Texas   \n",
       "4128020   12046151  0x863f79137a27e1e7:0x5e916e7fe74dcff5     Texas   \n",
       "\n",
       "                        user_id                  name_x       time  rating  \\\n",
       "0         1.018634613890113e+20  Maria Patricia Londoño 2021-08-16       4   \n",
       "1        1.0195463333253585e+20                 Kristal 2020-07-01       5   \n",
       "2        1.0898768409594107e+20        Efrain Hernandez 2020-07-07       3   \n",
       "3        1.1014632117097557e+20            Kiyoshi Sudo 2019-08-02       5   \n",
       "4        1.1262588515936902e+20      Charlotte Sheppard 2020-10-11       1   \n",
       "...                         ...                     ...        ...     ...   \n",
       "4128016   1.086757371066782e+20               Gary Kohr 2014-08-10       2   \n",
       "4128017  1.0378064778394966e+20             Joyce Earls 2016-09-23       5   \n",
       "4128018  1.1824656293609538e+20          Brenda Leffler 2012-09-28       3   \n",
       "4128019  1.0440035146720169e+20             Andrés Peña 2017-06-07       1   \n",
       "4128020  1.1718112804617642e+20           Shasha Robles 2018-05-15       3   \n",
       "\n",
       "                                                      text      resp_text  \\\n",
       "0        The donuts is always a good place to buy somet...  sin respuesta   \n",
       "1        I went into this Dunkin' yesterday and got som...  sin respuesta   \n",
       "2        Bought a Machiato and as soon as I touched the...  sin respuesta   \n",
       "3        Friendly staffs, nice donuts and muffins and c...  sin respuesta   \n",
       "4        They got my order wrong  food wasn't done unco...  sin respuesta   \n",
       "...                                                    ...            ...   \n",
       "4128016  Maybe it was bad because it's Sunday but only ...  sin respuesta   \n",
       "4128017                                     sin comentario  sin respuesta   \n",
       "4128018                                     sin comentario  sin respuesta   \n",
       "4128019                                     sin comentario  sin respuesta   \n",
       "4128020                                     sin comentario  sin respuesta   \n",
       "\n",
       "        resp_time  \n",
       "0             NaT  \n",
       "1             NaT  \n",
       "2             NaT  \n",
       "3             NaT  \n",
       "4             NaT  \n",
       "...           ...  \n",
       "4128016       NaT  \n",
       "4128017       NaT  \n",
       "4128018       NaT  \n",
       "4128019       NaT  \n",
       "4128020       NaT  \n",
       "\n",
       "[4128021 rows x 10 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seleccionamos solo las columnas específicas del dataframe resultado\n",
    "tabla_final = tabla_final[['review_id', 'id_negocio', 'estado', 'user_id', 'name_x', 'time', 'rating', 'text', 'resp_text', 'resp_time']]\n",
    "tabla_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>id_negocio</th>\n",
       "      <th>estado</th>\n",
       "      <th>user_id</th>\n",
       "      <th>name</th>\n",
       "      <th>time</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>resp_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148</td>\n",
       "      <td>0x89c261f60bdf13db:0x38da730e4687a97b</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.018634613890113e+20</td>\n",
       "      <td>Maria Patricia Londoño</td>\n",
       "      <td>2021-08-16</td>\n",
       "      <td>4</td>\n",
       "      <td>The donuts is always a good place to buy somet...</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>149</td>\n",
       "      <td>0x89c261f60bdf13db:0x38da730e4687a97b</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.0195463333253585e+20</td>\n",
       "      <td>Kristal</td>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>5</td>\n",
       "      <td>I went into this Dunkin' yesterday and got som...</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150</td>\n",
       "      <td>0x89c261f60bdf13db:0x38da730e4687a97b</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.0898768409594107e+20</td>\n",
       "      <td>Efrain Hernandez</td>\n",
       "      <td>2020-07-07</td>\n",
       "      <td>3</td>\n",
       "      <td>Bought a Machiato and as soon as I touched the...</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>151</td>\n",
       "      <td>0x89c261f60bdf13db:0x38da730e4687a97b</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.1014632117097557e+20</td>\n",
       "      <td>Kiyoshi Sudo</td>\n",
       "      <td>2019-08-02</td>\n",
       "      <td>5</td>\n",
       "      <td>Friendly staffs, nice donuts and muffins and c...</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>152</td>\n",
       "      <td>0x89c261f60bdf13db:0x38da730e4687a97b</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.1262588515936902e+20</td>\n",
       "      <td>Charlotte Sheppard</td>\n",
       "      <td>2020-10-11</td>\n",
       "      <td>1</td>\n",
       "      <td>They got my order wrong  food wasn't done unco...</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128016</th>\n",
       "      <td>12046147</td>\n",
       "      <td>0x863f79137a27e1e7:0x5e916e7fe74dcff5</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.086757371066782e+20</td>\n",
       "      <td>Gary Kohr</td>\n",
       "      <td>2014-08-10</td>\n",
       "      <td>2</td>\n",
       "      <td>Maybe it was bad because it's Sunday but only ...</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128017</th>\n",
       "      <td>12046148</td>\n",
       "      <td>0x863f79137a27e1e7:0x5e916e7fe74dcff5</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.0378064778394966e+20</td>\n",
       "      <td>Joyce Earls</td>\n",
       "      <td>2016-09-23</td>\n",
       "      <td>5</td>\n",
       "      <td>sin comentario</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128018</th>\n",
       "      <td>12046149</td>\n",
       "      <td>0x863f79137a27e1e7:0x5e916e7fe74dcff5</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.1824656293609538e+20</td>\n",
       "      <td>Brenda Leffler</td>\n",
       "      <td>2012-09-28</td>\n",
       "      <td>3</td>\n",
       "      <td>sin comentario</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128019</th>\n",
       "      <td>12046150</td>\n",
       "      <td>0x863f79137a27e1e7:0x5e916e7fe74dcff5</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.0440035146720169e+20</td>\n",
       "      <td>Andrés Peña</td>\n",
       "      <td>2017-06-07</td>\n",
       "      <td>1</td>\n",
       "      <td>sin comentario</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128020</th>\n",
       "      <td>12046151</td>\n",
       "      <td>0x863f79137a27e1e7:0x5e916e7fe74dcff5</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.1718112804617642e+20</td>\n",
       "      <td>Shasha Robles</td>\n",
       "      <td>2018-05-15</td>\n",
       "      <td>3</td>\n",
       "      <td>sin comentario</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4128021 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         review_id                             id_negocio    estado  \\\n",
       "0              148  0x89c261f60bdf13db:0x38da730e4687a97b  New York   \n",
       "1              149  0x89c261f60bdf13db:0x38da730e4687a97b  New York   \n",
       "2              150  0x89c261f60bdf13db:0x38da730e4687a97b  New York   \n",
       "3              151  0x89c261f60bdf13db:0x38da730e4687a97b  New York   \n",
       "4              152  0x89c261f60bdf13db:0x38da730e4687a97b  New York   \n",
       "...            ...                                    ...       ...   \n",
       "4128016   12046147  0x863f79137a27e1e7:0x5e916e7fe74dcff5     Texas   \n",
       "4128017   12046148  0x863f79137a27e1e7:0x5e916e7fe74dcff5     Texas   \n",
       "4128018   12046149  0x863f79137a27e1e7:0x5e916e7fe74dcff5     Texas   \n",
       "4128019   12046150  0x863f79137a27e1e7:0x5e916e7fe74dcff5     Texas   \n",
       "4128020   12046151  0x863f79137a27e1e7:0x5e916e7fe74dcff5     Texas   \n",
       "\n",
       "                        user_id                    name       time  rating  \\\n",
       "0         1.018634613890113e+20  Maria Patricia Londoño 2021-08-16       4   \n",
       "1        1.0195463333253585e+20                 Kristal 2020-07-01       5   \n",
       "2        1.0898768409594107e+20        Efrain Hernandez 2020-07-07       3   \n",
       "3        1.1014632117097557e+20            Kiyoshi Sudo 2019-08-02       5   \n",
       "4        1.1262588515936902e+20      Charlotte Sheppard 2020-10-11       1   \n",
       "...                         ...                     ...        ...     ...   \n",
       "4128016   1.086757371066782e+20               Gary Kohr 2014-08-10       2   \n",
       "4128017  1.0378064778394966e+20             Joyce Earls 2016-09-23       5   \n",
       "4128018  1.1824656293609538e+20          Brenda Leffler 2012-09-28       3   \n",
       "4128019  1.0440035146720169e+20             Andrés Peña 2017-06-07       1   \n",
       "4128020  1.1718112804617642e+20           Shasha Robles 2018-05-15       3   \n",
       "\n",
       "                                                      text      resp_text  \\\n",
       "0        The donuts is always a good place to buy somet...  sin respuesta   \n",
       "1        I went into this Dunkin' yesterday and got som...  sin respuesta   \n",
       "2        Bought a Machiato and as soon as I touched the...  sin respuesta   \n",
       "3        Friendly staffs, nice donuts and muffins and c...  sin respuesta   \n",
       "4        They got my order wrong  food wasn't done unco...  sin respuesta   \n",
       "...                                                    ...            ...   \n",
       "4128016  Maybe it was bad because it's Sunday but only ...  sin respuesta   \n",
       "4128017                                     sin comentario  sin respuesta   \n",
       "4128018                                     sin comentario  sin respuesta   \n",
       "4128019                                     sin comentario  sin respuesta   \n",
       "4128020                                     sin comentario  sin respuesta   \n",
       "\n",
       "        resp_time  \n",
       "0             NaT  \n",
       "1             NaT  \n",
       "2             NaT  \n",
       "3             NaT  \n",
       "4             NaT  \n",
       "...           ...  \n",
       "4128016       NaT  \n",
       "4128017       NaT  \n",
       "4128018       NaT  \n",
       "4128019       NaT  \n",
       "4128020       NaT  \n",
       "\n",
       "[4128021 rows x 10 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cambiamos el nombre de la columna 'name_x' a 'name'\n",
    "tabla_final = tabla_final.rename(columns={'name_x': 'name'})\n",
    "tabla_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el dataframe en un archivo parquet\n",
    "tabla_final.to_parquet(r\"C:\\Users\\Juli\\Desktop\\v0_estados_reviews.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>id_negocio</th>\n",
       "      <th>estado</th>\n",
       "      <th>user_id</th>\n",
       "      <th>name</th>\n",
       "      <th>time</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>resp_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148</td>\n",
       "      <td>0x89c261f60bdf13db:0x38da730e4687a97b</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.018634613890113e+20</td>\n",
       "      <td>Maria Patricia Londoño</td>\n",
       "      <td>2021-08-16</td>\n",
       "      <td>4</td>\n",
       "      <td>The donuts is always a good place to buy somet...</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>149</td>\n",
       "      <td>0x89c261f60bdf13db:0x38da730e4687a97b</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.0195463333253585e+20</td>\n",
       "      <td>Kristal</td>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>5</td>\n",
       "      <td>I went into this Dunkin' yesterday and got som...</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150</td>\n",
       "      <td>0x89c261f60bdf13db:0x38da730e4687a97b</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.0898768409594107e+20</td>\n",
       "      <td>Efrain Hernandez</td>\n",
       "      <td>2020-07-07</td>\n",
       "      <td>3</td>\n",
       "      <td>Bought a Machiato and as soon as I touched the...</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>151</td>\n",
       "      <td>0x89c261f60bdf13db:0x38da730e4687a97b</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.1014632117097557e+20</td>\n",
       "      <td>Kiyoshi Sudo</td>\n",
       "      <td>2019-08-02</td>\n",
       "      <td>5</td>\n",
       "      <td>Friendly staffs, nice donuts and muffins and c...</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>152</td>\n",
       "      <td>0x89c261f60bdf13db:0x38da730e4687a97b</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.1262588515936902e+20</td>\n",
       "      <td>Charlotte Sheppard</td>\n",
       "      <td>2020-10-11</td>\n",
       "      <td>1</td>\n",
       "      <td>They got my order wrong  food wasn't done unco...</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128016</th>\n",
       "      <td>12046147</td>\n",
       "      <td>0x863f79137a27e1e7:0x5e916e7fe74dcff5</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.086757371066782e+20</td>\n",
       "      <td>Gary Kohr</td>\n",
       "      <td>2014-08-10</td>\n",
       "      <td>2</td>\n",
       "      <td>Maybe it was bad because it's Sunday but only ...</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128017</th>\n",
       "      <td>12046148</td>\n",
       "      <td>0x863f79137a27e1e7:0x5e916e7fe74dcff5</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.0378064778394966e+20</td>\n",
       "      <td>Joyce Earls</td>\n",
       "      <td>2016-09-23</td>\n",
       "      <td>5</td>\n",
       "      <td>sin comentario</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128018</th>\n",
       "      <td>12046149</td>\n",
       "      <td>0x863f79137a27e1e7:0x5e916e7fe74dcff5</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.1824656293609538e+20</td>\n",
       "      <td>Brenda Leffler</td>\n",
       "      <td>2012-09-28</td>\n",
       "      <td>3</td>\n",
       "      <td>sin comentario</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128019</th>\n",
       "      <td>12046150</td>\n",
       "      <td>0x863f79137a27e1e7:0x5e916e7fe74dcff5</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.0440035146720169e+20</td>\n",
       "      <td>Andrés Peña</td>\n",
       "      <td>2017-06-07</td>\n",
       "      <td>1</td>\n",
       "      <td>sin comentario</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128020</th>\n",
       "      <td>12046151</td>\n",
       "      <td>0x863f79137a27e1e7:0x5e916e7fe74dcff5</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.1718112804617642e+20</td>\n",
       "      <td>Shasha Robles</td>\n",
       "      <td>2018-05-15</td>\n",
       "      <td>3</td>\n",
       "      <td>sin comentario</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4128021 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         review_id                             id_negocio    estado  \\\n",
       "0              148  0x89c261f60bdf13db:0x38da730e4687a97b  New York   \n",
       "1              149  0x89c261f60bdf13db:0x38da730e4687a97b  New York   \n",
       "2              150  0x89c261f60bdf13db:0x38da730e4687a97b  New York   \n",
       "3              151  0x89c261f60bdf13db:0x38da730e4687a97b  New York   \n",
       "4              152  0x89c261f60bdf13db:0x38da730e4687a97b  New York   \n",
       "...            ...                                    ...       ...   \n",
       "4128016   12046147  0x863f79137a27e1e7:0x5e916e7fe74dcff5     Texas   \n",
       "4128017   12046148  0x863f79137a27e1e7:0x5e916e7fe74dcff5     Texas   \n",
       "4128018   12046149  0x863f79137a27e1e7:0x5e916e7fe74dcff5     Texas   \n",
       "4128019   12046150  0x863f79137a27e1e7:0x5e916e7fe74dcff5     Texas   \n",
       "4128020   12046151  0x863f79137a27e1e7:0x5e916e7fe74dcff5     Texas   \n",
       "\n",
       "                        user_id                    name       time  rating  \\\n",
       "0         1.018634613890113e+20  Maria Patricia Londoño 2021-08-16       4   \n",
       "1        1.0195463333253585e+20                 Kristal 2020-07-01       5   \n",
       "2        1.0898768409594107e+20        Efrain Hernandez 2020-07-07       3   \n",
       "3        1.1014632117097557e+20            Kiyoshi Sudo 2019-08-02       5   \n",
       "4        1.1262588515936902e+20      Charlotte Sheppard 2020-10-11       1   \n",
       "...                         ...                     ...        ...     ...   \n",
       "4128016   1.086757371066782e+20               Gary Kohr 2014-08-10       2   \n",
       "4128017  1.0378064778394966e+20             Joyce Earls 2016-09-23       5   \n",
       "4128018  1.1824656293609538e+20          Brenda Leffler 2012-09-28       3   \n",
       "4128019  1.0440035146720169e+20             Andrés Peña 2017-06-07       1   \n",
       "4128020  1.1718112804617642e+20           Shasha Robles 2018-05-15       3   \n",
       "\n",
       "                                                      text      resp_text  \\\n",
       "0        The donuts is always a good place to buy somet...  sin respuesta   \n",
       "1        I went into this Dunkin' yesterday and got som...  sin respuesta   \n",
       "2        Bought a Machiato and as soon as I touched the...  sin respuesta   \n",
       "3        Friendly staffs, nice donuts and muffins and c...  sin respuesta   \n",
       "4        They got my order wrong  food wasn't done unco...  sin respuesta   \n",
       "...                                                    ...            ...   \n",
       "4128016  Maybe it was bad because it's Sunday but only ...  sin respuesta   \n",
       "4128017                                     sin comentario  sin respuesta   \n",
       "4128018                                     sin comentario  sin respuesta   \n",
       "4128019                                     sin comentario  sin respuesta   \n",
       "4128020                                     sin comentario  sin respuesta   \n",
       "\n",
       "        resp_time  \n",
       "0             NaT  \n",
       "1             NaT  \n",
       "2             NaT  \n",
       "3             NaT  \n",
       "4             NaT  \n",
       "...           ...  \n",
       "4128016       NaT  \n",
       "4128017       NaT  \n",
       "4128018       NaT  \n",
       "4128019       NaT  \n",
       "4128020       NaT  \n",
       "\n",
       "[4128021 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "reviews = pd.read_parquet(r'C:\\Users\\mauri\\OneDrive\\Escritorio\\proyectogrupal\\data\\v0_estados_reviews.parquet', engine='pyarrow')\n",
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'C:\\Users\\mauri\\OneDrive\\Escritorio\\proyectogrupal\\src')\n",
    "from analisis_sentimiento.sentimiento import analyze_sentiment\n",
    "\n",
    "reviews['sentiment'] = reviews.apply(\n",
    "    lambda row: analyze_sentiment(row['text'], rating=row.get('rating', None)), \n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>id_negocio</th>\n",
       "      <th>estado</th>\n",
       "      <th>user_id</th>\n",
       "      <th>name</th>\n",
       "      <th>time</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>resp_time</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148</td>\n",
       "      <td>0x89c261f60bdf13db:0x38da730e4687a97b</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.018634613890113e+20</td>\n",
       "      <td>Maria Patricia Londoño</td>\n",
       "      <td>2021-08-16</td>\n",
       "      <td>4</td>\n",
       "      <td>The donuts is always a good place to buy somet...</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "      <td>bueno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>149</td>\n",
       "      <td>0x89c261f60bdf13db:0x38da730e4687a97b</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.0195463333253585e+20</td>\n",
       "      <td>Kristal</td>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>5</td>\n",
       "      <td>I went into this Dunkin' yesterday and got som...</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "      <td>bueno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150</td>\n",
       "      <td>0x89c261f60bdf13db:0x38da730e4687a97b</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.0898768409594107e+20</td>\n",
       "      <td>Efrain Hernandez</td>\n",
       "      <td>2020-07-07</td>\n",
       "      <td>3</td>\n",
       "      <td>Bought a Machiato and as soon as I touched the...</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>151</td>\n",
       "      <td>0x89c261f60bdf13db:0x38da730e4687a97b</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.1014632117097557e+20</td>\n",
       "      <td>Kiyoshi Sudo</td>\n",
       "      <td>2019-08-02</td>\n",
       "      <td>5</td>\n",
       "      <td>Friendly staffs, nice donuts and muffins and c...</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "      <td>bueno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>152</td>\n",
       "      <td>0x89c261f60bdf13db:0x38da730e4687a97b</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.1262588515936902e+20</td>\n",
       "      <td>Charlotte Sheppard</td>\n",
       "      <td>2020-10-11</td>\n",
       "      <td>1</td>\n",
       "      <td>They got my order wrong  food wasn't done unco...</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "      <td>malo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128016</th>\n",
       "      <td>12046147</td>\n",
       "      <td>0x863f79137a27e1e7:0x5e916e7fe74dcff5</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.086757371066782e+20</td>\n",
       "      <td>Gary Kohr</td>\n",
       "      <td>2014-08-10</td>\n",
       "      <td>2</td>\n",
       "      <td>Maybe it was bad because it's Sunday but only ...</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128017</th>\n",
       "      <td>12046148</td>\n",
       "      <td>0x863f79137a27e1e7:0x5e916e7fe74dcff5</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.0378064778394966e+20</td>\n",
       "      <td>Joyce Earls</td>\n",
       "      <td>2016-09-23</td>\n",
       "      <td>5</td>\n",
       "      <td>sin comentario</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "      <td>sin comentario</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128018</th>\n",
       "      <td>12046149</td>\n",
       "      <td>0x863f79137a27e1e7:0x5e916e7fe74dcff5</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.1824656293609538e+20</td>\n",
       "      <td>Brenda Leffler</td>\n",
       "      <td>2012-09-28</td>\n",
       "      <td>3</td>\n",
       "      <td>sin comentario</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "      <td>sin comentario</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128019</th>\n",
       "      <td>12046150</td>\n",
       "      <td>0x863f79137a27e1e7:0x5e916e7fe74dcff5</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.0440035146720169e+20</td>\n",
       "      <td>Andrés Peña</td>\n",
       "      <td>2017-06-07</td>\n",
       "      <td>1</td>\n",
       "      <td>sin comentario</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "      <td>sin comentario</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128020</th>\n",
       "      <td>12046151</td>\n",
       "      <td>0x863f79137a27e1e7:0x5e916e7fe74dcff5</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.1718112804617642e+20</td>\n",
       "      <td>Shasha Robles</td>\n",
       "      <td>2018-05-15</td>\n",
       "      <td>3</td>\n",
       "      <td>sin comentario</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "      <td>sin comentario</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4128021 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         review_id                             id_negocio    estado  \\\n",
       "0              148  0x89c261f60bdf13db:0x38da730e4687a97b  New York   \n",
       "1              149  0x89c261f60bdf13db:0x38da730e4687a97b  New York   \n",
       "2              150  0x89c261f60bdf13db:0x38da730e4687a97b  New York   \n",
       "3              151  0x89c261f60bdf13db:0x38da730e4687a97b  New York   \n",
       "4              152  0x89c261f60bdf13db:0x38da730e4687a97b  New York   \n",
       "...            ...                                    ...       ...   \n",
       "4128016   12046147  0x863f79137a27e1e7:0x5e916e7fe74dcff5     Texas   \n",
       "4128017   12046148  0x863f79137a27e1e7:0x5e916e7fe74dcff5     Texas   \n",
       "4128018   12046149  0x863f79137a27e1e7:0x5e916e7fe74dcff5     Texas   \n",
       "4128019   12046150  0x863f79137a27e1e7:0x5e916e7fe74dcff5     Texas   \n",
       "4128020   12046151  0x863f79137a27e1e7:0x5e916e7fe74dcff5     Texas   \n",
       "\n",
       "                        user_id                    name       time  rating  \\\n",
       "0         1.018634613890113e+20  Maria Patricia Londoño 2021-08-16       4   \n",
       "1        1.0195463333253585e+20                 Kristal 2020-07-01       5   \n",
       "2        1.0898768409594107e+20        Efrain Hernandez 2020-07-07       3   \n",
       "3        1.1014632117097557e+20            Kiyoshi Sudo 2019-08-02       5   \n",
       "4        1.1262588515936902e+20      Charlotte Sheppard 2020-10-11       1   \n",
       "...                         ...                     ...        ...     ...   \n",
       "4128016   1.086757371066782e+20               Gary Kohr 2014-08-10       2   \n",
       "4128017  1.0378064778394966e+20             Joyce Earls 2016-09-23       5   \n",
       "4128018  1.1824656293609538e+20          Brenda Leffler 2012-09-28       3   \n",
       "4128019  1.0440035146720169e+20             Andrés Peña 2017-06-07       1   \n",
       "4128020  1.1718112804617642e+20           Shasha Robles 2018-05-15       3   \n",
       "\n",
       "                                                      text      resp_text  \\\n",
       "0        The donuts is always a good place to buy somet...  sin respuesta   \n",
       "1        I went into this Dunkin' yesterday and got som...  sin respuesta   \n",
       "2        Bought a Machiato and as soon as I touched the...  sin respuesta   \n",
       "3        Friendly staffs, nice donuts and muffins and c...  sin respuesta   \n",
       "4        They got my order wrong  food wasn't done unco...  sin respuesta   \n",
       "...                                                    ...            ...   \n",
       "4128016  Maybe it was bad because it's Sunday but only ...  sin respuesta   \n",
       "4128017                                     sin comentario  sin respuesta   \n",
       "4128018                                     sin comentario  sin respuesta   \n",
       "4128019                                     sin comentario  sin respuesta   \n",
       "4128020                                     sin comentario  sin respuesta   \n",
       "\n",
       "        resp_time       sentiment  \n",
       "0             NaT           bueno  \n",
       "1             NaT           bueno  \n",
       "2             NaT          neutro  \n",
       "3             NaT           bueno  \n",
       "4             NaT            malo  \n",
       "...           ...             ...  \n",
       "4128016       NaT          neutro  \n",
       "4128017       NaT  sin comentario  \n",
       "4128018       NaT  sin comentario  \n",
       "4128019       NaT  sin comentario  \n",
       "4128020       NaT  sin comentario  \n",
       "\n",
       "[4128021 rows x 11 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews['sentiment'] = reviews.apply(lambda row: 'sin comentario' if row['text'] == 'sin comentario' else row['sentiment'], axis=1)\n",
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>id_negocio</th>\n",
       "      <th>estado</th>\n",
       "      <th>user_id</th>\n",
       "      <th>name</th>\n",
       "      <th>time</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>resp_time</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148</td>\n",
       "      <td>0x89c261f60bdf13db:0x38da730e4687a97b</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.018634613890113e+20</td>\n",
       "      <td>Maria Patricia Londoño</td>\n",
       "      <td>2021-08-16</td>\n",
       "      <td>4</td>\n",
       "      <td>The donuts is always a good place to buy somet...</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "      <td>bueno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>149</td>\n",
       "      <td>0x89c261f60bdf13db:0x38da730e4687a97b</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.0195463333253585e+20</td>\n",
       "      <td>Kristal</td>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>5</td>\n",
       "      <td>I went into this Dunkin' yesterday and got som...</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "      <td>bueno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150</td>\n",
       "      <td>0x89c261f60bdf13db:0x38da730e4687a97b</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.0898768409594107e+20</td>\n",
       "      <td>Efrain Hernandez</td>\n",
       "      <td>2020-07-07</td>\n",
       "      <td>3</td>\n",
       "      <td>Bought a Machiato and as soon as I touched the...</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>151</td>\n",
       "      <td>0x89c261f60bdf13db:0x38da730e4687a97b</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.1014632117097557e+20</td>\n",
       "      <td>Kiyoshi Sudo</td>\n",
       "      <td>2019-08-02</td>\n",
       "      <td>5</td>\n",
       "      <td>Friendly staffs, nice donuts and muffins and c...</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "      <td>bueno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>152</td>\n",
       "      <td>0x89c261f60bdf13db:0x38da730e4687a97b</td>\n",
       "      <td>New York</td>\n",
       "      <td>1.1262588515936902e+20</td>\n",
       "      <td>Charlotte Sheppard</td>\n",
       "      <td>2020-10-11</td>\n",
       "      <td>1</td>\n",
       "      <td>They got my order wrong  food wasn't done unco...</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "      <td>malo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128016</th>\n",
       "      <td>12046147</td>\n",
       "      <td>0x863f79137a27e1e7:0x5e916e7fe74dcff5</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.086757371066782e+20</td>\n",
       "      <td>Gary Kohr</td>\n",
       "      <td>2014-08-10</td>\n",
       "      <td>2</td>\n",
       "      <td>Maybe it was bad because it's Sunday but only ...</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "      <td>neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128017</th>\n",
       "      <td>12046148</td>\n",
       "      <td>0x863f79137a27e1e7:0x5e916e7fe74dcff5</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.0378064778394966e+20</td>\n",
       "      <td>Joyce Earls</td>\n",
       "      <td>2016-09-23</td>\n",
       "      <td>5</td>\n",
       "      <td>sin comentario</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "      <td>sin comentario</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128018</th>\n",
       "      <td>12046149</td>\n",
       "      <td>0x863f79137a27e1e7:0x5e916e7fe74dcff5</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.1824656293609538e+20</td>\n",
       "      <td>Brenda Leffler</td>\n",
       "      <td>2012-09-28</td>\n",
       "      <td>3</td>\n",
       "      <td>sin comentario</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "      <td>sin comentario</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128019</th>\n",
       "      <td>12046150</td>\n",
       "      <td>0x863f79137a27e1e7:0x5e916e7fe74dcff5</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.0440035146720169e+20</td>\n",
       "      <td>Andrés Peña</td>\n",
       "      <td>2017-06-07</td>\n",
       "      <td>1</td>\n",
       "      <td>sin comentario</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "      <td>sin comentario</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128020</th>\n",
       "      <td>12046151</td>\n",
       "      <td>0x863f79137a27e1e7:0x5e916e7fe74dcff5</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.1718112804617642e+20</td>\n",
       "      <td>Shasha Robles</td>\n",
       "      <td>2018-05-15</td>\n",
       "      <td>3</td>\n",
       "      <td>sin comentario</td>\n",
       "      <td>sin respuesta</td>\n",
       "      <td>NaT</td>\n",
       "      <td>sin comentario</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4128021 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         review_id                             id_negocio    estado  \\\n",
       "0              148  0x89c261f60bdf13db:0x38da730e4687a97b  New York   \n",
       "1              149  0x89c261f60bdf13db:0x38da730e4687a97b  New York   \n",
       "2              150  0x89c261f60bdf13db:0x38da730e4687a97b  New York   \n",
       "3              151  0x89c261f60bdf13db:0x38da730e4687a97b  New York   \n",
       "4              152  0x89c261f60bdf13db:0x38da730e4687a97b  New York   \n",
       "...            ...                                    ...       ...   \n",
       "4128016   12046147  0x863f79137a27e1e7:0x5e916e7fe74dcff5     Texas   \n",
       "4128017   12046148  0x863f79137a27e1e7:0x5e916e7fe74dcff5     Texas   \n",
       "4128018   12046149  0x863f79137a27e1e7:0x5e916e7fe74dcff5     Texas   \n",
       "4128019   12046150  0x863f79137a27e1e7:0x5e916e7fe74dcff5     Texas   \n",
       "4128020   12046151  0x863f79137a27e1e7:0x5e916e7fe74dcff5     Texas   \n",
       "\n",
       "                        user_id                    name       time  rating  \\\n",
       "0         1.018634613890113e+20  Maria Patricia Londoño 2021-08-16       4   \n",
       "1        1.0195463333253585e+20                 Kristal 2020-07-01       5   \n",
       "2        1.0898768409594107e+20        Efrain Hernandez 2020-07-07       3   \n",
       "3        1.1014632117097557e+20            Kiyoshi Sudo 2019-08-02       5   \n",
       "4        1.1262588515936902e+20      Charlotte Sheppard 2020-10-11       1   \n",
       "...                         ...                     ...        ...     ...   \n",
       "4128016   1.086757371066782e+20               Gary Kohr 2014-08-10       2   \n",
       "4128017  1.0378064778394966e+20             Joyce Earls 2016-09-23       5   \n",
       "4128018  1.1824656293609538e+20          Brenda Leffler 2012-09-28       3   \n",
       "4128019  1.0440035146720169e+20             Andrés Peña 2017-06-07       1   \n",
       "4128020  1.1718112804617642e+20           Shasha Robles 2018-05-15       3   \n",
       "\n",
       "                                                      text      resp_text  \\\n",
       "0        The donuts is always a good place to buy somet...  sin respuesta   \n",
       "1        I went into this Dunkin' yesterday and got som...  sin respuesta   \n",
       "2        Bought a Machiato and as soon as I touched the...  sin respuesta   \n",
       "3        Friendly staffs, nice donuts and muffins and c...  sin respuesta   \n",
       "4        They got my order wrong  food wasn't done unco...  sin respuesta   \n",
       "...                                                    ...            ...   \n",
       "4128016  Maybe it was bad because it's Sunday but only ...  sin respuesta   \n",
       "4128017                                     sin comentario  sin respuesta   \n",
       "4128018                                     sin comentario  sin respuesta   \n",
       "4128019                                     sin comentario  sin respuesta   \n",
       "4128020                                     sin comentario  sin respuesta   \n",
       "\n",
       "        resp_time       sentiment  \n",
       "0             NaT           bueno  \n",
       "1             NaT           bueno  \n",
       "2             NaT          neutro  \n",
       "3             NaT           bueno  \n",
       "4             NaT            malo  \n",
       "...           ...             ...  \n",
       "4128016       NaT          neutro  \n",
       "4128017       NaT  sin comentario  \n",
       "4128018       NaT  sin comentario  \n",
       "4128019       NaT  sin comentario  \n",
       "4128020       NaT  sin comentario  \n",
       "\n",
       "[4128021 rows x 11 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "reviews = pd.read_parquet(r'C:\\Users\\mauri\\OneDrive\\Escritorio\\proyectogrupal\\data\\v0_g_reviews.parquet', engine='pyarrow')\n",
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2380094 entries, 0 to 4127988\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Dtype         \n",
      "---  ------      -----         \n",
      " 0   review_id   int64         \n",
      " 1   id_negocio  object        \n",
      " 2   estado      object        \n",
      " 3   time        datetime64[ns]\n",
      " 4   rating      int64         \n",
      " 5   text        object        \n",
      " 6   sentiment   object        \n",
      "dtypes: datetime64[ns](1), int64(2), object(4)\n",
      "memory usage: 145.3+ MB\n"
     ]
    }
   ],
   "source": [
    "filtro = reviews[reviews['time'] >= '2019-01-01']\n",
    "filtro = filtro.drop(columns=['user_id', 'name', 'resp_text', 'resp_time'])\n",
    "filtro.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtro.to_parquet('bi_g_reviews.parquet', engine='pyarrow')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AAentorno",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
