{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que haremos aquí es recopilar los datos de las reviews de los estados que utilizaremos en el análisis. Los archivos en crudo se presentaron en formato Json, en archivos individuales para cada uno de los 51 estados de EEUU, por lo que procederemos a ordenar los mismos, en formato parquet, y por la dimension, en dataframes individuales, para luego crear una tabla única para los 5 estados pertinentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New York"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se convierten los archivos Json a formato parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convertido: 1.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\NORESTE_parquet\\Rev-New_York.parquet\\1.parquet\n",
      "Convertido: 10.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\NORESTE_parquet\\Rev-New_York.parquet\\10.parquet\n",
      "Convertido: 11.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\NORESTE_parquet\\Rev-New_York.parquet\\11.parquet\n",
      "Convertido: 12.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\NORESTE_parquet\\Rev-New_York.parquet\\12.parquet\n",
      "Convertido: 13.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\NORESTE_parquet\\Rev-New_York.parquet\\13.parquet\n",
      "Convertido: 14.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\NORESTE_parquet\\Rev-New_York.parquet\\14.parquet\n",
      "Convertido: 15.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\NORESTE_parquet\\Rev-New_York.parquet\\15.parquet\n",
      "Convertido: 16.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\NORESTE_parquet\\Rev-New_York.parquet\\16.parquet\n",
      "Convertido: 17.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\NORESTE_parquet\\Rev-New_York.parquet\\17.parquet\n",
      "Convertido: 18.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\NORESTE_parquet\\Rev-New_York.parquet\\18.parquet\n",
      "Convertido: 2.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\NORESTE_parquet\\Rev-New_York.parquet\\2.parquet\n",
      "Convertido: 3.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\NORESTE_parquet\\Rev-New_York.parquet\\3.parquet\n",
      "Convertido: 4.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\NORESTE_parquet\\Rev-New_York.parquet\\4.parquet\n",
      "Convertido: 5.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\NORESTE_parquet\\Rev-New_York.parquet\\5.parquet\n",
      "Convertido: 6.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\NORESTE_parquet\\Rev-New_York.parquet\\6.parquet\n",
      "Convertido: 7.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\NORESTE_parquet\\Rev-New_York.parquet\\7.parquet\n",
      "Convertido: 8.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\NORESTE_parquet\\Rev-New_York.parquet\\8.parquet\n",
      "Convertido: 9.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\NORESTE_parquet\\Rev-New_York.parquet\\9.parquet\n",
      "Conversión completada.\n"
     ]
    }
   ],
   "source": [
    "# Carpeta de entrada que contiene los archivos JSON\n",
    "input_folder = r'C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_json\\NORESTE\\review-New_York'\n",
    "# Carpeta de salida para guardar los archivos Parquet\n",
    "output_folder = r'C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\NORESTE_parquet\\Rev-New_York.parquet'\n",
    "\n",
    "# Crear la carpeta de salida si no existe\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterar sobre todos los archivos en la carpeta de entrada\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.json'):\n",
    "        # Ruta completa del archivo JSON\n",
    "        json_file_path = os.path.join(input_folder, filename)\n",
    "        \n",
    "        # Leer el archivo JSON\n",
    "        try:\n",
    "            df = pd.read_json(json_file_path, lines=True)\n",
    "            \n",
    "            # Crear nombre de archivo Parquet (sin la extensión .json)\n",
    "            parquet_file_path = os.path.join(output_folder, filename.replace('.json', '.parquet'))\n",
    "            \n",
    "            # Guardar el DataFrame como archivo Parquet\n",
    "            df.to_parquet(parquet_file_path, index=False, compression='snappy', engine='pyarrow')\n",
    "            \n",
    "            print(f\"Convertido: {filename} a {parquet_file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al convertir {filename}: {e}\")\n",
    "\n",
    "print(\"Conversión completada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2700000 entries, 0 to 2699999\n",
      "Data columns (total 8 columns):\n",
      " #   Column   Dtype  \n",
      "---  ------   -----  \n",
      " 0   user_id  float64\n",
      " 1   name     object \n",
      " 2   time     int64  \n",
      " 3   rating   int64  \n",
      " 4   text     object \n",
      " 5   pics     object \n",
      " 6   resp     object \n",
      " 7   gmap_id  object \n",
      "dtypes: float64(1), int64(2), object(5)\n",
      "memory usage: 164.8+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Leer el archivo Parquet\n",
    "Rev_New_York_1 = pd.read_parquet(r'C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\NORESTE_parquet\\Rev-New_York.parquet')\n",
    "\n",
    "print(Rev_New_York_1.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se concatenan los archivos parquet para formar un dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2700000, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function print(*args, sep=' ', end='\\n', file=None, flush=False)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carpeta que contiene los archivos Parquet\n",
    "input_folder_parquet = r'C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\NORESTE_parquet\\Rev-New_York.parquet'\n",
    "\n",
    "# Lista para almacenar DataFrames\n",
    "df_list_New_York = []\n",
    "\n",
    "# Iterar sobre todos los archivos Parquet en la carpeta de entrada\n",
    "for filename in os.listdir(input_folder_parquet):\n",
    "    if filename.endswith('.parquet'):\n",
    "        # Ruta completa del archivo Parquet\n",
    "        parquet_file_path_ny = os.path.join(input_folder_parquet, filename)\n",
    "        \n",
    "        # Leer el archivo Parquet y añadir el DataFrame a la lista\n",
    "        df = pd.read_parquet(parquet_file_path_ny)\n",
    "        df_list_New_York.append(df)\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_new_york = pd.concat(df_list_New_York, ignore_index=True)\n",
    "\n",
    "# Verificar el DataFrame combinado\n",
    "print(df_new_york.shape)\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se elimina columna irrelevante para el análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_york = df_new_york.drop(columns=['pics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se convierten las columnas necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1.1372210469230823e+20\n",
      "1    1.0729344149210933e+20\n",
      "2     1.003785858018194e+20\n",
      "Name: user_id, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Se convierte la columna 'user_id' a tipo object\n",
    "df_new_york['user_id'] = df_new_york['user_id'].astype('str')\n",
    "print(df_new_york['user_id'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2020-10-23\n",
      "1   2021-05-04\n",
      "2   2020-08-14\n",
      "Name: time, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# Se convierte la columna 'time' a datetime y luego a solo fecha manteniendo tipo datetime\n",
    "df_new_york['time'] = pd.to_datetime(df_new_york['time'], unit='ms').dt.normalize()\n",
    "print(df_new_york['time'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se desempaqueta la columna 'resp' y se agregan dos columnas al df\n",
    "df_new_york[['resp_text', 'resp_time']] = df_new_york['resp'].apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          NaT\n",
      "1   2021-05-06\n",
      "2          NaT\n",
      "Name: resp_time, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# Se convierte la columna 'resp_time' a datetime y luego a solo fecha manteniendo tipo datetime\n",
    "df_new_york['resp_time'] = pd.to_datetime(df_new_york['resp_time'], unit='ms').dt.normalize()\n",
    "print(df_new_york['resp_time'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se elimina la columna 'resp' original\n",
    "df_new_york = df_new_york.drop(columns=['resp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se renombra la columna gmap_id como 'id_negocio', para mayor comprensión en el análisis\n",
    "df_new_york = df_new_york.rename(columns={'gmap_id': 'id_negocio'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se reordena la columna 'id_negocio'\n",
    "columnas = list(df_new_york.columns)\n",
    "columnas.insert(0, columnas.pop(columnas.index('id_negocio')))\n",
    "df_new_york = df_new_york[columnas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se añade la columna 'review_id' autoincremental\n",
    "df_new_york.insert(0, 'review_id', range(1, 1 + len(df_new_york)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se añade la columna 'Estado'\n",
    "df_new_york.insert(2, 'Estado', 'New York')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2700000 entries, 0 to 2699999\n",
      "Data columns (total 10 columns):\n",
      " #   Column      Dtype         \n",
      "---  ------      -----         \n",
      " 0   review_id   int64         \n",
      " 1   id_negocio  object        \n",
      " 2   Estado      object        \n",
      " 3   user_id     object        \n",
      " 4   name        object        \n",
      " 5   time        datetime64[ns]\n",
      " 6   rating      int64         \n",
      " 7   text        object        \n",
      " 8   resp_text   object        \n",
      " 9   resp_time   datetime64[ns]\n",
      "dtypes: datetime64[ns](2), int64(2), object(6)\n",
      "memory usage: 206.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Se verifica como queda el dataframe\n",
    "print(df_new_york.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Mexico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se pasan de formato Json a Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convertido: 1.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\OESTE.parquet\\Rev-New_Mexico.parquet\\1.parquet\n",
      "Convertido: 10.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\OESTE.parquet\\Rev-New_Mexico.parquet\\10.parquet\n",
      "Convertido: 11.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\OESTE.parquet\\Rev-New_Mexico.parquet\\11.parquet\n",
      "Convertido: 12.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\OESTE.parquet\\Rev-New_Mexico.parquet\\12.parquet\n",
      "Convertido: 2.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\OESTE.parquet\\Rev-New_Mexico.parquet\\2.parquet\n",
      "Convertido: 3.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\OESTE.parquet\\Rev-New_Mexico.parquet\\3.parquet\n",
      "Convertido: 4.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\OESTE.parquet\\Rev-New_Mexico.parquet\\4.parquet\n",
      "Convertido: 5.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\OESTE.parquet\\Rev-New_Mexico.parquet\\5.parquet\n",
      "Convertido: 6.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\OESTE.parquet\\Rev-New_Mexico.parquet\\6.parquet\n",
      "Convertido: 7.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\OESTE.parquet\\Rev-New_Mexico.parquet\\7.parquet\n",
      "Convertido: 8.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\OESTE.parquet\\Rev-New_Mexico.parquet\\8.parquet\n",
      "Convertido: 9.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\OESTE.parquet\\Rev-New_Mexico.parquet\\9.parquet\n",
      "Conversión completada.\n"
     ]
    }
   ],
   "source": [
    "# Carpeta de entrada que contiene los archivos JSON\n",
    "input_folder = r'C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_json\\OESTE\\review-New_Mexico'\n",
    "# Carpeta de salida para guardar los archivos Parquet\n",
    "output_folder = r'C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\OESTE.parquet\\Rev-New_Mexico.parquet'\n",
    "\n",
    "# Crear la carpeta de salida si no existe\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterar sobre todos los archivos en la carpeta de entrada\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.json'):\n",
    "        # Ruta completa del archivo JSON\n",
    "        json_file_path = os.path.join(input_folder, filename)\n",
    "        \n",
    "        # Leer el archivo JSON\n",
    "        try:\n",
    "            df = pd.read_json(json_file_path, lines=True)\n",
    "            \n",
    "            # Crear nombre de archivo Parquet (sin la extensión .json)\n",
    "            parquet_file_path = os.path.join(output_folder, filename.replace('.json', '.parquet'))\n",
    "            \n",
    "            # Guardar el DataFrame como archivo Parquet\n",
    "            df.to_parquet(parquet_file_path, index=False, compression='snappy', engine='pyarrow')\n",
    "            \n",
    "            print(f\"Convertido: {filename} a {parquet_file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al convertir {filename}: {e}\")\n",
    "\n",
    "print(\"Conversión completada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1800000 entries, 0 to 1799999\n",
      "Data columns (total 8 columns):\n",
      " #   Column   Dtype  \n",
      "---  ------   -----  \n",
      " 0   user_id  float64\n",
      " 1   name     object \n",
      " 2   time     int64  \n",
      " 3   rating   int64  \n",
      " 4   text     object \n",
      " 5   pics     object \n",
      " 6   resp     object \n",
      " 7   gmap_id  object \n",
      "dtypes: float64(1), int64(2), object(5)\n",
      "memory usage: 109.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Leer el archivo Parquet\n",
    "Rev_New_Mexico_1 = pd.read_parquet(r'C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\OESTE.parquet\\Rev-New_Mexico.parquet')\n",
    "\n",
    "# Mostrar las primeras filas\n",
    "print(Rev_New_Mexico_1.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se concatenan los archivos .parquet para formar un Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800000, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function print(*args, sep=' ', end='\\n', file=None, flush=False)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carpeta que contiene los archivos Parquet\n",
    "input_folder_parquet = r'C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\OESTE.parquet\\Rev-New_Mexico.parquet'\n",
    "\n",
    "# Lista para almacenar DataFrames\n",
    "df_list_New_Mexico = []\n",
    "\n",
    "# Iterar sobre todos los archivos Parquet en la carpeta de entrada\n",
    "for filename in os.listdir(input_folder_parquet):\n",
    "    if filename.endswith('.parquet'):\n",
    "        # Ruta completa del archivo Parquet\n",
    "        parquet_file_path_nmex = os.path.join(input_folder_parquet, filename)\n",
    "        \n",
    "        # Leer el archivo Parquet y añadir el DataFrame a la lista\n",
    "        df = pd.read_parquet(parquet_file_path_nmex)\n",
    "        df_list_New_Mexico.append(df)\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_new_mexico = pd.concat(df_list_New_Mexico, ignore_index=True)\n",
    "\n",
    "# Verificar el DataFrame combinado\n",
    "print(df_new_mexico.shape)\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se elimina columna irrelevante para el análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_mexico = df_new_mexico.drop(columns=['pics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se convierten las columnas necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1.0901513065141957e+20\n",
      "1    1.1464198972453546e+20\n",
      "2    1.1817331659914551e+20\n",
      "Name: user_id, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Se convierte la columna 'user_id' a tipo object\n",
    "df_new_mexico['user_id'] = df_new_mexico['user_id'].astype('str')\n",
    "print(df_new_mexico['user_id'].head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2020-12-07\n",
      "1   2019-05-20\n",
      "2   2018-11-20\n",
      "Name: time, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# Se convierte la columna 'time' a datetime y luego a solo fecha manteniendo tipo datetime\n",
    "df_new_mexico['time'] = pd.to_datetime(df_new_mexico['time'], unit='ms').dt.normalize()\n",
    "print(df_new_mexico['time'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se desempaqueta la columna 'resp' y se agregan dos columnas al df\n",
    "df_new_mexico[['resp_text', 'resp_time']] = df_new_mexico['resp'].apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2021-01-12\n",
      "1   2019-05-22\n",
      "2   2018-11-20\n",
      "Name: resp_time, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# Se convierte la columna 'time' a datetime y luego a solo fecha manteniendo tipo datetime\n",
    "df_new_mexico['resp_time'] = pd.to_datetime(df_new_mexico['resp_time'], unit='ms').dt.normalize()\n",
    "print(df_new_mexico['resp_time'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se elimina la columna 'resp' original\n",
    "df_new_mexico = df_new_mexico.drop(columns=['resp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se renombra la columna gmap_id como 'id_negocio', para mayor comprensión en el análisis\n",
    "df_new_mexico = df_new_mexico.rename(columns={'gmap_id': 'id_negocio'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se reordena la columna 'id_negocio'\n",
    "columnas = list(df_new_mexico.columns)\n",
    "columnas.insert(0, columnas.pop(columnas.index('id_negocio')))\n",
    "df_new_mexico = df_new_mexico[columnas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se añade la columna 'review_id' autoincremental desde 2700001 para continuar desde el df_new_york\n",
    "df_new_mexico.insert(0, 'review_id', range(2700001, 2700001 + len(df_new_mexico)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se añade la columna 'Estado'\n",
    "df_new_mexico.insert(2, 'Estado', 'New Mexico')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1800000 entries, 0 to 1799999\n",
      "Data columns (total 10 columns):\n",
      " #   Column      Dtype         \n",
      "---  ------      -----         \n",
      " 0   review_id   int64         \n",
      " 1   id_negocio  object        \n",
      " 2   user_id     object        \n",
      " 3   Estado      object        \n",
      " 4   name        object        \n",
      " 5   time        datetime64[ns]\n",
      " 6   rating      int64         \n",
      " 7   text        object        \n",
      " 8   resp_text   object        \n",
      " 9   resp_time   datetime64[ns]\n",
      "dtypes: datetime64[ns](2), int64(2), object(6)\n",
      "memory usage: 137.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Se verifica como queda el dataframe\n",
    "print(df_new_mexico.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pennsylvania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se convierten los archivos Json a formato parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convertido: 1.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\NORESTE_parquet\\Rev-Pennsylvania.parquet\\1.parquet\n",
      "Convertido: 10.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\NORESTE_parquet\\Rev-Pennsylvania.parquet\\10.parquet\n",
      "Convertido: 11.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\NORESTE_parquet\\Rev-Pennsylvania.parquet\\11.parquet\n",
      "Convertido: 12.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\NORESTE_parquet\\Rev-Pennsylvania.parquet\\12.parquet\n",
      "Convertido: 13.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\NORESTE_parquet\\Rev-Pennsylvania.parquet\\13.parquet\n",
      "Convertido: 14.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\NORESTE_parquet\\Rev-Pennsylvania.parquet\\14.parquet\n",
      "Convertido: 15.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\NORESTE_parquet\\Rev-Pennsylvania.parquet\\15.parquet\n",
      "Convertido: 16.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\NORESTE_parquet\\Rev-Pennsylvania.parquet\\16.parquet\n",
      "Convertido: 2.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\NORESTE_parquet\\Rev-Pennsylvania.parquet\\2.parquet\n",
      "Convertido: 3.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\NORESTE_parquet\\Rev-Pennsylvania.parquet\\3.parquet\n",
      "Convertido: 4.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\NORESTE_parquet\\Rev-Pennsylvania.parquet\\4.parquet\n",
      "Convertido: 5.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\NORESTE_parquet\\Rev-Pennsylvania.parquet\\5.parquet\n",
      "Convertido: 6.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\NORESTE_parquet\\Rev-Pennsylvania.parquet\\6.parquet\n",
      "Convertido: 7.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\NORESTE_parquet\\Rev-Pennsylvania.parquet\\7.parquet\n",
      "Convertido: 8.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\NORESTE_parquet\\Rev-Pennsylvania.parquet\\8.parquet\n",
      "Convertido: 9.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\NORESTE_parquet\\Rev-Pennsylvania.parquet\\9.parquet\n",
      "Conversión completada.\n"
     ]
    }
   ],
   "source": [
    "# Carpeta de entrada que contiene los archivos JSON\n",
    "input_folder = r'C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_json\\NORESTE\\review-Pennsylvania'\n",
    "# Carpeta de salida para guardar los archivos Parquet\n",
    "output_folder = r'C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\NORESTE_parquet\\Rev-Pennsylvania.parquet'\n",
    "\n",
    "# Crear la carpeta de salida si no existe\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterar sobre todos los archivos en la carpeta de entrada\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.json'):\n",
    "        # Ruta completa del archivo JSON\n",
    "        json_file_path = os.path.join(input_folder, filename)\n",
    "        \n",
    "        # Leer el archivo JSON\n",
    "        try:\n",
    "            df = pd.read_json(json_file_path, lines=True)\n",
    "            \n",
    "            # Crear nombre de archivo Parquet (sin la extensión .json)\n",
    "            parquet_file_path = os.path.join(output_folder, filename.replace('.json', '.parquet'))\n",
    "            \n",
    "            # Guardar el DataFrame como archivo Parquet\n",
    "            df.to_parquet(parquet_file_path, index=False, compression='snappy', engine='pyarrow')\n",
    "            \n",
    "            print(f\"Convertido: {filename} a {parquet_file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al convertir {filename}: {e}\")\n",
    "\n",
    "print(\"Conversión completada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2400000 entries, 0 to 2399999\n",
      "Data columns (total 8 columns):\n",
      " #   Column   Dtype  \n",
      "---  ------   -----  \n",
      " 0   user_id  float64\n",
      " 1   name     object \n",
      " 2   time     int64  \n",
      " 3   rating   int64  \n",
      " 4   text     object \n",
      " 5   pics     object \n",
      " 6   resp     object \n",
      " 7   gmap_id  object \n",
      "dtypes: float64(1), int64(2), object(5)\n",
      "memory usage: 146.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Leer el archivo Parquet\n",
    "Rev_Pennsylvania_1 = pd.read_parquet(r'C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\NORESTE_parquet\\Rev-Pennsylvania.parquet')\n",
    "\n",
    "print(Rev_Pennsylvania_1.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se concatenan los archivos parquet para formar un solo dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400000, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function print(*args, sep=' ', end='\\n', file=None, flush=False)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carpeta que contiene los archivos Parquet\n",
    "input_folder_parquet = r'C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\NORESTE_parquet\\Rev-Pennsylvania.parquet'\n",
    "\n",
    "# Lista para almacenar DataFrames\n",
    "df_list_Pennsylvania = []\n",
    "\n",
    "# Iterar sobre todos los archivos Parquet en la carpeta de entrada\n",
    "for filename in os.listdir(input_folder_parquet):\n",
    "    if filename.endswith('.parquet'):\n",
    "        # Ruta completa del archivo Parquet\n",
    "        parquet_file_path_psv = os.path.join(input_folder_parquet, filename)\n",
    "        \n",
    "        # Leer el archivo Parquet y añadir el DataFrame a la lista\n",
    "        df = pd.read_parquet(parquet_file_path_psv)\n",
    "        df_list_Pennsylvania.append(df)\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_pennsylvania = pd.concat(df_list_Pennsylvania, ignore_index=True)\n",
    "\n",
    "# Verificar el DataFrame combinado\n",
    "print(df_pennsylvania.shape)\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se elimina columna irrelevante para el análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pennsylvania = df_pennsylvania.drop(columns=['pics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se convierten las columnas necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1.0494474255907976e+20\n",
      "1    1.1760970283298033e+20\n",
      "2    1.1056324201842663e+20\n",
      "Name: user_id, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Se convierte la columna 'user_id' a tipo object\n",
    "df_pennsylvania['user_id'] = df_pennsylvania['user_id'].astype('str')\n",
    "print(df_pennsylvania['user_id'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2018-02-04\n",
      "1   2016-10-12\n",
      "2   2012-06-04\n",
      "Name: time, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# Se convierte la columna 'time' a datetime y luego a solo fecha manteniendo tipo datetime\n",
    "df_pennsylvania['time'] = pd.to_datetime(df_pennsylvania['time'], unit='ms').dt.normalize()\n",
    "print(df_pennsylvania['time'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se desempaqueta la columna 'resp' y se agregan dos columnas al df\n",
    "df_pennsylvania[['resp_text', 'resp_time']] = df_pennsylvania['resp'].apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   NaT\n",
      "1   NaT\n",
      "2   NaT\n",
      "Name: resp_time, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# Se convierte la columna 'resp_time' a datetime y luego a solo fecha manteniendo tipo datetime\n",
    "df_pennsylvania['resp_time'] = pd.to_datetime(df_pennsylvania['resp_time'], unit='ms').dt.normalize()\n",
    "print(df_pennsylvania['resp_time'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se elimina la columna 'resp' original\n",
    "df_pennsylvania = df_pennsylvania.drop(columns=['resp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se renombra la columna gmap_id como 'id_negocio', para mayor comprensión en el análisis\n",
    "df_pennsylvania = df_pennsylvania.rename(columns={'gmap_id': 'id_negocio'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se reordena la columna 'id_negocio'\n",
    "columnas = list(df_pennsylvania.columns)\n",
    "columnas.insert(0, columnas.pop(columnas.index('id_negocio')))\n",
    "df_pennsylvania = df_pennsylvania[columnas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se añade la columna 'review_id' autoincremental desde 4500001 para continuar desde el df_new_mexico\n",
    "df_pennsylvania.insert(0, 'review_id', range(4500001, 4500001 + len(df_pennsylvania)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se añade la columna 'Estado'\n",
    "df_pennsylvania.insert(2, 'Estado', 'Pennsylvania')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2400000 entries, 0 to 2399999\n",
      "Data columns (total 10 columns):\n",
      " #   Column      Dtype         \n",
      "---  ------      -----         \n",
      " 0   review_id   int64         \n",
      " 1   id_negocio  object        \n",
      " 2   Estado      object        \n",
      " 3   user_id     object        \n",
      " 4   name        object        \n",
      " 5   time        datetime64[ns]\n",
      " 6   rating      int64         \n",
      " 7   text        object        \n",
      " 8   resp_text   object        \n",
      " 9   resp_time   datetime64[ns]\n",
      "dtypes: datetime64[ns](2), int64(2), object(6)\n",
      "memory usage: 183.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Se verifica como queda el dataframe\n",
    "print(df_pennsylvania.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Florida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se convierten los archivos Json a formato parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convertido: 1.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\SUR_parquet\\Rev-Florida.parquet\\1.parquet\n",
      "Convertido: 10.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\SUR_parquet\\Rev-Florida.parquet\\10.parquet\n",
      "Convertido: 11.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\SUR_parquet\\Rev-Florida.parquet\\11.parquet\n",
      "Convertido: 12.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\SUR_parquet\\Rev-Florida.parquet\\12.parquet\n",
      "Convertido: 13.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\SUR_parquet\\Rev-Florida.parquet\\13.parquet\n",
      "Convertido: 14.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\SUR_parquet\\Rev-Florida.parquet\\14.parquet\n",
      "Convertido: 15.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\SUR_parquet\\Rev-Florida.parquet\\15.parquet\n",
      "Convertido: 16.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\SUR_parquet\\Rev-Florida.parquet\\16.parquet\n",
      "Convertido: 17.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\SUR_parquet\\Rev-Florida.parquet\\17.parquet\n",
      "Convertido: 18.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\SUR_parquet\\Rev-Florida.parquet\\18.parquet\n",
      "Convertido: 19.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\SUR_parquet\\Rev-Florida.parquet\\19.parquet\n",
      "Convertido: 2.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\SUR_parquet\\Rev-Florida.parquet\\2.parquet\n",
      "Convertido: 3.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\SUR_parquet\\Rev-Florida.parquet\\3.parquet\n",
      "Convertido: 4.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\SUR_parquet\\Rev-Florida.parquet\\4.parquet\n",
      "Convertido: 5.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\SUR_parquet\\Rev-Florida.parquet\\5.parquet\n",
      "Convertido: 6.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\SUR_parquet\\Rev-Florida.parquet\\6.parquet\n",
      "Convertido: 7.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\SUR_parquet\\Rev-Florida.parquet\\7.parquet\n",
      "Convertido: 8.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\SUR_parquet\\Rev-Florida.parquet\\8.parquet\n",
      "Convertido: 9.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\SUR_parquet\\Rev-Florida.parquet\\9.parquet\n",
      "Conversión completada.\n"
     ]
    }
   ],
   "source": [
    "# Carpeta de entrada que contiene los archivos JSON\n",
    "input_folder = r'C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_json\\SUR\\review-Florida'\n",
    "# Carpeta de salida para guardar los archivos Parquet\n",
    "output_folder = r'C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\SUR_parquet\\Rev-Florida.parquet'\n",
    "\n",
    "# Crear la carpeta de salida si no existe\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterar sobre todos los archivos en la carpeta de entrada\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.json'):\n",
    "        # Ruta completa del archivo JSON\n",
    "        json_file_path = os.path.join(input_folder, filename)\n",
    "        \n",
    "        # Leer el archivo JSON\n",
    "        try:\n",
    "            df = pd.read_json(json_file_path, lines=True)\n",
    "            \n",
    "            # Crear nombre de archivo Parquet (sin la extensión .json)\n",
    "            parquet_file_path = os.path.join(output_folder, filename.replace('.json', '.parquet'))\n",
    "            \n",
    "            # Guardar el DataFrame como archivo Parquet\n",
    "            df.to_parquet(parquet_file_path, index=False, compression='snappy', engine='pyarrow')\n",
    "            \n",
    "            print(f\"Convertido: {filename} a {parquet_file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al convertir {filename}: {e}\")\n",
    "\n",
    "print(\"Conversión completada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2850000 entries, 0 to 2849999\n",
      "Data columns (total 8 columns):\n",
      " #   Column   Dtype  \n",
      "---  ------   -----  \n",
      " 0   user_id  float64\n",
      " 1   name     object \n",
      " 2   time     int64  \n",
      " 3   rating   int64  \n",
      " 4   text     object \n",
      " 5   pics     object \n",
      " 6   resp     object \n",
      " 7   gmap_id  object \n",
      "dtypes: float64(1), int64(2), object(5)\n",
      "memory usage: 174.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Leer el archivo Parquet\n",
    "Rev_Florida_1 = pd.read_parquet(r'C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\SUR_parquet\\Rev-Florida.parquet')\n",
    "\n",
    "print(Rev_Florida_1.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se concatenan los archivos parquet para formar un solo dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2850000, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function print(*args, sep=' ', end='\\n', file=None, flush=False)>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carpeta que contiene los archivos Parquet\n",
    "input_folder_parquet = r'C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\SUR_parquet\\Rev-Florida.parquet'\n",
    "\n",
    "# Lista para almacenar DataFrames\n",
    "df_list_Florida = []\n",
    "\n",
    "# Iterar sobre todos los archivos Parquet en la carpeta de entrada\n",
    "for filename in os.listdir(input_folder_parquet):\n",
    "    if filename.endswith('.parquet'):\n",
    "        # Ruta completa del archivo Parquet\n",
    "        parquet_file_path_psv = os.path.join(input_folder_parquet, filename)\n",
    "        \n",
    "        # Leer el archivo Parquet y añadir el DataFrame a la lista\n",
    "        df = pd.read_parquet(parquet_file_path_psv)\n",
    "        df_list_Florida.append(df)\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_florida = pd.concat(df_list_Florida, ignore_index=True)\n",
    "\n",
    "# Verificar el DataFrame combinado\n",
    "print(df_florida.shape)\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se elimina columna irrelevante para el análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_florida = df_florida.drop(columns=['pics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se convierten las columnas necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1.0147185615514873e+20\n",
      "1    1.1547723478903833e+20\n",
      "2    1.0180501024489284e+20\n",
      "Name: user_id, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Se convierte la columna 'user_id' a tipo object\n",
    "df_florida['user_id'] = df_florida['user_id'].astype('str')\n",
    "print(df_florida['user_id'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2021-08-03\n",
      "1   2020-07-18\n",
      "2   2018-04-05\n",
      "Name: time, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# Se convierte la columna 'time' a datetime y luego a solo fecha manteniendo tipo datetime\n",
    "df_florida['time'] = pd.to_datetime(df_florida['time'], unit='ms').dt.normalize()\n",
    "print(df_florida['time'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se desempaqueta la columna 'resp' y se agregan dos columnas al df\n",
    "df_florida[['resp_text', 'resp_time']] = df_florida['resp'].apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2021-07-23\n",
      "1   2020-02-23\n",
      "2          NaT\n",
      "Name: resp_time, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# Se convierte la columna 'resp_time' a datetime y luego a solo fecha manteniendo tipo datetime\n",
    "df_florida['resp_time'] = pd.to_datetime(df_florida['resp_time'], unit='ms').dt.normalize()\n",
    "print(df_florida['resp_time'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se elimina la columna 'resp' original\n",
    "df_florida = df_florida.drop(columns=['resp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se renombra la columna gmap_id como 'id_negocio', para mayor comprensión en el análisis\n",
    "df_florida = df_florida.rename(columns={'gmap_id': 'id_negocio'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se reordena la columna 'id_negocio'\n",
    "columnas = list(df_florida.columns)\n",
    "columnas.insert(0, columnas.pop(columnas.index('id_negocio')))\n",
    "df_florida = df_florida[columnas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se añade la columna 'review_id' autoincremental desde 6900001 para continuar desde el df_pennsylvania\n",
    "df_florida.insert(0, 'review_id', range(6900001, 6900001 + len(df_florida)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se añade la columna 'Estado'\n",
    "df_florida.insert(2, 'Estado', 'Pennsylvania')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2850000 entries, 0 to 2849999\n",
      "Data columns (total 10 columns):\n",
      " #   Column      Dtype         \n",
      "---  ------      -----         \n",
      " 0   review_id   int64         \n",
      " 1   id_negocio  object        \n",
      " 2   Estado      object        \n",
      " 3   user_id     object        \n",
      " 4   name        object        \n",
      " 5   time        datetime64[ns]\n",
      " 6   rating      int64         \n",
      " 7   text        object        \n",
      " 8   resp_text   object        \n",
      " 9   resp_time   datetime64[ns]\n",
      "dtypes: datetime64[ns](2), int64(2), object(6)\n",
      "memory usage: 217.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Se verifica como queda el dataframe\n",
    "print(df_florida.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>id_negocio</th>\n",
       "      <th>Estado</th>\n",
       "      <th>user_id</th>\n",
       "      <th>name</th>\n",
       "      <th>time</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>resp_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2849995</th>\n",
       "      <td>9749996</td>\n",
       "      <td>0x8890966585e36d3f:0x131d47c2c60a8d31</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>1.0028771480080918e+20</td>\n",
       "      <td>James Rudolph</td>\n",
       "      <td>2021-04-01</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>Thank you!</td>\n",
       "      <td>2021-04-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2849996</th>\n",
       "      <td>9749997</td>\n",
       "      <td>0x8890966585e36d3f:0x131d47c2c60a8d31</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>1.1409893309742336e+20</td>\n",
       "      <td>Vincent Alexander</td>\n",
       "      <td>2020-02-16</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2849997</th>\n",
       "      <td>9749998</td>\n",
       "      <td>0x8890966585e36d3f:0x131d47c2c60a8d31</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>1.1234480481217744e+20</td>\n",
       "      <td>Brett Owen</td>\n",
       "      <td>2019-02-27</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2849998</th>\n",
       "      <td>9749999</td>\n",
       "      <td>0x8890966585e36d3f:0x131d47c2c60a8d31</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>1.0544647183143027e+20</td>\n",
       "      <td>ashly kindle</td>\n",
       "      <td>2018-03-15</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>Thank you, Ashly</td>\n",
       "      <td>2018-01-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2849999</th>\n",
       "      <td>9750000</td>\n",
       "      <td>0x8890966585e36d3f:0x131d47c2c60a8d31</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>1.0874328165743621e+20</td>\n",
       "      <td>Jomarra Auge</td>\n",
       "      <td>2018-09-09</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>Thank you, Mrs. Jomarra:)</td>\n",
       "      <td>2018-09-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         review_id                             id_negocio        Estado  \\\n",
       "2849995    9749996  0x8890966585e36d3f:0x131d47c2c60a8d31  Pennsylvania   \n",
       "2849996    9749997  0x8890966585e36d3f:0x131d47c2c60a8d31  Pennsylvania   \n",
       "2849997    9749998  0x8890966585e36d3f:0x131d47c2c60a8d31  Pennsylvania   \n",
       "2849998    9749999  0x8890966585e36d3f:0x131d47c2c60a8d31  Pennsylvania   \n",
       "2849999    9750000  0x8890966585e36d3f:0x131d47c2c60a8d31  Pennsylvania   \n",
       "\n",
       "                        user_id               name       time  rating  text  \\\n",
       "2849995  1.0028771480080918e+20      James Rudolph 2021-04-01       5  None   \n",
       "2849996  1.1409893309742336e+20  Vincent Alexander 2020-02-16       3  None   \n",
       "2849997  1.1234480481217744e+20         Brett Owen 2019-02-27       5  None   \n",
       "2849998  1.0544647183143027e+20       ashly kindle 2018-03-15       5  None   \n",
       "2849999  1.0874328165743621e+20       Jomarra Auge 2018-09-09       5  None   \n",
       "\n",
       "                         resp_text  resp_time  \n",
       "2849995                 Thank you! 2021-04-04  \n",
       "2849996                        NaN        NaT  \n",
       "2849997                        NaN        NaT  \n",
       "2849998           Thank you, Ashly 2018-01-25  \n",
       "2849999  Thank you, Mrs. Jomarra:) 2018-09-09  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_florida.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Texas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se convierten los archivos Json a formato parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convertido: 1.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\SUR_parquet\\Rev-Texas.parquet\\1.parquet\n",
      "Convertido: 10.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\SUR_parquet\\Rev-Texas.parquet\\10.parquet\n",
      "Convertido: 11.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\SUR_parquet\\Rev-Texas.parquet\\11.parquet\n",
      "Convertido: 12.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\SUR_parquet\\Rev-Texas.parquet\\12.parquet\n",
      "Convertido: 13.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\SUR_parquet\\Rev-Texas.parquet\\13.parquet\n",
      "Convertido: 14.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\SUR_parquet\\Rev-Texas.parquet\\14.parquet\n",
      "Convertido: 15.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\SUR_parquet\\Rev-Texas.parquet\\15.parquet\n",
      "Convertido: 16.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\SUR_parquet\\Rev-Texas.parquet\\16.parquet\n",
      "Convertido: 2.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\SUR_parquet\\Rev-Texas.parquet\\2.parquet\n",
      "Convertido: 3.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\SUR_parquet\\Rev-Texas.parquet\\3.parquet\n",
      "Convertido: 4.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\SUR_parquet\\Rev-Texas.parquet\\4.parquet\n",
      "Convertido: 5.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\SUR_parquet\\Rev-Texas.parquet\\5.parquet\n",
      "Convertido: 6.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\SUR_parquet\\Rev-Texas.parquet\\6.parquet\n",
      "Convertido: 7.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\SUR_parquet\\Rev-Texas.parquet\\7.parquet\n",
      "Convertido: 8.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\SUR_parquet\\Rev-Texas.parquet\\8.parquet\n",
      "Convertido: 9.json a C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\SUR_parquet\\Rev-Texas.parquet\\9.parquet\n",
      "Conversión completada.\n"
     ]
    }
   ],
   "source": [
    "# Carpeta de entrada que contiene los archivos JSON\n",
    "input_folder = r'C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_json\\SUR\\review-Texas'\n",
    "# Carpeta de salida para guardar los archivos Parquet\n",
    "output_folder = r'C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\SUR_parquet\\Rev-Texas.parquet'\n",
    "\n",
    "# Crear la carpeta de salida si no existe\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterar sobre todos los archivos en la carpeta de entrada\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.json'):\n",
    "        # Ruta completa del archivo JSON\n",
    "        json_file_path = os.path.join(input_folder, filename)\n",
    "        \n",
    "        # Leer el archivo JSON\n",
    "        try:\n",
    "            df = pd.read_json(json_file_path, lines=True)\n",
    "            \n",
    "            # Crear nombre de archivo Parquet (sin la extensión .json)\n",
    "            parquet_file_path = os.path.join(output_folder, filename.replace('.json', '.parquet'))\n",
    "            \n",
    "            # Guardar el DataFrame como archivo Parquet\n",
    "            df.to_parquet(parquet_file_path, index=False, compression='snappy', engine='pyarrow')\n",
    "            \n",
    "            print(f\"Convertido: {filename} a {parquet_file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al convertir {filename}: {e}\")\n",
    "\n",
    "print(\"Conversión completada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2296824 entries, 0 to 2296823\n",
      "Data columns (total 8 columns):\n",
      " #   Column   Dtype  \n",
      "---  ------   -----  \n",
      " 0   user_id  float64\n",
      " 1   name     object \n",
      " 2   time     int64  \n",
      " 3   rating   int64  \n",
      " 4   text     object \n",
      " 5   pics     object \n",
      " 6   resp     object \n",
      " 7   gmap_id  object \n",
      "dtypes: float64(1), int64(2), object(5)\n",
      "memory usage: 140.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Leer el archivo Parquet\n",
    "Rev_Texas_1 = pd.read_parquet(r'C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\SUR_parquet\\Rev-Texas.parquet')\n",
    "\n",
    "print(Rev_Texas_1.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se concatenan los archivos parquet para formar un solo dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400000, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function print(*args, sep=' ', end='\\n', file=None, flush=False)>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carpeta que contiene los archivos Parquet\n",
    "input_folder_parquet = r'C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\estados_parquet\\NORESTE_parquet\\Rev-Pennsylvania.parquet'\n",
    "\n",
    "# Lista para almacenar DataFrames\n",
    "df_list_Texas = []\n",
    "\n",
    "# Iterar sobre todos los archivos Parquet en la carpeta de entrada\n",
    "for filename in os.listdir(input_folder_parquet):\n",
    "    if filename.endswith('.parquet'):\n",
    "        # Ruta completa del archivo Parquet\n",
    "        parquet_file_path_psv = os.path.join(input_folder_parquet, filename)\n",
    "        \n",
    "        # Leer el archivo Parquet y añadir el DataFrame a la lista\n",
    "        df = pd.read_parquet(parquet_file_path_psv)\n",
    "        df_list_Texas.append(df)\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_texas = pd.concat(df_list_Texas, ignore_index=True)\n",
    "\n",
    "# Verificar el DataFrame combinado\n",
    "print(df_texas.shape)\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se elimina columna irrelevante para el análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_texas = df_texas.drop(columns=['pics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se convierten las columnas necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1.0494474255907976e+20\n",
      "1    1.1760970283298033e+20\n",
      "2    1.1056324201842663e+20\n",
      "Name: user_id, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Se convierte la columna 'user_id' a tipo object\n",
    "df_texas['user_id'] = df_texas['user_id'].astype('str')\n",
    "print(df_texas['user_id'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2018-02-04\n",
      "1   2016-10-12\n",
      "2   2012-06-04\n",
      "Name: time, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# Se convierte la columna 'time' a datetime y luego a solo fecha manteniendo tipo datetime\n",
    "df_texas['time'] = pd.to_datetime(df_texas['time'], unit='ms').dt.normalize()\n",
    "print(df_texas['time'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se desempaqueta la columna 'resp' y se agregan dos columnas al df\n",
    "df_texas[['resp_text', 'resp_time']] = df_texas['resp'].apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   NaT\n",
      "1   NaT\n",
      "2   NaT\n",
      "Name: resp_time, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# Se convierte la columna 'resp_time' a datetime y luego a solo fecha manteniendo tipo datetime\n",
    "df_texas['resp_time'] = pd.to_datetime(df_texas['resp_time'], unit='ms').dt.normalize()\n",
    "print(df_texas['resp_time'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se elimina la columna 'resp' original\n",
    "df_texas = df_texas.drop(columns=['resp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se renombra la columna gmap_id como 'id_negocio', para mayor comprensión en el análisis\n",
    "df_texas = df_texas.rename(columns={'gmap_id': 'id_negocio'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se reordena la columna 'id_negocio'\n",
    "columnas = list(df_texas.columns)\n",
    "columnas.insert(0, columnas.pop(columnas.index('id_negocio')))\n",
    "df_texas = df_texas[columnas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se añade la columna 'review_id' autoincremental desde 9750001 para continuar desde el df_florida\n",
    "df_texas.insert(0, 'review_id', range(9750001, 9750001 + len(df_texas)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se añade la columna 'Estado'\n",
    "df_texas.insert(2, 'Estado', 'Pennsylvania')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2400000 entries, 0 to 2399999\n",
      "Data columns (total 10 columns):\n",
      " #   Column      Dtype         \n",
      "---  ------      -----         \n",
      " 0   review_id   int64         \n",
      " 1   id_negocio  object        \n",
      " 2   Estado      object        \n",
      " 3   user_id     object        \n",
      " 4   name        object        \n",
      " 5   time        datetime64[ns]\n",
      " 6   rating      int64         \n",
      " 7   text        object        \n",
      " 8   resp_text   object        \n",
      " 9   resp_time   datetime64[ns]\n",
      "dtypes: datetime64[ns](2), int64(2), object(6)\n",
      "memory usage: 183.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Se verifica como queda el dataframe\n",
    "print(df_texas.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se concatenan los  DataFrames anteriores para formar uno solo\n",
    "g_estados_seleccionados = pd.concat([df_new_york, df_new_mexico, df_pennsylvania, df_florida, df_texas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 12150000 entries, 0 to 2399999\n",
      "Data columns (total 10 columns):\n",
      " #   Column      Dtype         \n",
      "---  ------      -----         \n",
      " 0   review_id   int64         \n",
      " 1   id_negocio  object        \n",
      " 2   Estado      object        \n",
      " 3   user_id     object        \n",
      " 4   name        object        \n",
      " 5   time        datetime64[ns]\n",
      " 6   rating      int64         \n",
      " 7   text        object        \n",
      " 8   resp_text   object        \n",
      " 9   resp_time   datetime64[ns]\n",
      "dtypes: datetime64[ns](2), int64(2), object(6)\n",
      "memory usage: 1019.7+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(g_estados_seleccionados.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_id            0\n",
      "id_negocio           0\n",
      "Estado               0\n",
      "user_id              0\n",
      "name                 0\n",
      "time                 0\n",
      "rating               0\n",
      "text           5113655\n",
      "resp_text     10612641\n",
      "resp_time     10612641\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "nulos_por_columna = g_estados_seleccionados.isnull().sum()\n",
    "print(nulos_por_columna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_sitios = pd.read_parquet(r'C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\metadata-sitios\\google_sitios_v2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'g_estados_seleccionados' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m g_sitios_ids \u001b[38;5;241m=\u001b[39m g_sitios[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid_negocio\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Filtrar las tablas por los IDs que coincidan con los de g_sitios\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m g_estados_seleccionados \u001b[38;5;241m=\u001b[39m \u001b[43mg_estados_seleccionados\u001b[49m[g_estados_seleccionados[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid_negocio\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(g_sitios_ids)]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'g_estados_seleccionados' is not defined"
     ]
    }
   ],
   "source": [
    "# Obtenemos los ID unicos\n",
    "g_sitios_ids = g_sitios['id_negocio'].unique()\n",
    "\n",
    "# Filtrar las tablas por los IDs que coincidan con los de g_sitios\n",
    "g_estados_seleccionados = g_estados_seleccionados[g_estados_seleccionados['id_negocio'].isin(g_sitios_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetear los índices del DataFrame filtrado\n",
    "g_estados_seleccionados = g_estados_seleccionados.reset_index(drop=True)\n",
    "\n",
    "# Crear una nueva columna 'review_id' con valores consecutivos solo para el DataFrame filtrado\n",
    "g_estados_seleccionados['review_id'] = range(1, len(g_estados_seleccionados) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9434806 entries, 0 to 9434805\n",
      "Data columns (total 10 columns):\n",
      " #   Column      Dtype         \n",
      "---  ------      -----         \n",
      " 0   review_id   int64         \n",
      " 1   id_negocio  object        \n",
      " 2   Estado      object        \n",
      " 3   user_id     object        \n",
      " 4   name        object        \n",
      " 5   time        datetime64[ns]\n",
      " 6   rating      int64         \n",
      " 7   text        object        \n",
      " 8   resp_text   object        \n",
      " 9   resp_time   datetime64[ns]\n",
      "dtypes: datetime64[ns](2), int64(2), object(6)\n",
      "memory usage: 719.8+ MB\n"
     ]
    }
   ],
   "source": [
    "g_estados_seleccionados.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_estados_seleccionados.to_parquet(r'C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\Datasets\\datasets_google\\g_estados_seleccionados.parquet', engine='auto', compression='snappy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AAentorno",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
